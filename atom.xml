<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>谭新宇的博客</title>
  
  <subtitle>Make Each Day Count</subtitle>
  <link href="https://tanxinyu.work/atom.xml" rel="self"/>
  
  <link href="https://tanxinyu.work/"/>
  <updated>2022-08-28T15:01:40.099Z</updated>
  <id>https://tanxinyu.work/</id>
  
  <author>
    <name>谭新宇</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Talent-Plan：用 Rust 实现简易 KV 引擎</title>
    <link href="https://tanxinyu.work/naive-kvengine-in-rust/"/>
    <id>https://tanxinyu.work/naive-kvengine-in-rust/</id>
    <published>2022-08-28T14:48:16.000Z</published>
    <updated>2022-08-28T15:01:40.099Z</updated>
    
    <content type="html"><![CDATA[<h2 id="版本"><a href="#版本" class="headerlink" title="版本"></a>版本</h2><ul><li><a href="https://github.com/pingcap/talent-plan/tree/master/courses/rust" target="_blank" rel="noopener">官网版本</a></li></ul><h2 id="前期准备"><a href="#前期准备" class="headerlink" title="前期准备"></a>前期准备</h2><p>Rust 学习</p><ul><li><a href="https://fasterthanli.me/articles/a-half-hour-to-learn-rust" target="_blank" rel="noopener">半小时学习 Rust</a></li><li><a href="https://github.com/rust-lang/rustlings" target="_blank" rel="noopener">Rustling</a> 及 <a href="https://github.com/OneSizeFitsQuorum/rustlings/pull/1" target="_blank" rel="noopener">解答</a></li><li><a href="https://course.rs/about-book.html" target="_blank" rel="noopener">Rust 语言圣经</a> 及 <a href="https://zh.practice.rs/why-exercise.html" target="_blank" rel="noopener">习题</a></li><li><a href="https://kaisery.github.io/trpl-zh-cn/title-page.html" target="_blank" rel="noopener">Rust 官方文档</a></li><li><a href="https://zhuanlan.zhihu.com/p/558415847" target="_blank" rel="noopener">Talent Plan Percolator Lab</a></li></ul><h2 id="过关过程"><a href="#过关过程" class="headerlink" title="过关过程"></a>过关过程</h2><h3 id="Rust-Project-1-The-Rust-toolbox"><a href="#Rust-Project-1-The-Rust-toolbox" class="headerlink" title="Rust Project 1: The Rust toolbox"></a>Rust Project 1: The Rust toolbox</h3><p>本 project 过关代码可参考该 <a href="https://github.com/OneSizeFitsQuorum/PracticalNetworkedApplications/commit/1a0ca3eb1af93c33a3bb9881dec782dbd623aa49" target="_blank" rel="noopener">commit</a>。</p><p>主要参照了 <a href="https://github.com/OneSizeFitsQuorum/talent-plan/blob/master/courses/rust/projects/project-1/README.md" target="_blank" rel="noopener">README</a> 来完成本 project，具体过程比较 trivial 不再细述。主要工作如下：</p><ul><li>搭建项目基本目录结构。</li><li>使用 <a href="https://docs.rs/clap/3.2.15/clap/" target="_blank" rel="noopener">clap</a> 来解析命令行参数，根据官方文档学习 crate 的具体使用方法。</li><li>使用 cargo.toml 中的若干参数，包括 dev-dependencies，条件编译等等。</li><li>完成基于内存 hashmap 的 KvStore 的增删改查接口。</li><li>增加包文档和函数文档并在文档中添加了文档测试</li><li>使用 cargo fmt 和 cargo clippy 来提升代码质量</li></ul><h3 id="Rust-Project-2-Log-structured-file-I-O"><a href="#Rust-Project-2-Log-structured-file-I-O" class="headerlink" title="Rust Project 2: Log-structured file I/O"></a>Rust Project 2: Log-structured file I/O</h3><p>本 project 过关代码可参考该 <a href="https://github.com/OneSizeFitsQuorum/PracticalNetworkedApplications/commit/fb097b172c46784ed7ebe24afdec0a8ab5d3d399" target="_blank" rel="noopener">commit</a>。</p><h4 id="错误处理"><a href="#错误处理" class="headerlink" title="错误处理"></a>错误处理</h4><p>在阅读 failure crate 的 <a href="https://boats.gitlab.io/failure/" target="_blank" rel="noopener">文档</a> 之后，在本 project 中采用了第二种错误处理方式——自定义错误结构。通过定义 KVStoreError 结构体并使用 failure crate 提供的能力，可以很轻易地捕捉不同的错误并列举他们的表示，调用者也可以直接通过模式匹配的方式得到错误类型。</p><p>此外，通过为 io:Error 和 serde_json:Error 添加转换到 KVStoreError 的函数，在主逻辑中可以轻松的使用 ? 来向上传递错误，从而避免对 Result 类型的暴力 unwrap。</p><p>此外，还定义了 Result<T> 类型别名来统一本项目中所有的 Result 返回类型。</T></p><p><img src="/naive-kvengine-in-rust/boxcnvQkj4GJmiTwJeV21SmMeFf.png" srcset="/img/loading.gif" lazyload alt></p><h4 id="包结构"><a href="#包结构" class="headerlink" title="包结构"></a>包结构</h4><p>对于包含一个 lib 包和一个 bin 包的 crate ，在 lib 包中，需要引用所有新增文件的文件名当做其模块名将其引入，此外还需要使用 pub use 语法来将 bin 包会用到的结构公开导出。</p><p>在 lib 包的任何文件里，都可以通过 crate:: 的方式来引入本 lib 库被公开导出的结构。</p><p>在 bin 包中，需要通过实际 crate 名：: 的方式来引入同名 lib 库被公开导出的结构。</p><p><img src="/naive-kvengine-in-rust/boxcnYwXTYAHx39aKM0Pljd8R7b.png" srcset="/img/loading.gif" lazyload alt></p><p><img src="/naive-kvengine-in-rust/boxcnnTJ4rUCXtxN59nGmDPclRc.png" srcset="/img/loading.gif" lazyload alt></p><p><img src="/naive-kvengine-in-rust/boxcnwOPKgrmNT5kp3zgO5UB2Tc.png" srcset="/img/loading.gif" lazyload alt></p><p><img src="/naive-kvengine-in-rust/boxcn1GHZz1EW2O6yQvws5Xz0Ah.png" srcset="/img/loading.gif" lazyload alt></p><h4 id="结果捕捉"><a href="#结果捕捉" class="headerlink" title="结果捕捉"></a>结果捕捉</h4><p><img src="/naive-kvengine-in-rust/boxcn4BhJAA4AnwlfT6ksZkY7Ye.png" srcset="/img/loading.gif" lazyload alt></p><p>结果捕捉中的正常/异常处理需要满足以上题意的要求，因而在 main 函数中原样实现了以上需求如下。</p><p><img src="/naive-kvengine-in-rust/boxcnFEf4FsRUzUJXg3v9CORCQc.png" srcset="/img/loading.gif" lazyload alt></p><h4 id="结构体"><a href="#结构体" class="headerlink" title="结构体"></a>结构体</h4><p>KvStore 结构体中各个变量含义如下：</p><ul><li>Index ：参照 bitcask 的模型，key 为 kv pair 的 key，value 并不存储对应的 value，而是存储该 value 在第 file_number 个文件的 offset 处，长度为 length。</li><li>current_readers：对于所有已经存在的文件，KvStore 都缓存了一个 BufReader 来便于 seek 到对应的 offset 去 read。实际上也可以没有该结构体每次需要 reader 时新建即可，但复用 reader 可以一定程度上减少资源的损耗。</li><li>current_writer：当前正在写入的 file，其每次写入只需要 append 即可，不需要 seek。新建一个 BufWriterWithPosition 结构体的原因是能够快速的获取当前写入的 offset，而不需要在通过 seek(SeekFrom::Current(0))（可能是系统调用） 的方式去获取。</li><li>current_file_number：当前最大的 file_number，每次 compaction 之后会新增 1。每个数据文件都会附带一个 file_number，file_number 越大的文件越新，该 version 能够保证恢复时的正确性。</li><li>dir_path：当前文件目录路径。</li><li>useless_size：当前无用的数据总和。当改值大于某一个阈值时，会触发一次 compaction。</li></ul><p><img src="/naive-kvengine-in-rust/boxcnY6aDLMIS6gKAnE8vWIz1Gh.png" srcset="/img/loading.gif" lazyload alt></p><p><img src="/naive-kvengine-in-rust/boxcnQyOZ7qGBDfLOSbKLQdpKXg.png" srcset="/img/loading.gif" lazyload alt></p><p><img src="/naive-kvengine-in-rust/boxcnRZrLuatQsEySdLCweP9hJf.png" srcset="/img/loading.gif" lazyload alt></p><h4 id="写流程"><a href="#写流程" class="headerlink" title="写流程"></a>写流程</h4><p>使用 serde_json 将 set 命令序列化，接着再写入到 current_writer 中，然后在 index map 中维护该 key 的索引。注意如果某 key 之前已在 KvStore 中存在，则 insert 函数会返回该 key 的旧 value，此时需要维护 useless_size。最后判断如果 useless_size 超过某一个阈值，则进行一次 compact。</p><p>需要注意许多返回 Result<T, error> 的函数都可以通过 ? 而直接向上传递异常，这得益于 Rust 错误处理的良好抽象。</T,></p><p><img src="/naive-kvengine-in-rust/boxcnvogcxYCj6ePDFug2C3BYUg.png" srcset="/img/loading.gif" lazyload alt></p><h4 id="读流程"><a href="#读流程" class="headerlink" title="读流程"></a>读流程</h4><p>首先在 index 中获取该 key 的索引，如果不存在则说明该 key 不存在直接返回即可，否则根据索引中的 file_number 在 current_readers 中拿到对应的 reader，seek 到对应的 offset 并读取长度为 length 的数据。如果存在则返回 value，否则说明遇到了异常，返回错误即可。</p><p><img src="/naive-kvengine-in-rust/boxcnjPcxYncr6MzPFaF5vITIFf.png" srcset="/img/loading.gif" lazyload alt></p><h4 id="删除流程"><a href="#删除流程" class="headerlink" title="删除流程"></a>删除流程</h4><p>首先在 index 中获取该 key 的索引，如果不存在则说明该 key 不存在返回 ErrNotFound 错误即可，否则移除该索引，接着将 rm 命令序列化并写入到 current_writer 中以保证该 key 能够被确定性删除。注意对于能够找到对应 key 的 rm 命令，useless_size 不仅需要增加 rm 命令本身的长度，还需要增加之前 set 命令的长度，因为此时他们俩都已经可以被一起回收。 最后判断如果 useless_size 超过某一个阈值，则进行一次 compact。</p><p><img src="/naive-kvengine-in-rust/boxcnxthxhEe0zV8eIkzz9VYGOf.png" srcset="/img/loading.gif" lazyload alt></p><h4 id="重启流程"><a href="#重启流程" class="headerlink" title="重启流程"></a>重启流程</h4><p>重启时首先初始化若干重要结构，最重要的是调用 recover 函数，该函数将遍历当前所有的文件，不仅将索引维护到 index 结构中，还会将 reader 维护到 current_readers 结构中，最后返回（当前最大的文件版本，当前所有文件的 useless_size)，接着利用 current_file_number 构建当前最大文件的 writer，需要注意由于 bitcask 模型是 append_only 的机制，所以在构建 writer 时需要使用 OpenOptions 来使得 append 属性为 true，这样重启后直接 append 即可。最后根据 use_less 判断是否需要 compact，最后返回即可。</p><p><img src="/naive-kvengine-in-rust/boxcnBf5TfEwrTv7qGI7BqvI1Lh.png" srcset="/img/loading.gif" lazyload alt></p><p>对于 Recover 函数，其需要读取数据目录中的所有文件，按照 file_number 从小到大的顺序去按序 apply 从而保证重启的正确性。</p><p>对于排序，不能直接对文件名排序，因为这样的排序是按照字母编码而不是按照 file_number 大小。因此需要先将所有的 file_number 解析出来再对数字进行排序，之后再利用这些数字索引文件名即可。需要注意这里利用了许多文件操作的链式调用，需要查很多文档。</p><p>在获取到排序好的 versions 之后，可以按序读取文件并将其维护到 index 和 current_readers 中去，注意在该过程中也要注意维护 useless_size。此外得益于 serde_json 的 from_reader().into_iter() 接口，可以按照迭代器的方式去读取 command，而不用关注何时到了末尾，应该读多少字节才可以解析出一个 command，这极大的简化了读取流程。</p><p><img src="/naive-kvengine-in-rust/boxcnLi3nwF781srG69oyMI22Dh.png" srcset="/img/loading.gif" lazyload alt></p><p><img src="/naive-kvengine-in-rust/boxcnBDm1Ma5cBgjhEakKRxtvMe.png" srcset="/img/loading.gif" lazyload alt></p><h4 id="合并流程"><a href="#合并流程" class="headerlink" title="合并流程"></a>合并流程</h4><p>当前的合并流程采用了暴力的全部合并策略，同时将合并放在了客户端可感知延迟的执行流程中。</p><p>当 useless_size 大于某个阈值时，会触发一次合并，此时会增加 file_number 并将 index 中所有的数据都写入到当前新建的文件中，同时更新内存中的索引。接着再删除老文件和对应的 reader，最后再新建一个文件承载之后的写入即可。</p><p>需要注意按照这个流程即使在合并的写文件过程中出现了重启也不会出现正确性问题。如果新文件的所有数据尚未 flush 成功，老文件并不会被删除，那么只要重启时会按照 file_number 从小到大的顺序进行重放，数据便不会丢失。</p><p><img src="/naive-kvengine-in-rust/boxcnwANdnQiduaMqWuTfuqYqtf.png" srcset="/img/loading.gif" lazyload alt></p><p><img src="/naive-kvengine-in-rust/boxcnfOG27BeMg3VNRx3DnIOySb.png" srcset="/img/loading.gif" lazyload alt></p><h3 id="Rust-Project-3-Synchronous-client-server-networking"><a href="#Rust-Project-3-Synchronous-client-server-networking" class="headerlink" title="Rust Project 3: Synchronous client-server networking"></a>Rust Project 3: Synchronous client-server networking</h3><p>本 project 过关代码可参考该 <a href="https://github.com/OneSizeFitsQuorum/PracticalNetworkedApplications/commit/4bf43135e3f662bd7f3a40078d2e648f436ce632" target="_blank" rel="noopener">commit</a>。</p><h4 id="命令行解析"><a href="#命令行解析" class="headerlink" title="命令行解析"></a>命令行解析</h4><p>在本 project 中，命令行分为了客户端 kvs-client 和服务端 kvs-server 两处，因此需要分别进行解析。</p><p>对于 kvs-client，基本继承了 project2 的命令行解析工具，仅仅增加了 addr 的解析。此外也按照题意将正常输出打印在了 stdout 中，将错误输出打印在了 stderr 中并以非 0 值结束进程</p><p><img src="/naive-kvengine-in-rust/boxcnlWsXGHwpQAAaeTPofjL0Kd.png" srcset="/img/loading.gif" lazyload alt></p><p><img src="/naive-kvengine-in-rust/boxcn6NSpWGbC9UEkuyXIACdTJf.png" srcset="/img/loading.gif" lazyload alt></p><p>对于 kvs-server，则是按照题意重新写了参数解析器，并对于 engine 增加了只能 2 选 1 的约束。同时还利用 judge_engine 函数实现了引擎选择的判断：对于第一次启动，按照用户参数来启动对应的引擎，如未指定则使用 kvs；对于之后的启动，必须按照之前的引擎启动，若与用户参数冲突则报错。在参数无问题之后打出对应的关键配置既可。</p><p><img src="/naive-kvengine-in-rust/boxcnI8W6PH9TwZcCP0DC53yUub.png" srcset="/img/loading.gif" lazyload alt></p><p><img src="/naive-kvengine-in-rust/boxcn0ExuHszDnabB7fosTNAXXq.png" srcset="/img/loading.gif" lazyload alt></p><p><img src="/naive-kvengine-in-rust/boxcnzfeeo29RWnUZNdMlRtSNKg.png" srcset="/img/loading.gif" lazyload alt></p><p><img src="/naive-kvengine-in-rust/boxcnyJQWFUM6irMEkN22WYrW7c.png" srcset="/img/loading.gif" lazyload alt></p><h4 id="日志打印"><a href="#日志打印" class="headerlink" title="日志打印"></a>日志打印</h4><p>在本 project 中对日志采用了集成轻量的 env_logger，参照 <a href="https://docs.rs/env_logger/0.9.0/env_logger/" target="_blank" rel="noopener">文档</a> 仅仅需要在进程启动时指定日志的最低级别即可。</p><p><img src="/naive-kvengine-in-rust/boxcncl0pVAsp6LqYau0QRjXAbc.png" srcset="/img/loading.gif" lazyload alt></p><h4 id="命令传输"><a href="#命令传输" class="headerlink" title="命令传输"></a>命令传输</h4><p>本 project 直接使用了 tcp 级别的网络接口来传输命令，因而会有黏包的问题需要处理。</p><p>一般的解决方案是在流上发送每段数据前先写入长度，再写入真实的数据；这样在流上读数据时便可以先读长度，再读对应长度的数据后解除阻塞返回了。</p><p>这样的思路可以自己手写，也可以使用 serde 现成的 reader/writer 接口去实现。因而在客户端构建了一个 Client 结构体对 socket 进行了简单的包装。对于 request，使用了一个 BufWriter 的装饰器配以每次写完数据后的 flush 来降低系统调用的开销啊，其在内部已经能够做到先写入长度再写入数据。对于 response，则是参照重启恢复时的逻辑使用 Deserializer 接口构建 reader，并指定对应的反序列化类型以达到先读长度再读数据的问题。这样便可以利用 serde 帮助解决黏包问题。</p><p><img src="/naive-kvengine-in-rust/boxcnGwjWKxNs7ZmeiWebu1lIoc.png" srcset="/img/loading.gif" lazyload alt></p><p><img src="/naive-kvengine-in-rust/boxcnEArkStGv1C1Us4ew1SguPf.png" srcset="/img/loading.gif" lazyload alt></p><p>对于服务端，获取 request 和发送 response 的流程和客户端类似。</p><p><img src="/naive-kvengine-in-rust/boxcnUw2Q2c0Bvoe3GjIFMHSE05.png" srcset="/img/loading.gif" lazyload alt></p><h4 id="可扩展存储引擎"><a href="#可扩展存储引擎" class="headerlink" title="可扩展存储引擎"></a>可扩展存储引擎</h4><p>为了扩展存储引擎的多种实现，抽象出来了统一的 trait 接口 KvsEngine 以对上暴露 trait 的抽象而隐藏具体的实现细节。这样 kvs-server 在启动时便可以以 trait 的方式去访问 engine，而不需要在意其内部的实现细节。</p><p><img src="/naive-kvengine-in-rust/boxcnPj0lx11Jj3YB6h1REJY7Zf.png" srcset="/img/loading.gif" lazyload alt></p><p><img src="/naive-kvengine-in-rust/boxcn5rZYcmvMfYnXcat5LSeCos.png" srcset="/img/loading.gif" lazyload alt></p><p><img src="/naive-kvengine-in-rust/boxcnG99bQVGuok6gTqhr05Kmkn.png" srcset="/img/loading.gif" lazyload alt></p><p>对于 KvStore，将其 set/get/remove 这三个方法抽象到了 KvsEngine 的实现中。</p><p><img src="/naive-kvengine-in-rust/boxcnzQkpPos1qxbygU4KvMx7Xc.png" srcset="/img/loading.gif" lazyload alt></p><p>对于 Sled，同样实现了 KvsEngine 的三个方法。需要注意其默认接口的语义和格式与 KvsEngine 不一致，因而需要增加对应的转换。</p><p>此外在 set 时注释掉对应的 flush 操作是由于增加上之后性能过于慢，无法在之后的 bench 阶段跑出结果。</p><p><img src="/naive-kvengine-in-rust/boxcniO3GQy0JnxunBBQ3rH7ewg.png" srcset="/img/loading.gif" lazyload alt></p><h4 id="性能测试"><a href="#性能测试" class="headerlink" title="性能测试"></a>性能测试</h4><p>参照 <a href="https://github.com/OneSizeFitsQuorum/talent-plan/blob/master/courses/rust/projects/project-3/README.md" target="_blank" rel="noopener">Project3 文档</a> 中的介绍创建了 benches/benches 文件并参照 criterion 的 <a href="https://bheisler.github.io/criterion.rs/book/getting_started.html" target="_blank" rel="noopener">用户手册</a> 开始构建性能测试。</p><p><img src="/naive-kvengine-in-rust/boxcnkIOzLCHIFKMU89CnoFKRMc.png" srcset="/img/loading.gif" lazyload alt></p><p>对于性能测试中的三个问题：</p><ul><li>如何精准测量想要测量的时间，而不包括初始化和清理的时间：参照 <a href="https://bheisler.github.io/criterion.rs/book/user_guide/timing_loops.html" target="_blank" rel="noopener">criterion 计时迭代的文档</a> 选择了 iter_batched 接口来精准测量读写的时间，初始化的清理的时间并不会被包括在内。</li><li>尽管使用了 rand，如何使得每次迭代都确定性：这里通过在迭代之前利用 rand 的 <a href="https://rust-random.github.io/book/guide-seq.html" target="_blank" rel="noopener">choose_multiple</a> 函数创建好对应的写入数据，使得每次迭代的操作数都具有相同的集合。</li><li>在读 benchmark 中，如何保证选到的读集合是写集合的子集：这里采用了同样的方法，读集合是在写集合的集成上去随机选择，从而保证了读取必然能够读到。</li></ul><p><img src="/naive-kvengine-in-rust/boxcnH9edzFxF6Ey7CpeEELfj3d.png" srcset="/img/loading.gif" lazyload alt></p><p><img src="/naive-kvengine-in-rust/boxcnjxp5q574LIf0hidMFs4E7d.png" srcset="/img/loading.gif" lazyload alt></p><p><img src="/naive-kvengine-in-rust/boxcnq23CIUQa25i4rf4mFEbk5b.png" srcset="/img/loading.gif" lazyload alt></p><p>最终性能对比如下：尽管已经去掉了 sled 每次写入时的 flush 操作来减少其随机 IO，在单线程客户端的情况下，sled 引擎的写延时大概是自写 bitcask 引擎写延时的 20 倍；sled 引擎的读延时大概是自写 bitcask 引擎读延时的 800 倍。</p><p><img src="/naive-kvengine-in-rust/boxcno48X01i1c9r5i1aeN1XUQh.png" srcset="/img/loading.gif" lazyload alt></p><p>个人猜测产生如此悬殊对比的原因有可能是：</p><ul><li>Sled 专为多线程无锁设计，在单线程下无法体现其性能优势。</li><li>Sled 本质上是一种树状结构，其相比 hash 结构能够提供高效的范围查询，也能够在海量数据场景与磁盘结合起来提供稳定的读延时，因而在小数据量的单点查询场景相比 hash 结构并不占优势。</li><li>当前自写 bitcask 模型还没有引入并发处理的开销，而 sled 是并发安全的，如此对比并不公平。</li></ul><p>本来想用一些 profile 工具测量一下 sled 的火焰图查找一下原因，由于本人的电脑芯片是 M1Pro，许多 profile 工具类如 perf 安装还不是很方便。在参照 <a href="https://github.com/tikv/pprof-rs/blob/master/examples/criterion.rs" target="_blank" rel="noopener"> pprof-rs 的文档</a> 为 criterion 配置之后依然无法打出火焰图，猜测可能跟环境有关系，便没有进一步再研究了，之后有机会在 Linux 下再进行 profile 吧。</p><h3 id="Rust-Project-4-Concurrency-and-parallelism"><a href="#Rust-Project-4-Concurrency-and-parallelism" class="headerlink" title="Rust Project 4: Concurrency and parallelism"></a>Rust Project 4: Concurrency and parallelism</h3><p>本 project 过关代码可参考该 <a href="https://github.com/OneSizeFitsQuorum/PracticalNetworkedApplications/commit/3d31896e832cef31e78380794f768375fc1bfc70" target="_blank" rel="noopener">commit</a>。</p><h4 id="线程池"><a href="#线程池" class="headerlink" title="线程池"></a>线程池</h4><p>为了多线程需要抽象出线程池的概念，ThreadPool trait 定义如下：spawn 函数中的闭包 F 不仅需要满足 FnOnce() 的 bound 来满足近执行一次的语义，还要实现 Send + ‘static 的 bound 来实现线程安全的发送接收和足够长的生命周期。</p><p><img src="/naive-kvengine-in-rust/boxcnZcbACyBVBV9JlUYqicx0we.png" srcset="/img/loading.gif" lazyload alt></p><p>对于最简单的 NaiveThreadPool，仅仅需要在 spawn 的时候创建一个线程让其执行即可。</p><p><img src="/naive-kvengine-in-rust/boxcnkG00kSYp5ZcIaGm4TmiKpe.png" srcset="/img/loading.gif" lazyload alt></p><p>对于共享队列的 ThreadPool，参照 RustBook 中的 <a href="https://kaisery.github.io/trpl-zh-cn/ch20-02-multithreaded.html" target="_blank" rel="noopener">举例</a> 即可实现。大体思路是用 channel 做通信，让多个子线程竞争 job 去执行即可。需要注意以下三点：</p><ul><li>std 库自带的 channel 是 MPSC 类型，因而可以支持并发写但不支持并发读。因而要想实现多个子 thread 对 channel 的监听便需要用 Arc<Mutex<receiver>&gt; 来保证不存在并发读。此外也可以使用 crossbeam 的 <a href="https://github.com/crossbeam-rs/crossbeam/tree/master/crossbeam-channel" target="_blank" rel="noopener">mpsc channel</a> 来支持并发读，那样便直接 clone 即可。</Mutex<receiver></li><li>为了优雅停机，对于 Job 又包装了一层枚举和 Terminate 类型来支持子 thread 的优雅退出，此外还需要利用 Box 将闭包 F 放在堆上来支持线程安全的传递闭包。</li><li>由于单元测试中传入的闭包可能会 panic 但不想看到线程池中的线程减少，一种方案是检测到线程 panic 退出之后新增新的线程，另一种方式则是捕获可能得 panic。例如在 Java 中可以使用 try catch 捕捉一个 throwable 的错误，在 go 中可以 defer recover 一个 panic。在 rust 中类似的语法是 <a href="https://doc.rust-lang.org/std/panic/fn.catch_unwind.html" target="_blank" rel="noopener">catch_unwind</a>，因而在执行真正的 job 闭包时，会使用 panic::catch_unwind(AssertUnwindSafe(job)) 的方式来确保该线程不会由于执行闭包而 panic。</li></ul><p><img src="/naive-kvengine-in-rust/boxcnb0wlmKE1MrtOLIjPnjaiBd.png" srcset="/img/loading.gif" lazyload alt></p><p><img src="/naive-kvengine-in-rust/boxcnkPconSZtYcnSV1O8K4loPk.png" srcset="/img/loading.gif" lazyload alt></p><p><img src="/naive-kvengine-in-rust/boxcn1WUh1RyixFp1UHgsRv44TV.png" srcset="/img/loading.gif" lazyload alt></p><p>对于 RayonThreadPool，直接参考官网的样例初始化对应的 pool 并直接 spawn 给其即可。</p><p><img src="/naive-kvengine-in-rust/boxcnsmyPz8LfrnLhML6AksCPDc.png" srcset="/img/loading.gif" lazyload alt></p><h4 id="多线程服务端"><a href="#多线程服务端" class="headerlink" title="多线程服务端"></a>多线程服务端</h4><p>在 KvServer 初始化时使用了一个线程池来管理不同 tcp 连接的读写，这样便可以使得并发的请求能够在多核 CPU 的服务端并行执行而不是并发执行。</p><p><img src="/naive-kvengine-in-rust/boxcnnVupdcBFwenEaIuUvFhWDh.png" srcset="/img/loading.gif" lazyload alt></p><p>注意在 KvServer 中还维护了一个 is_stop 的原子变量，该变量的作用是能够便于当前线程结束阻塞等待进而退出。之所以阻塞的原因是由于 tcplistener 的 incoming() 函数是阻塞的，因而一旦进入 serve 函数当前线程就阻塞了。在之后的性能测试中可能一个线程内想在启动 server 后开始迭代测试并最后关闭 server 并进行下一轮测试：此时如果是同步的写法就无法执行 serve 之后的函数，如果新建一个线程则无法在迭代测试之后通知该线程结束，因而加入了该原子变量之后不仅可以异步启动 server 从而在当前线程进行性能测试，又能够在当前线程的测试结束后以新建一个空 client 的方式关闭 server 以便下一轮测试不会再出现 address already in use 的错误。</p><p><img src="/naive-kvengine-in-rust/boxcn0lodECLFH3AjTT7CGOFGLg.png" srcset="/img/loading.gif" lazyload alt></p><h4 id="KvsEngine-线程安全"><a href="#KvsEngine-线程安全" class="headerlink" title="KvsEngine  线程安全"></a>KvsEngine  线程安全</h4><p>KvsEngine trait 需要满足 Clone + Send + ‘static 的 bound，同时三个对应的接口也可以去掉 &amp;mut，因为变量的所有权和可变性已经转移到了智能指针中。</p><p><img src="/naive-kvengine-in-rust/boxcnJFwgaF5IXUqkq7iPFqorWd.png" srcset="/img/loading.gif" lazyload alt></p><h4 id="SledKvsEngine-线程安全"><a href="#SledKvsEngine-线程安全" class="headerlink" title="SledKvsEngine 线程安全"></a>SledKvsEngine 线程安全</h4><p>Sled 引擎本身支持并发读写，因而直接对结构体 derive(Clone) 即可，其 set/get/remove 函数仅需挪去 self 的 &amp;mut 即可。</p><p><img src="/naive-kvengine-in-rust/boxcnzGfVRmcZ2kjZyKatOZ9uHb.png" srcset="/img/loading.gif" lazyload alt></p><p><img src="/naive-kvengine-in-rust/boxcnPZjYAEANb7Z9O28Zm5rfIg.png" srcset="/img/loading.gif" lazyload alt></p><p><img src="/naive-kvengine-in-rust/boxcnW993QBlpNeVJ1vocEYpCwe.png" srcset="/img/loading.gif" lazyload alt></p><h4 id="KvStore-线程安全"><a href="#KvStore-线程安全" class="headerlink" title="KvStore 线程安全"></a>KvStore 线程安全</h4><p>KvStore 的线程安全则需要对之前的结构体做大量的改造，改造之后的 KvStore 不仅支持读写请求互相不阻塞，甚至对同一 FileReader 的读请求也可以不在应用层阻塞。</p><ul><li>对于 index 结构，将其转换为了并发安全的 DashMap 结构，同时又增加了 Arc 指针以便于在不同线程间共享。</li></ul><p><img src="/naive-kvengine-in-rust/boxcnaTmdxQINxNh6bgoVnbVpIh.png" srcset="/img/loading.gif" lazyload alt></p><ul><li>对于 writer 结构，由于对同一文件的 append 操作从语义上来说便不支持并行，因而便通过 Arc<Mutex<Writer>&gt; 的方式将所有的线程串行起来</Mutex<Writer></li></ul><p><img src="/naive-kvengine-in-rust/boxcn35QJnz7gVjIWmD7xrNrTCh.png" srcset="/img/loading.gif" lazyload alt></p><ul><li>对于 readers 结构，我参照了部分 project4 样例的源码设计进行了无锁实现。注意其内部的 readers 在 clone 时并不是拷贝指针，而是初始化一个全新的 map，因而当多线程读同一个文件时，会创建多个 reader，这些 reader 可以在应用层对同一个文件执行并发的 IO 读请求。使用 RefCell 包装的原因是由于接口并未提供可变引用，如果还想保留对 map 的更改权限就需要用到 RefCell 了。</li><li>最容易想到的一种读请求并发控制方案是在上层做一定的串行以使得每个文件最多同时只有一个读请求在执行，从而减少磁盘的随机 IO。但实际上这样的设计并不一定有效果，一方面由于在文件系统中一个 file 的所有 data block 不一定完全在连续的 block 上，因而仅仅限制对一个文件不能并发读而不限制对多个文件不能并发读不一定能够起到减少随机 IO 的效果，另一方面 linux VFS 下的 IO 调度层本来就已经会对 IO 请求通过电梯算法等方式来做一些乱序处理来减少随机 IO，如果完全在上层做了串行反而会丢失部分可优化吞吐的空间。因而如果系统还没到完全掌握磁盘快中数据的分布来减少随机 IO 的地步，可以先尽可能的将读请求并行起来让底层去串行，而不是在上层就做好串行。</li></ul><p><img src="/naive-kvengine-in-rust/boxcnlZ4coU7mWqKybLfYC7pqgp.png" srcset="/img/loading.gif" lazyload alt></p><h4 id="写流程-1"><a href="#写流程-1" class="headerlink" title="写流程"></a>写流程</h4><p>由于 append 的写请求语义上就不能并行，因而当前 KvsEngine 的 set 请求被全部串行了起来</p><p><img src="/naive-kvengine-in-rust/boxcnrM7s7ntdUZwS41euVNO2af.png" srcset="/img/loading.gif" lazyload alt></p><p><img src="/naive-kvengine-in-rust/boxcnFBgZG4DdIytqlfeqrlmqxe.png" srcset="/img/loading.gif" lazyload alt></p><h4 id="删除流程-1"><a href="#删除流程-1" class="headerlink" title="删除流程"></a>删除流程</h4><p>由于删除也需要顺序 append，因而其语义与 set 类似不能并行，因而当前 KvsEngine 的 remove 请求也被全部串行了起来</p><p><img src="/naive-kvengine-in-rust/boxcnDij7HnjT3xTO0tBYJBmyEg.png" srcset="/img/loading.gif" lazyload alt></p><h4 id="读流程-1"><a href="#读流程-1" class="headerlink" title="读流程"></a>读流程</h4><p>读流程语义理论上可以并行执行，因而首先在可并发读的 DashMap 中获取到索引，接着在当前线程内读取对应的 file_number 的 reader，如果当前线程不存在该 reader 则创建出对应的 reader 读取即可（使用了 entry API 来避免两次 hash）。</p><p><img src="/naive-kvengine-in-rust/boxcnPQEGztT4pID88lJEVIpTKJ.png" srcset="/img/loading.gif" lazyload alt></p><p><img src="/naive-kvengine-in-rust/boxcnRs4vfkVcwiEJoQLIJpHM7o.png" srcset="/img/loading.gif" lazyload alt></p><p><img src="/naive-kvengine-in-rust/boxcn9YBWH6fWTEqZroFFEmjfQd.png" srcset="/img/loading.gif" lazyload alt></p><h4 id="合并流程-1"><a href="#合并流程-1" class="headerlink" title="合并流程"></a>合并流程</h4><p>在实现无锁读之后，reader 的清理便不再能够串行起来了，因而需要一个多线程共享的原子变量来记录最新 compaction 之后的 file_number，小于这个 file_number 的文件和对应的 reader 便都可以删除了。</p><p>compact 流程会始终持有 writer 的写锁，因而此时并不存在并发安全问题，其在结束后会尝试删除掉过时的文件。不过该删除并不会影响其他读线程的 reader 句柄继续读去文件，这与 linux 文件系统的实现原理有关，直到任何线程都不存在指向该文件对应 inode 的句柄时便可以安全的释放该文件了。</p><p><img src="/naive-kvengine-in-rust/boxcnj3SztibcQ3dVtk80AgHB5r.png" srcset="/img/loading.gif" lazyload alt></p><p><img src="/naive-kvengine-in-rust/boxcnkFizeIlIHOkdtLdjiJXa1b.png" srcset="/img/loading.gif" lazyload alt></p><p>对于 reader，在 compaction 中其执行的索引尽管可能文件已经被删除了，但由于其持有句柄因而始终能够读到数据，在 compaction 之后其执行的索引一定是更新的文件，因而老的 reader 便不会再被用到，如果这些老 reader 一直不被释放，则可能导致合并过后的老文件始终无法在文件系统被释放，最终导致磁盘变满。因此在每次查询时都可以判断一下该原子变量并尝试删除本线程的老 reader，这样便可以既实现 lock-less 的 reader 又满足 compaction 消息的无锁感知和对应的资源清理了。</p><p><img src="/naive-kvengine-in-rust/boxcnbIWT6ofbomscJjhYDsI3og.png" srcset="/img/loading.gif" lazyload alt></p><h4 id="性能测试-1"><a href="#性能测试-1" class="headerlink" title="性能测试"></a>性能测试</h4><p>按照题意写出对应的六个 benchmark 即可，主要做了以下工作：</p><ul><li>使用 Once 接口来确保在多个函数中 logger 仅被初始化一次，从而避免报错。</li></ul><p><img src="/naive-kvengine-in-rust/boxcn2a3QFIu6KvawbPR6I8lb0d.png" srcset="/img/loading.gif" lazyload alt></p><ul><li>使用了 <a href="https://bheisler.github.io/criterion.rs/book/user_guide/benchmarking_with_inputs.html" target="_blank" rel="noopener">bench_with_input</a> 接口来支持不同线程数的对比</li></ul><p><img src="/naive-kvengine-in-rust/boxcncFWYwfER6WT9yFEYNoIIud.png" srcset="/img/loading.gif" lazyload alt></p><ul><li>使用了之前提到的方式来异步启动 server 并在一轮迭代结束后回收 server。</li></ul><p><img src="/naive-kvengine-in-rust/boxcnjhiY2nLL2YcXE6hcSQvbqg.png" srcset="/img/loading.gif" lazyload alt></p><ul><li>使用了 waitGroup 来实现客户端线程和迭代线程的同步。</li></ul><p><img src="/naive-kvengine-in-rust/boxcnUGMyrZgPdodAwns1WyUrwc.png" srcset="/img/loading.gif" lazyload alt></p><ul><li>减少 sample_size 来加快性能测试时间。</li></ul><p><img src="/naive-kvengine-in-rust/boxcnWKzKDZwn4IZ1VuXbeuicbg.png" srcset="/img/loading.gif" lazyload alt></p><p>最终的测试结果如下：</p><ul><li>write_queue_kvstore：随着线程数增大，延时先降再升，但变化幅度不大，尽管 read/write socket 可以并行起来了，但 set 还是必须得串行起来，与实现基本相符。</li></ul><p><img src="/naive-kvengine-in-rust/boxcnrpuZM9Ienhb4A3R8PGHdAd.png" srcset="/img/loading.gif" lazyload alt></p><ul><li>read_queued_kvstore：随着线程数增大，延时先大幅度降低再大幅度升高，大幅度降低符合预期，因为不同的读请求现在可以串行起来，大幅度升高则不太符合预期，观察到了客户端建立连接 Timeout 的现象，不确定是否与本地的 Mac M1 Pro 环境有关。</li></ul><p><img src="/naive-kvengine-in-rust/boxcn4HmL8lEVkATOr4kedonK3f.png" srcset="/img/loading.gif" lazyload alt></p><p>其他测试：</p><ul><li>write_rayon_kvstore</li></ul><p><img src="/naive-kvengine-in-rust/boxcnqWEQYmfb87Ku1fwAHwutEg.png" srcset="/img/loading.gif" lazyload alt></p><ul><li>read_rayon_kvstore</li></ul><p><img src="/naive-kvengine-in-rust/boxcn31tkUHW2shoqZ4XO6Mx3dR.png" srcset="/img/loading.gif" lazyload alt></p><ul><li>write_rayon_sledkvengine</li></ul><p><img src="/naive-kvengine-in-rust/boxcnAgA1VT5aM02gLqyi9YHJ6d.png" srcset="/img/loading.gif" lazyload alt></p><ul><li>read_rayon_sledkvengine</li></ul><p><img src="/naive-kvengine-in-rust/boxcnoPA82ks3t9N0DQJDlpDcIf.png" srcset="/img/loading.gif" lazyload alt></p><p>测试总结：</p><ul><li>总体来看，不同的存储引擎，不同的线程池策略，随着服务端线程池的线程数增大，延时都能够在某点得到最小值，这说明并行能够在部分场景起到效果。</li><li>在 MacOS M1 Pro 的环境上测试性能不太稳定，还容易出现 timeout 的情况，因而便没有进行更详细分析，感觉要想真的对比出性能的差异，还是需要在 Linux 环境下配上稳定的 CPU，磁盘，网络的可观测性工具结合不同引擎和不同线程池的内部 metric 来分析原因。害，底层软件就是这么难测试。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过本 Rust Lab，总共写了大约 2000+ 行的 Rust 代码。从 Cargo 包管理到 Rust 的所有权机制，然后到错误管理和若干标准库三方库的使用，再到线程池和并发引擎的设计以及异步 Runtime 的学习，虽然在性能测试和对比部分做的并不完善，但这些内容已经涵盖了开发大型 Rust 项目的方方面面。</p><p>下一步计划开始从 TiKV 的小 issue 入手，进一步深入学习 Rust。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;版本&quot;&gt;&lt;a href=&quot;#版本&quot; class=&quot;headerlink&quot; title=&quot;版本&quot;&gt;&lt;/a&gt;版本&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/talent-plan/tree/master/cour</summary>
      
    
    
    
    
    <category term="Rust" scheme="https://tanxinyu.work/tags/Rust/"/>
    
    <category term="存储引擎" scheme="https://tanxinyu.work/tags/%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E/"/>
    
  </entry>
  
  <entry>
    <title>Talent-Plan：用 Rust 实现 Percolator 算法</title>
    <link href="https://tanxinyu.work/percolator-in-rust/"/>
    <id>https://tanxinyu.work/percolator-in-rust/</id>
    <published>2022-08-25T03:36:53.000Z</published>
    <updated>2022-08-28T14:50:59.704Z</updated>
    
    <content type="html"><![CDATA[<h2 id="版本"><a href="#版本" class="headerlink" title="版本"></a>版本</h2><ul><li><a href="https://github.com/pingcap/talent-plan/tree/master/courses/dss/percolator" target="_blank" rel="noopener">官网版本</a></li></ul><h2 id="前期准备"><a href="#前期准备" class="headerlink" title="前期准备"></a>前期准备</h2><p>Rust 学习</p><ul><li><a href="https://fasterthanli.me/articles/a-half-hour-to-learn-rust" target="_blank" rel="noopener">半小时学习 Rust</a></li><li><a href="https://github.com/rust-lang/rustlings" target="_blank" rel="noopener">Rustling</a> 及 <a href="https://github.com/OneSizeFitsQuorum/rustlings/pull/1" target="_blank" rel="noopener">解答</a></li><li><a href="https://course.rs/about-book.html" target="_blank" rel="noopener">Rust 语言圣经</a> 及 <a href="https://zh.practice.rs/why-exercise.html" target="_blank" rel="noopener">习题</a></li><li><a href="https://kaisery.github.io/trpl-zh-cn/title-page.html" target="_blank" rel="noopener">Rust 官方文档</a></li></ul><p>Percolator 学习</p><ul><li><a href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/36726.pdf" target="_blank" rel="noopener">论文</a></li><li><a href="https://github.com/OneSizeFitsQuorum/tinykv/blob/test/solution/lab4.md" target="_blank" rel="noopener">TinyKV lab4 文档</a></li></ul><h2 id="过关思路"><a href="#过关思路" class="headerlink" title="过关思路"></a>过关思路</h2><p>在基本环境搭建好之后，观察发现 lab 只有 13 个测试，测试全部 AC 即可通过项目。实际上一个生产级别的 Percolator 实现这些测试是远远不够的，需要考虑和测试的 case 很多。</p><p>由于本 lab 的主要目标是帮助学习 Rust，因此本次过关思路便是通过 TDD 的方式通关，将主要精力放在通过测试以及对应 Rust 语法的学习和练习上，对于实现的 Percolator 算法满足论文要求和当前的测试 case 即可，不再去新增更多的 test case 和并发情况。</p><p><img src="/percolator-in-rust/0.png" srcset="/img/loading.gif" lazyload alt></p><h2 id="过关过程"><a href="#过关过程" class="headerlink" title="过关过程"></a>过关过程</h2><h3 id="TSO-测试"><a href="#TSO-测试" class="headerlink" title="TSO 测试"></a>TSO 测试</h3><ul><li>test_get_timestamp_under_unreliable_network</li></ul><p>该测试期望在不稳定网络下 client 的 get_timestamp 接口在超时重试时可以满足 backoff 幂增重试属性。<br><img src="/percolator-in-rust/1.png" srcset="/img/loading.gif" lazyload alt></p><p>因此一方面在 TSO Server 端的 get_timestamp 接口处提供一个空实现，另一方面在 Client 中维护对应初始化时传入的 rpc client 字段并在 get_timestamp 函数中增加幂增 sleep 逻辑即可。<br><img src="/percolator-in-rust/2.png" srcset="/img/loading.gif" lazyload alt></p><p>有关 async 和 await 的原理需要进一步理解学习，但仅就完成本 lab 而言，可以查看 labrpc example 中的使用样例来模仿使用，即使用 await 来驱动 async 的代码块执行，使用 block_on 达到同步执行的效果。<br><img src="/percolator-in-rust/3.png" srcset="/img/loading.gif" lazyload alt></p><p>通过本测试的新增代码可查看 <a href="https://github.com/OneSizeFitsQuorum/talent-plan/commit/2deab4fec9c2c6a925963dd65cd690507cd7f565" target="_blank" rel="noopener">commit</a></p><h3 id="TXN-正常测试"><a href="#TXN-正常测试" class="headerlink" title="TXN 正常测试"></a>TXN 正常测试</h3><ul><li>test_predicate_many_preceders_read_predicates</li><li>test_predicate_many_preceders_write_predicates</li><li>test_lost_update</li><li>test_read_skew_read_only</li><li>test_read_skew_predicate_dependencies</li><li>test_read_skew_write_predicate</li><li>test_write_skew</li><li>test_anti_dependency_cycles</li></ul><p>以上测试要求在网络正常的情况下能够正确实现 Percolator 的事务提交，即可按照论文中的伪码实现即可。</p><h4 id="client"><a href="#client" class="headerlink" title="client"></a>client</h4><p>对于 Client 的 new 函数，保存两个 rpc client 并初始化 start_ts 和 mem_buffer。注意，为了在客户端进行去重，mem_buffer 使用了 hashmap 而不是 Vec。<br><img src="/percolator-in-rust/4.png" srcset="/img/loading.gif" lazyload alt></p><p>对于 Client 的 get_timestamp 函数，参照上一小节实现发 RPC 和对应的重试以及错误处理逻辑即可。</p><p>对于 Client 的 begin 函数，使用 get_timestamp 函数更新本地的 start_ts 即可，start_ts 将作为快照读取的依据。</p><p>对于 Client 的 set 函数，直接缓存到 mem_buffer 中即可。</p><p>对于 Client 的 commit 函数，开始 Percolator 的提交流程，即先 prewrite 再 commit。注意不论是 prewrite 还是 commit 都是先对 primary 进行处理再对 secondary 处理。</p><p><img src="/percolator-in-rust/5.png" srcset="/img/loading.gif" lazyload alt></p><h4 id="proto"><a href="#proto" class="headerlink" title="proto"></a>proto</h4><p>参照论文写出了对应的 proto 文件如下，以下将分别分析四种 RPC 请求：<br><figure class="highlight plain"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs proto">message TimestampRequest &#123;&#125;<br><br>message TimestampResponse &#123;<br>    uint64 timestamp &#x3D; 1;<br>&#125;<br><br>message GetRequest &#123;<br>    bytes key &#x3D; 1;<br>    uint64 start_ts &#x3D; 2;<br>&#125;<br><br>message GetResponse &#123;<br>    bytes value &#x3D; 1;<br>&#125;<br><br>message PrewriteRequest &#123;<br>    uint64 start_ts &#x3D; 1;<br>    bytes primary &#x3D; 2;<br>    bytes key &#x3D; 3;<br>    bytes value &#x3D; 4;<br>&#125;<br><br>message PrewriteResponse &#123;<br>    bool success &#x3D; 1;<br>&#125;<br><br>message CommitRequest &#123;<br>    bool is_primary &#x3D; 1;<br>    uint64 start_ts &#x3D; 2;<br>    uint64 commit_ts &#x3D; 3;<br>    bytes key &#x3D; 4;<br>&#125;<br><br>message CommitResponse &#123;<br>    bool success &#x3D; 1;<br>&#125;<br></code></pre></div></td></tr></table></figure></p><h4 id="get-timestamp"><a href="#get-timestamp" class="headerlink" title="get_timestamp"></a>get_timestamp</h4><p>对于 TSO 请求，此时需要使用一个全局唯一 ID 的递增生成器而不能硬编码成 0 了。最暴力的方法便是采用 wall clock time 来生成，然而在高并发情况下 wall clock time 可能产生重复的 ID，因此一般需要 HLC 结合物理时间戳和逻辑时间戳的方式来保证全局唯一。出于时间原因，我直接采用了逻辑递增 ID 的方式来生成全局唯一的递增 ID。为了性能考虑，我采用了 Arc<AtmoicU64> 而不是 Arc<Mutex<u64>&gt; 来生成 id，这样实现的理论性能会更高，同时由于此处仅仅需要生成 ID 没有其他逻辑，因此 Ordering 采用 Relaxed 即可。<br><img src="/percolator-in-rust/6.png" srcset="/img/loading.gif" lazyload alt></Mutex<u64></AtmoicU64></p><h4 id="get"><a href="#get" class="headerlink" title="get"></a>get</h4><p>对于 Get 请求，需要携带 key 和对应的 start_ts 来针对某一全局快照读取。因此客户端构建好 request 后向服务端发送即可，注意此时依然实现了超时重试的逻辑。<br><img src="/percolator-in-rust/7.png" srcset="/img/loading.gif" lazyload alt></p><p>在服务端的 RPC handler 里，首先使用 lock 进入临界区，接着判断 lock 列中是否存在与本事务冲突的其他事务，如果存在则 backoff 等待并在稍后重试。接着查找 write 列中该 key 最接近当前事务 start_ts 的 commit_ts，如果不存在则返回空字符串，否则获取到其 start_ts 将其从 data 列读出来即可。<br><img src="/percolator-in-rust/8.png" srcset="/img/loading.gif" lazyload alt></p><p>在 KVTable 的 read 函数中，使用 BTreeMap 的 range 接口和 last 接口来获取某个区间的最大值，如果不存在则返回 None。<br><img src="/percolator-in-rust/9.png" srcset="/img/loading.gif" lazyload alt></p><h4 id="prewrite"><a href="#prewrite" class="headerlink" title="prewrite"></a>prewrite</h4><p>对于 PreWrite 请求，需要携带 start_ts，primary 和对应的 KV 对来进行写入。客户端添加了对应的重试和错误处理逻辑，并将参数发送给服务端。<br><img src="/percolator-in-rust/10.png" srcset="/img/loading.gif" lazyload alt></p><p>在服务端，首先使用 lock 进入临界区，接着判断 lock 列和 Write 列中是否存在与本事务冲突的其他事务，如果存在则返回错误，此时只能事务执行失败让应用层重试。如果没有冲突的事务，则可以向 data 列写入暂时不可见的数据，接着向 lock 列写入锁来保证不会出现 lost update。<br><img src="/percolator-in-rust/11.png" srcset="/img/loading.gif" lazyload alt></p><p>在 KVTable 的 write 函数中，使用 BTreeMap 的 insert 接口来新增对应的 entry。<br><img src="/percolator-in-rust/12.png" srcset="/img/loading.gif" lazyload alt></p><p>需要注意的是，该 Lab 中并未引入分片的概念，因此可以认为所有的数据均在一个分片，因此理论上可以直接使用 1PC 的方式将请求直接在服务端进行 PreWrite &amp; Commit 处理。本人也进行了 1PC 的实现，在实现完后发现存在一个测试并不希望按照 1PC 去处理，因而便采用了效率最低但和论文伪码以及测试能够对应的方式，每个 key 都会发一次 rpc 且 prewrite 和 commit 会发两轮 rpc。</p><h4 id="commit"><a href="#commit" class="headerlink" title="commit"></a>commit</h4><p>对于 Commit 请求，需要携带 start_ts, commit_ts，是否为 primary 以及对应要 commit 的 key。客户端添加了对应的重试和错误处理逻辑，并将参数发送给服务端。<br><img src="/percolator-in-rust/13.png" srcset="/img/loading.gif" lazyload alt></p><p>在服务端，首先使用 lock 进入临界区，接着判断 lock 列是否存在自己的锁，如果不存在则说明发生了不符合预期的错误（比如此次 RPC 延期到达，或者本事务已经被其他事务 abort 且清理），此时只能返回失败让应用层处理。如果自己的锁存在，则可以向 write 列写入 ((key, commit_ts), start_ts) 以让 data 列的数据可见，接着移除掉 lock 列写入的锁即可。<br><img src="/percolator-in-rust/14.png" srcset="/img/loading.gif" lazyload alt></p><p>在 KVTable 的 erase 函数中，使用 BTreeMap 的 remove 接口来移除对应的 entry。<br><img src="/percolator-in-rust/15.png" srcset="/img/loading.gif" lazyload alt></p><h4 id="测试结果"><a href="#测试结果" class="headerlink" title="测试结果"></a>测试结果</h4><p>以上逻辑通过了事务正常提交时的测试，然而网络异常或者协调者异常时的测试还未完全 AC，因此还需要进一步处理。<br><img src="/percolator-in-rust/16.png" srcset="/img/loading.gif" lazyload alt></p><p>通过本测试的新增代码可查看 <a href="https://github.com/OneSizeFitsQuorum/talent-plan/commit/5ca761e06f7eefe5da901ca2e37c7c3fb7c2a3b7" target="_blank" rel="noopener">commit</a></p><h3 id="TXN-异常测试"><a href="#TXN-异常测试" class="headerlink" title="TXN 异常测试"></a>TXN 异常测试</h3><ul><li>test_commit_primary_drop_secondary_requests</li><li>test_commit_primary_success</li><li>test_commit_primary_success_without_response</li><li>test_commit_primary_fail</li></ul><p>异常测试会模拟 commit 请求的 req/resp 被丢弃的场景，但是 primary 的 commit 会始终被满足，因而 secondary 的请求有可能会被丢弃。</p><p>在客户端侧，首先需要针对 test_commit_primary_fail 和 test_commit_primary_success_without_response 返回的错误进行特判处理，同样是发 rpc 出错，但他们一个要求 client 返回 Err(Error::Other(“resphook”.to_owned()))，另一个要求返回 Ok(False)。尽管并不统一，但可以通过修改特判 Err 类型的方式来通过测试<br><img src="/percolator-in-rust/17.png" srcset="/img/loading.gif" lazyload alt></p><p>在服务端侧，异常测试主要实现的函数便是 back_off_maybe_clean_up_lock，即当遇到其他事务遗留下来的 prewrite 结果时要如何处理。</p><p>对于 percolator 这种事务模型，primary key 的提交与否便是整个事务提交与否的标志。任何事务在读某一 key 时，如果遇到遗留的 Lock 列锁，在 sleep 超过 TTL 时间后，可以接着获取该冲突 key1 在 lock 列  key 中的 start_ts 和 value 中存的 primary 值。然后再去 Write 列中寻找 (primarykey，0) 和 (primarykey， u64::MAX) 范围内是否有指向 start_ts 的记录。如果存在，则说明该事务已经提交且能够获取到 commit_ts，此时对该 key1 做 commit 处理即可，即清理 Lock 列并在 Write 列添加对应的记录。如果不存在，则说明该事务尚未提交，且其他任何 rpc 再执行的时候都能够确定性的判断出该事务并未提交（即便是乱序到达的 primary  commit rpc，其也会检测 lock 记录是否存在，只有存在时才能 commit），此时只需要将当前 key1 的遗留 lock 清理即可。尽管也可以顺便检测清理其他的遗留 key，但让其他的遗留 key 在需要清理时再进行清理也不影响 safety，因而只用清理 key1 即可。在 key1 清理完之后，当前事务便可以正常读取 key 的值了。</p><p>代码如下，需要注意在进入 back_off_maybe_clean_up_lock 函数前需要 drop 掉锁，back_off_maybe_clean_up_lock 函数在 sleep 完之后需要先拿到锁再进行操作。<br><img src="/percolator-in-rust/18.png" srcset="/img/loading.gif" lazyload alt></p><p>contains_in_write_column 函数遍历 write 列寻找 (primarykey，0) 和 (primarykey， u64::MAX) 范围内是否有指向 start_ts 的记录，如果存在则返回 commit_ts，否则返回 None。<br><img src="/percolator-in-rust/19.png" srcset="/img/loading.gif" lazyload alt></p><p>此外 kvtable 中的 erase 函数的参数之前是 commit_ts，但由于在 lock 列中记录的是 start_ts，不论是正常 commit 还是 rollback 均需要按照 (key, start_ts) 去清理而不是 (key, commit_ts)，因而感觉这里应该是论文中的笔误被照抄过来了，在此处从 commit_ts 改为了 start_ts。<br><img src="/percolator-in-rust/20.png" srcset="/img/loading.gif" lazyload alt></p><p>通过本测试的新增代码可查看 <a href="https://github.com/OneSizeFitsQuorum/talent-plan/commit/8a62c5bc6096408433bfb0e0b6e04d9040b4082a" target="_blank" rel="noopener">commit</a>。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>最终所有测试通过如下，本人过关代码可参考该 <a href="https://github.com/OneSizeFitsQuorum/talent-plan/pull/1" target="_blank" rel="noopener">PR</a>。<br><img src="/percolator-in-rust/21.png" srcset="/img/loading.gif" lazyload alt></p><p>通过本 lab，对 Rust 编程的若干关键工具和重要知识点有了一定的实践，包括但不限于 cargo 管理，模式匹配，流程控制，错误处理，所有权与借用，迭代器，宏编程等等。希望未来能够深入理解 rust 的异步编程，成为一名有经验的 Rustacean。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;版本&quot;&gt;&lt;a href=&quot;#版本&quot; class=&quot;headerlink&quot; title=&quot;版本&quot;&gt;&lt;/a&gt;版本&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/talent-plan/tree/master/cour</summary>
      
    
    
    
    
    <category term="分布式系统理论" scheme="https://tanxinyu.work/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%90%86%E8%AE%BA/"/>
    
    <category term="Rust" scheme="https://tanxinyu.work/tags/Rust/"/>
    
  </entry>
  
  <entry>
    <title>分布式事务概述和对应代码框架介绍</title>
    <link href="https://tanxinyu.work/talent-plan-transaction-talk/"/>
    <id>https://tanxinyu.work/talent-plan-transaction-talk/</id>
    <published>2022-04-21T09:44:10.000Z</published>
    <updated>2022-04-27T09:51:24.000Z</updated>
    
    <content type="html"><![CDATA[<p>分享一下前两天在 <a href="https://tidb.net/talent-plan" target="_blank" rel="noopener">Talent Plan Community</a> 做的有关分布式事务和 Distributed-Txn 代码框架的介绍。</p><p>这次分享除了对 2021 VLDB summer school 中讲授的若干重要主题进行了概述，还着重介绍了事件排序这一很本质的问题，此外也参考了不少优质资料，现在 share 出来希望能对这块知识感兴趣的同学有帮助。由于本人水平有限，如有原理错误欢迎与我沟通~</p><p>注：以下仅为图片，可以在<a href="https://pingcap.feishu.cn/drive/folder/fldcn9zPuLSTqoL2JDQOT5jbpQd" target="_blank" rel="noopener">此处</a>在线浏览 PPT 原件和录屏。</p><p><img src="/talent-plan-transaction-talk/1.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-transaction-talk/2.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-transaction-talk/3.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-transaction-talk/4.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-transaction-talk/5.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-transaction-talk/6.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-transaction-talk/7.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-transaction-talk/8.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-transaction-talk/9.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-transaction-talk/10.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-transaction-talk/11.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-transaction-talk/12.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-transaction-talk/13.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-transaction-talk/14.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-transaction-talk/15.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-transaction-talk/16.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-transaction-talk/17.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-transaction-talk/18.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-transaction-talk/19.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-transaction-talk/20.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-transaction-talk/21.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-transaction-talk/22.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-transaction-talk/23.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-transaction-talk/24.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-transaction-talk/25.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-transaction-talk/26.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-transaction-talk/27.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-transaction-talk/28.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-transaction-talk/29.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-transaction-talk/30.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-transaction-talk/31.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-transaction-talk/32.png" srcset="/img/loading.gif" lazyload alt></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;分享一下前两天在 &lt;a href=&quot;https://tidb.net/talent-plan&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Talent Plan Community&lt;/a&gt; 做的有关分布式事务和 Distributed-Txn 代码框架的</summary>
      
    
    
    
    
    <category term="分布式系统理论" scheme="https://tanxinyu.work/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%90%86%E8%AE%BA/"/>
    
    <category term="分享" scheme="https://tanxinyu.work/tags/%E5%88%86%E4%BA%AB/"/>
    
  </entry>
  
  <entry>
    <title>Raft 算法和对应代码框架介绍</title>
    <link href="https://tanxinyu.work/talent-plan-raft-talk/"/>
    <id>https://tanxinyu.work/talent-plan-raft-talk/</id>
    <published>2022-03-02T02:10:42.000Z</published>
    <updated>2022-04-27T03:42:24.000Z</updated>
    
    <content type="html"><![CDATA[<p>分享一下前不久在 <a href="https://tidb.net/talent-plan" target="_blank" rel="noopener">Talent Plan Community</a> 做的有关 Raft 算法和 Etcd/TinyKV 代码框架的介绍。</p><p>这次分享对 Raft 算法和对应实现做了较为系统的调研整理，不仅对若干经典问题做了介绍，也提供了不少优质参考资料，现在 share 出来希望能对这块知识感兴趣的同学有帮助。</p><p>注：以下仅为图片，可以在<a href="https://vevotse3pn.feishu.cn/file/boxcn6tQX2I5QxyLM4ZGGVu2wmd" target="_blank" rel="noopener">此处</a>在线浏览 PPT 原件。</p><p><img src="/talent-plan-raft-talk/1.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-raft-talk/2.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-raft-talk/3.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-raft-talk/4.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-raft-talk/5.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-raft-talk/6.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-raft-talk/7.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-raft-talk/8.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-raft-talk/9.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-raft-talk/10.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-raft-talk/11.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-raft-talk/12.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-raft-talk/13.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-raft-talk/14.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-raft-talk/15.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-raft-talk/16.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-raft-talk/17.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-raft-talk/18.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-raft-talk/19.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-raft-talk/20.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-raft-talk/21.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-raft-talk/22.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-raft-talk/23.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-raft-talk/24.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-raft-talk/25.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-raft-talk/26.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-raft-talk/27.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-raft-talk/28.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-raft-talk/29.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-raft-talk/30.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-raft-talk/31.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-raft-talk/32.png" srcset="/img/loading.gif" lazyload alt><br><img src="/talent-plan-raft-talk/33.png" srcset="/img/loading.gif" lazyload alt></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;分享一下前不久在 &lt;a href=&quot;https://tidb.net/talent-plan&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Talent Plan Community&lt;/a&gt; 做的有关 Raft 算法和 Etcd/TinyKV 代码框架的介</summary>
      
    
    
    
    
    <category term="分布式系统理论" scheme="https://tanxinyu.work/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%90%86%E8%AE%BA/"/>
    
    <category term="共识算法" scheme="https://tanxinyu.work/tags/%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/"/>
    
    <category term="分享" scheme="https://tanxinyu.work/tags/%E5%88%86%E4%BA%AB/"/>
    
  </entry>
  
  <entry>
    <title>2021 年终总结：记我在清华 Apache IoTDB 组的成长</title>
    <link href="https://tanxinyu.work/2021-annual-summary/"/>
    <id>https://tanxinyu.work/2021-annual-summary/</id>
    <published>2022-01-24T15:37:53.000Z</published>
    <updated>2022-02-06T01:56:35.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>2021 年即将结束，这一年忙忙碌碌收获许多，也认识到了定期总结定期反省的重要性。今年回家后意识到自己应该养成写年终总结的习惯了，于是断断续续用近一周的时间写了第一次年终总结，算给自己的一整年一个交代。希望我的经历和感悟能给大家一些启发。</p><p>首先进行一个简单的自我介绍，我叫谭新宇，清华本硕，现在清华大学软件学院 Apache IoTDB 组就读研二，师从王建民/黄向东老师，我对分布式系统，时序数据库和共识算法较感兴趣。</p><p>接着简单介绍一下我们组的工作：Apache IoTDB（物联网数据库）是一体化收集、存储、管理与分析物联网时序数据的软件系统。 Apache IoTDB 采用轻量式架构，具有高性能和丰富的功能，并与 Apache Hadoop、Spark 和 Flink 等进行了深度集成，可以满足工业物联网领域的海量数据存储、高速数据读取和复杂数据分析需求。截止 2021 年底，Apache IoTDB 是国内高校唯一在 Apache TLP 中的开源软件，目前已经被建设成了百余贡献者的开源社区。 </p><h2 id="本科经历"><a href="#本科经历" class="headerlink" title="本科经历"></a>本科经历</h2><p>既然是第一次年终总结，在此简单提提自己的本科经历。</p><p>2016 年，高考完填清华志愿的我并不知道自己的专业兴趣是什么，也并不知道不同的专业有什么区别，于是随便报了个机械大类便入学了。</p><p>在大一下的时候，一方面是成绩尚可（前 10%），另一方面是大一下接触编程课的印象还不错。对游戏一直很感兴趣的我便萌生了转专业的想法，最终也顺利的平转到了对编程萌新相对友好的软件学院。</p><p>在大二刚转过来的时候，我感受到的课程压力是非常大的，当然主要原因还是自己的编程基础太差。因为大学之前从未接触过编程，而且大一只学了一门针对机械学院同学开的比较水的编程课，所以大二学数据结构操作系统的时候非常吃力。幸运的是，软件学院的大佬同学们（教主，杰神，方舟，冯老板，伟哥等等）和辅导员们（岳导，李导）帮助了我很多，再结合自己的努力，我的成绩也慢慢能够跟上来了。</p><p>在大三的时候，得益于学院这个大平台和自己的努力，我分别在旷视和微软进行了短期实习。在旷视，我主要在 Data 组打杂，也是在这个时候开始了解分布式系统领域，学习了 Google 的三篇马车（GFS/MapReduce/BigTable）和 Facebook 的小文件冷热存储解决方案（HayStack/F4）并逐渐对这个领域产生了浓厚的兴趣。此外，我在旷视的 mentor 庄大师（外号纯纯）也给我留下了非常深刻的印象，现在他已经是成功人士了（手动狗头）。在微软，我们跟着邹欣老师（现在已经跳槽到 CSDN 当副总裁了）组队完成了一个 NLP 领域的 CodeSearch 项目，主要功能是实现一个 VSCode 插件，能够输入自然语言输出对应的代码，这个工作还拿到微软内部 Hackathon 比赛的奖。不过我的任务主要是爬虫数据清理之类的工作，一方面是自己对 AI 不是很感兴趣，另一方面也是自己炼丹的能力不足。这段经历到现在留给我的印象就是微软内部和谐友好的氛围了，是真的很 WLB 啊。</p><p>到了大四做保研还是工作的决定时，有很厉害的学长劝我早点到工业界，说很多研究生的工作都是在虚度年华，贵清也不例外。我的父母则是坚持要求我接着读书。在平衡各方面利弊后，我最终决定继续读个硕士，给自己一些缓冲的时间，并希望能够找到一个不虚度年华的实验室。</p><p>最开始我找到了李振华老师的实验室，主要原因是振华老师在上本科课程《网络系统》时实在是太过于风趣幽默，吸引了一大批小学弟。在振华组里最开始我是希望能够做一段有意义的科研的，然而呆了半年后我逐渐感觉到，我所感兴趣的分布式系统方面振华老师做的工作不多，我自己也很难独立的搞出一份科研工作出来。此外，振华老师喜欢的学生特质是英文写作能力强，基础扎实，对计算机网络/操作系统感兴趣，而当时的我与这三个特质都不太搭边，因此我的离开便不是很意外了。不过 u1s1，振华老师写论文的功底真的非常强，MobiCom，NSDI 年年发，21 年他们组还中了软院第一篇 SIGCOMM，可谓是风光无限了。尽管会比较 push，但好歹 push 的结果对大家都好，满足上述三个特征的同学也可以去跟振华老师联系。</p><p>当时，我和同在振华组里的苏总相约好了一起润，我们几乎打听了学院的所有实验室，在过滤掉不感兴趣的搞前后端和 AI 的实验室后，我们最后面临了两个选择，一个是散养的实验室，去了之后可以在企业自由的实习三年。一个是学院里在做的时序数据库 Apache IoTDB（当时还是孵化器项目），尽管去了之后会比较忙，但由于主力是学生，相比在外实习可能能得到更多的锻炼，而且硕士期间也能够拥有三年的数据库内核开发经验，对找工作比较友好。最终经过深思熟虑，我们相约一起润去了 Apache IoTDB 实验室。</p><p>进组后，我当时给自己的目标便是利用研究生三年时间提升自己的理论知识水平和工程能力。因而我一进去就开始了解开源，学习数据库知识并加入了分布式模块进行实践。做了不到一年，2020 年底我便被社区接纳为了 committer，这里也很感谢组里分布式模块的实力大哥江天学长的举荐。</p><p>如果让我在 2020 年底反思读研是否值得，我可能还很难给出明确的答案。但如果让我现在反思读研是否值得的话，我的答案一定是值得。这与我进实验室后的经历有关，我不确定当时直接工作的话在企业是否也能得到这样的成长，但至少 2021 年在实验室的成长已经让我觉得读研不虚此行了。其实仅仅深度参与一个开源基础软件从雏形不断发展壮大再到可能的商业化，感受项目管理，产品功能，人员心态，宣传运营，社区交流等方面的进化就已经是非常难得的一段经历了，更不要谈在这期间借助这个平台我们自己的成长了。</p><p>当然，读研到底是不是正收益。家庭情况，实验室情况，个人兴趣，时代发展，专业方向，人脉关系这些都是影响很大的因素，不可一概而论。</p><h2 id="2021"><a href="#2021" class="headerlink" title="2021"></a>2021</h2><p>讲完了之前的经历，在这里谈谈 2021 年的经历。如果要给 2021 年的自己一个关键字的话，我觉得应该是<strong>成长</strong>。</p><p>卷了一整个本科的我在研究生选课时遵循了兴趣导向的原则。即对于自己不感兴趣的课水水即可，对于自己感兴趣的课付出大量精力。前半年，我了解到了贵系陈康老师的《分布式导论》课程，这门课基本 Follow 了 MIT 6.824 的课纲，大作业也是完成 6.824 的 lab。尽管听起来就很硬核，但我毅然决然的选了它。在上这门课的时候，我一方面同步听了线下陈康老师和线上 MIT Robert Morris 老师的课程，另一方面花费了大量的精力去完成大作业，尽管大作业不要求完成 challenge 的内容，但我还是完成了 lab1-4 和 challenge 1-2 的全部内容，最终我如愿以偿的拿到了 A+ 的成绩，这门课也成为了我在贵清收获最大的课。现在想想，这一门课上下来，我阅读了近 20 篇经典论文，学习了很多分布式经典理论，包括各种一致性级别，CAP，BASE，FLP，共识算法，偏序/全序关系，若干系统架构等等，基本重铸了分布式系统的知识体系。此外，我也将我的大作业文档在 Github 上进行了开源，短短半年已经收获近 300 个 star，新增 160+ follower。而且我在知乎上也给自己的文档打了广告，半年收获了 13 万+ 阅读，500+ 赞，1200+ 收藏，成为了”如何的才能更好地学习 MIT6.824 分布式系统课程？”排名第三的答案（第一是 PingCAP 的黄东旭老师）。通过这些关注，我在这半年里也认识了很多志同道合的朋友，这是一生的财富。</p><p>7 月，刷完 6.824 收获很大的我又开始关注另一门课 CMU 15-445，想要就此机会认真学习一下数据库理论。用时一个多月，我利用晚上和周末的时间学习完了 15-445 的线上课程并被 Andy 老师的个人魅力深深吸引。无论是上课时的 DJ drop table，还是 Andy 老师对数据库知识的热情都让我印象深刻。通过这门课程，我基本了解了如何实现一个支持 ACID 的单机关系型数据库。略感遗憾的是，其大作业 bustub 的 codebase 是 C++，我一方面对 C++ 没有技术积累，另一方面也对自己的代码有一定的洁癖，如果我不足够了解 C++ 的话，我也不想写出一堆很丑陋的 lab 代码出来。主要是基于这个原因，我放弃了刷 bustub，换为 MIT 6.830 数据库课程的 lab （codebase 是 Java）刷了刷，但后来刷到 lab2 便由于时间原因被搁置了，之后有机会的话希望能重捡起来。</p><p>8 月，经过一年多的开发，我们终于迎来了 Apache IoTDB 分布式版本的第一个线上用户，针对于 20TB 每天的入库流量，我们用 3 节点的分布式 IoTDB 集群替代了 20 节点的 HBase 集群， 经过初步统计，预约未来 5 年能为企业节省上百万的硬件成本。尽管在上线前的内测期间我们遇到了不少问题，但我们顶住了压力并都进行了修复，最终也顺利上线。通过这次经历，我不仅进一步认可了自己在做的工作，也理解了能为用户创造实际价值才是项目存亡的关键。</p><p>8 月底，对 TiDB 架构一直感兴趣的我发现 PingCAP 提供了免费的 PCTA （TiDB 系统管理基础能力认证）考试机会，于是看了看文档考了一个 PCTA 证书玩玩。</p><p>在暑假，我参加了中科院组织的开源之夏活动，题目为 《Apache IoTDB 分布式混沌测试框架》。这份工作基于阿里的 ChaosBlade 搭建了一个混沌测试平台并对 Apache IoTDB 进行了若干种异常 case 的混沌测试。通过测试，我们发现了 Apache IoTDB 当前分布式版本在异常环境下存在的一些问题，有一些容易解决的问题已经得到了修复，然而也有一些较复杂的问题到今天依然存在，这也多多少少间接引起了我们的一次大规模重构，勉强算是一件有意义的工作吧。令人略感遗憾的是，尽管该混沌测试框架在部署好之后可以用 Dashboard 的方式方便地注入特定的异常，然而该框架依然是基于物理节点来实现的，很难做到自动化。如果没有测试人员去维护并定期手动测试，如果没有开发人员愿意抽出时间来完全解决其中发现的问题，如果整个团队没有足够重视异常场景下系统的对外表现并愿意为之付出大量的精力，该框架就很难形成正向反馈，最终只能被遗忘在历史的角落里。作为一点反省，我现在觉得混沌测试还是应该尽可能的通过持续集成的方式自动化起来（参照 ChaosMesh），这样释放人力的方式是大家都喜爱的，也只有这样，混沌测试才能对项目产生持续的正向收益。</p><p>9 月，打算硕士开题的我阅读了 Raft 相关的近 20 篇论文，感觉近几年与 Raft 相关的论文主要还是在区块链，新硬件和一些特定场景的优化上，没有太多本质的变化。</p><p>10 月，抱着学习的心态，我和家贝一唱报名参加了第一届九坤并行程序优化大赛，这个比赛主要考验选手最大化压榨 CPU 和 IO 性能的能力。由于赛题是 C/C++ 的 codebase，然而我们基本都对 C/C++ 不太熟悉，于是我们起名叫做了”只会 JAVA 队”。作为三个在体系结构几乎一窍不通的小白，在一个多月断断续续的不到 10 次线下沟通中，我们逐渐对体系结构入了门，在 192 个队伍脱颖而出，并在决赛取得了第 4 名的成绩（离苹果周边只差一步）。个人认为，数据库做到极致就是硬件性能的一种体现。因此，一个优秀的数据库工程师应该对体系结构具有一定的了解，这样才有可能进一步压榨硬件性能，从而达到更好的数据库性能。一直以来，我希望分布式数据库能够成为自己的一个标签。通过这次比赛，我意识到高性能计算也是一个很有趣且硬核的方向，其不仅能够给企业迅速带来真金白银的收益（节约成本），而且也是很多领域做到极致的一种出路。</p><p>11 月，PingCAP 举办了第一届 Talent Plan KV 学习营。我和汉乙组队参与了这次比赛，由于我们俩之前都刷过 MIT 6.824，已经对教学级别的 raft 有一定的了解，所以参加此次比赛的目的就是去感受一下生产级别分布式 KV 的代码实现，学习实践一下 lsm, etcd, raftstore 和 percolator 的理论知识和 codebase。u1s1，刷 lab 的过程十分曲折，我们俩所在的实验室到年底的时候都非常忙，前几周基本每周都只能抽出顶多一两天的时间来写代码，而理解 lab2b/lab3b raftstore 的难度是非常大的，我们用了一周多的时间才勉强看懂 raftstore 的代码。这使得到还剩两周时间的时候，我们才刷到 lab2c。最后两周我们利用中午午休时间和晚上睡觉时间疯狂加班，在 lab 上花了更多的时间，最后才堪堪刷完。出人意料的是，我们得了第二名的好成绩。事后反省一下，在 safety 上我们遇到的问题都解决了；在 liveness 上我们没投入太多精力；在文档上，我们简单介绍了代码实现，但将重点放在了我们对相关知识的理解和思考上；在性能上，我们重点做了最容易做的 batching 优化，其本质上是使用 raft 的优化而不是 raft 自身的优化，但对性能的提升却异常关键，比如 tidb 对于一个事务打包的一堆写请求，到 tikv 的 region 之后，这些写请求同步成一条还是多条 raftlog 对于性能的影响是巨大的。</p><p>年底，我和祥威思屹其骏受到实验室的支持，前去参加了海口的 2021 VLDB summer school，今年的 topic 是分布式事务。于向遥，吴英骏和魏星达老师清晰地介绍了分布式事务的方方面面，李飞飞和黄东旭老师则是分享了很多工业界的思考。虽然 KeyNote 请了 Andy 过来我也很喜欢 Andy 老师，但 u1s1 topic 和分布式事务不搭边，感觉基本是 Andy 老师在狂荡不羁的给 ottertune 打广告哈哈哈。这次 VLDB 第一次采用了理论 + 实践 + 随机组队的方式，我们除了每天早上要去感受学术界的熏陶外，下午还会去学习 PingCAP 工程师有关实践 lab 的 talk。我们的 lab 要实现 tinykv + tinysql 中有关分布式事务的所有部分。由于我之前已经了解过 tinykv 的 codebase，所以我的 lab 完成的很顺利，我也尽我所能在下午的实践课程中帮助了很多不了解 codebase 的其他组同学，这也使得我最终拿到了积极参与奖的荣誉。此外我们组由于做了 3 份独立的作业而且最后汇报的表现也还不错，最终成为了排名第一的组，我也幸运的拿到了优秀学员的荣誉并领了一堆 PingCAP 周边。当然对于我来说收获最大的还是跟大佬们的交流。我有幸代表学员跟周傲英老师简单分享了这次 VLDB summer school 的学习方式。我也在吴英骏老师课后跟其请教了应该怎么看待 serializable 和 linearizable，实际上前者是在说事务之间的隔离性，并不需要具备满足时序的偏序关系，而后者是在说单个对象读写操作间的可见性，需要严格满足时序的偏序关系。如果将事务看成单个对象来理解，事务之间又有隔离性又能够满足时序的偏序关系，则就可以被称为 strict serializable（也被 Google Spanner 称为 external consistency）。黄东旭老师在 KeyNote 上很直白的分享了很多工业界分布式事务的真实挑战，比如超大事务（100GB+），Online DDL （有 Google F1 Online DDL 可能还不够）等，在课下交流时，东旭老师则觉得将复制和共识解耦很可能是一件有意义的工作。我们是不是一直以来用错了共识算法？即共识只应该是共识索引而不是共识数据，比如对于 100GB 的数据，是不是在架构上将其写到一个共享存储/内存池上，然后将轻量的索引共识即可。此外我还跟 PingCAP 的童牧老哥请教了不少问题，主要问题集中在 SEDA 模型和 TPC 模型的优劣上。TiKV 利用 rust 在应用态做了非公平业务感知的 CPU scheduler，使得大数据的聚合查询和小数据量的单点查询在高并发时后者的延时依然稳定，感觉是非常有意义的工作。除了技术方面的交流外，王岩广老师介绍的 Talent Plan 社区也让我印象深刻。就我所看到的情况而言，PingCAP 在国产数据库人才培养方面付出了大量的努力并且成果斐然，在这里真诚感谢一下 PingCAP 社区。</p><p>除了参加一些活动学习一些课程，这一年我在 Apache IoTDB 社区 Review 了近 140 个 PR，提交并被合并了 47 个 PR，但大多都是一些维护性重构性的工作。说来也惭愧，进组一年了直到今年后半年我才开始认真研究时序数据库并逐渐意识到了它的挑战。最后一个季度，我和思屹调研了若干个国产时序数据库的分布式架构并进行了多次分享，从中发现了许多优点和大家依然没有解决的痛点。这些优点和痛点的解决方案我们已经进行了大量的讨论和分析，这期间东哥，乔老师，田原学长，金锐学长，江天学长，荣钊学弟，洪胤学弟等也都结合一些论文和自己的思考提出了不少很有特点的想法。这些工作目前正在被逐步吸收进 Apache IoTDB 的分布式版本里面去。尽管短期内还没有成效，但我对 2022 年充满信心。</p><p>后半年，我们实验室引入了在管理方面很有经验的刘海老师，对我们实验室的管理模式进行了一系列改造，包括但不限于分组管理和汇报，利用 sprint 管理进度，利用 confluence 管理文档，目标驱动等等。整个实验室的管理模式发生了翻天覆地的变化，我也有幸成为了一个 3 人小组的小组长，负责分布式模块的维护和推进工作。实际上我们实验室由于一直在搞开源基础软件，所以学生的工程能力一般都还不错。今年引入成熟的管理模式之后，每个人都感受到了管理的力量，也能够在毕业之前提前学习到一些有关管理的知识。长远来看，这对于学生的成长是非常有帮助的。在企业里，调动积极性还可以通过期权，职级等手段来实现。在学校里，调动积极性就只能靠个人魅力和让大家都得到成长的能力了。在我个人看来，这是更富有挑战的。</p><p>之前我觉得在实验室里追求个人的发展和完成组里的工作一定程度上是冲突的，我也就此事跟刘海老师沟通过。渐渐的，我意识到这两者并不一定冲突，它反而促进了我对于高效率的追求。今年我参加的课外比赛和线上学习的课程基本都是利用晚上和周末的时间完成的，并没有影响实验室的工作，在上午和下午的工作时间我依然会专心的进行实验室的工作。由于时间紧张，我会更加专注于手头上的工作，减少摸鱼时间，并不断反思总结如何更高效的工作和交流。从结果来看，思路的转变使我在相同的时间内得到了更多的成长。当然，代价就是牺牲了一些休息时间。</p><p>今年我写了大约 25 篇技术博客，开始阶段性地记录自己的学习过程，字数合计在十万以上。我觉得分享的本质是让大家变得更好，所以我会尽可能的写一些技术总结或者思想感悟，希望读者看了后有所收获。通过博客，我也认识了不少新朋友，希望自己之后能继续坚持下去。</p><p>今年我抽了一些休息时间断断续续看了一些比较感兴趣的电视剧《朱元璋》，《走向共和》和《天道》，也阅读了《邓小平时代》，《原则》和《人生的智慧》等书籍。尽管感悟不少，但它们的主题都比较深沉，我也常常会陷入悲观历史主义中去。我特别感谢我的女朋友小杨同学能够纠正我一些过于悲观的思想，能够让我始终燃着对生活的热爱。如果自己能多抽出一些时间陪陪她就好了。</p><p>当然，以上都是有好结果的经历，今年我也有很多失败的经历，包括但不限于没刷完 6.830 lab，没好好参加 OceanBase 数据库比赛，没去成字节跳动暑期夏令营，没有坚持刷 leetcode，没有完全解决对 IoTDB 进行混沌测试后发现的问题等等。</p><p>除了自己的成长外，这一年 Apache IoTDB 也在我们团队的努力下成长了很多，不论从功能还是性能上都有了很大的提升。当然最重要的还是社区的良性成长：从 2020 年的 396 人增长到 1532 人，国内社群用户数量较 2020 年增长超 287 % ！目前已经有 162 位贡献者为 IoTDB 主仓库贡献了代码，从 2020 年的 94 增长到 162，相比 2020 年初增长了近 70 人！目前已经有多家公司深度参与到 IoTDB 的开发中，如东方国信、阿里、云智慧、360、用友、华为、中冶赛迪等等。学生群体方面，去年一年新增了来自清华、北大、北航、西北工业大学、复旦大学、南京大学、厦门大学、威斯康星大学、新加坡国立大学等国内外高校学生的身影，更有同学选择 IoTDB 作为他们的毕业设计方向。在 IoTDB，我们不仅在个人能力上得到了锻炼，也通过社区认识了很多很厉害很友善的新朋友。</p><h2 id="一些感悟"><a href="#一些感悟" class="headerlink" title="一些感悟"></a>一些感悟</h2><p>谈完了这么多经历，也想谈谈自己的感悟，这些感悟不一定适用于每个人，但都是我个人在今年得到成长的诀窍，一年前的我要是能明白这些道理也许就能少踩很多坑了。</p><h3 id="从要我干什么到我能干什么"><a href="#从要我干什么到我能干什么" class="headerlink" title="从要我干什么到我能干什么"></a>从要我干什么到我能干什么</h3><p>今年感觉自己认知转变最大的一点就是从”要我干什么”过渡到了”我能干什么”。认知的变化彻底改变了我思考问题的方式。</p><p>我不再荒废太多时间在娱乐上，而是思考自己当前能做什么有意义的事情。我不再将一些工作视为是锅，而是将其视为成长的机会。我不再低头闷声学习，而是开始热衷于交流，反思，请教和提问。我不再认为自己当前仅仅专注技术就够了，而是开始学习一些有关管理，表达，领导力的技巧。</p><p>读研三年，仅仅拿个学位在我看来还是有些虚度年华的。在未来我希望自己能够继续跳出自己的舒适圈，继续折腾和挑战自己。</p><h3 id="人与人最大的区别是认知"><a href="#人与人最大的区别是认知" class="headerlink" title="人与人最大的区别是认知"></a>人与人最大的区别是认知</h3><p>这个感悟也是今年体会特别深的一点。人与人最大的区别不是出身，不是社会地位，而是认知。同样的一件事，认知不同的人会有不同的看法，有的人视之为机会，有的人视之为灾祸，最后当然会有不同的结果。</p><p>今年有幸跟很多大佬都请教过，他们对这个世界和行业的认知都让我印象深刻。我的大导师清华大学软件学院院长王建民教授就常会来我们组跟我们讲一些做研究做工程的方法论。在这样的熏陶下，我们组同学们对行业的认知也在不断的提高。</p><p>就我个人的想法而言，多跟大佬们请教，怀着开放的心态去学习，把批评当做进步的机会就是提升自己认知的绝好方式。</p><h3 id="找准自己的兴趣点并持之以恒的专注"><a href="#找准自己的兴趣点并持之以恒的专注" class="headerlink" title="找准自己的兴趣点并持之以恒的专注"></a>找准自己的兴趣点并持之以恒的专注</h3><p>今年的 VLDB summer school 有一个数据库人才培养论坛，交流的大牛有北大的崔斌老师，人大的陈红老师，浙大的陈刚老师，华东师大的钱卫宁老师，阿里的李飞飞老师和 PingCAP 的黄东旭老师，期间有这样一个问题”作为新一代的数据库人，现在应该怎么做才有可能在未来成为一个在数据库界有影响力的人？”。各位老师都分享了自己的看法，包括但不限于努力，专注，思考，交流等等，但所有老师均认为保持长期专注是很重要的一个特性：只有长期的专注才有可能做到顶尖，这一点上没有捷径。</p><p>这个回答也引起了我强烈的共鸣，不论是在学术界还是在工业界，很重要的一点就是找准自己的兴趣点。一定要找到一个自己感兴趣的方向，通俗点来说，就是找到那种自己在休息时间也愿意去学习去钻研的兴趣点。这样，我们不仅在工作学习时更有热情，而且自己的价值也会随着长期地专注而得到不断增长。</p><h3 id="拒绝无效内卷"><a href="#拒绝无效内卷" class="headerlink" title="拒绝无效内卷"></a>拒绝无效内卷</h3><p>尽管长期地专注很重要，但无效内卷是不可取的。从长远来看，无效内卷会影响我们的工作热情并破坏团队氛围，最终一定是得不偿失的。不同人对”无效内卷”的评判标准不同，但我觉得每个人都应该有意识的去反思自己到底是不是在无效内卷。如果是，那一定要想到缓解甚至拒绝它的方法，包括但不限于和 leader one-one，润等。世界这么大，要相信自己一定能够找到立足之地，没必要恶心自己。</p><p>这两天有一个比较火的新闻是腾讯企业微信的应届生怒怼领导恶意内卷。说实话看新闻看得我热血沸腾，我也很佩服这位老哥，要是我自己可能就默默润了，然而他却敢于发声，这是非常难能可贵的。祝愿他未来一切顺利。</p><h3 id="脑袋决定屁股，屁股驱动行为"><a href="#脑袋决定屁股，屁股驱动行为" class="headerlink" title="脑袋决定屁股，屁股驱动行为"></a>脑袋决定屁股，屁股驱动行为</h3><p>在其位谋其职，任何组织都有光明的一面和黑暗的一面。屁股在哪就要努力让它变得更好，而不是见到黑暗面后就只知道诋毁和抱怨。从长远来看，后者最终只会损坏自己的名誉浪费自己的时间，得不到什么好处。</p><p>当然，如果经过自己的努力依然无法改变任何东西，那就用脑袋挑选一个更符合自己价值观的位置去坐吧。</p><h3 id="营造团队氛围，每个人都有责任"><a href="#营造团队氛围，每个人都有责任" class="headerlink" title="营造团队氛围，每个人都有责任"></a>营造团队氛围，每个人都有责任</h3><p>今年阅读了桥水基金创始人瑞·达利欧的《原则》这本书，我被其中描述的创意择优像家一样的工作氛围所深深吸引，幻想着自己未来能到这样的公司工作。</p><p>然而，我和很多读者一样都进入了相同的思维误区，那就是等着别人来营造这种氛围而我们坐享其成。既然每个人都喜欢这样的氛围，那么为什么不能在自己力所能及的范围内行动呢？</p><p>在我们实验室的三人分布式小组里，我作为小组长和两位学弟就努力在组内营造了这种氛围。大家有不错的学习资料就相互分享，有棘手的脏活就平分来干，功能按照兴趣去推动，bug 按照难易尽量平均分配，有人周中有事的话其他人可以暂时帮忙顶住工程压力，而他在忙完之后往往周末也会主动加班赶上落下的进度。在这期间，我也与学弟们建立了良好的私人关系，这样的氛围让我觉得开心，也让我进一步确信每个人都可以去营造团队氛围。</p><p>尽管营造团队氛围每个人都有责任，但我还是觉得考虑自己职业规划时一定要优先考虑关注员工成长的公司，否则推进这种氛围可能会非常难，这会进而影响自己的热情和产出。我个人是非常不认同企业付高工资就可以将工程师当做工具人来使用的企业文化的，员工和项目只有共同成长才有可能建立一个长期高效持续创新的团队，也只有这样的团队才值得大家齐心协力为之奋斗。</p><h3 id="真诚坦率"><a href="#真诚坦率" class="headerlink" title="真诚坦率"></a>真诚坦率</h3><p>《原则》这本书里谈到了真诚坦率是很难得的高价值品质，它带给我们的收益往往比我们想象的还要大。</p><p>当我们能够真诚坦率的公开我们的工作内容和进度时，一方面会使我们更容易获得上级和同事的理解信任，另一方面也会促使我们减少摸鱼时间，更加重视自己的单位时间产出并努力提高自己的工作效率，而后者在我看来是更关键的。人都会存在惰性，这样的机制可以帮助自己克服惰性，追求效率至上。</p><h3 id="不要妄想别人帮自己指出最优解，路都是自己走出来的"><a href="#不要妄想别人帮自己指出最优解，路都是自己走出来的" class="headerlink" title="不要妄想别人帮自己指出最优解，路都是自己走出来的"></a>不要妄想别人帮自己指出最优解，路都是自己走出来的</h3><p>我在振华老师组的时候曾经请教他让他为我指一条发展最快的捷径。振华老师当时说他也不确定我走哪条路是最快的捷径，只能结合我自己的状态一步步走着看。</p><p>当时的我还不能够理解，现在的我逐渐明白了：对于大部分人来说，路都是要自己一步步走出来的，不要妄想有个大佬能直接给自己指出全局最优的捷径。保持开放的心态，一直学习一直反思，多向大佬们请教，能走出一条局部最优解的路就已经很不错了。</p><h3 id="养成定期反思的好习惯"><a href="#养成定期反思的好习惯" class="headerlink" title="养成定期反思的好习惯"></a>养成定期反思的好习惯</h3><p>犯错误不可怕，不反思错误的原因并进行针对性的分析和改进就很可怕了。</p><p>随着见识的增长，我逐渐意识到定期反思总结对于个人的发展至关重要。因此我们小组除了每周同步两次工作进度以外，每周每月还会做反思总结并在组间进行分享和互相评价，这些总结内容包括但不限于工作总结，个人发展，自我反思和想法建议等。每月抽一个小时反思下存在的问题，不论是自身的还是团队的，一经分享讨论下个月就可能已经进行了改进，这形成了一个正反馈效应，对大家都是有益无害的。</p><p>通过这些分享，我们每个人都在逐渐的进步，这些进步包括但不限于代码设计，时间管理，汇报技巧，工作方式，做事思维等等。就跟当年高考刷题一样，悲观的人认为题量是无尽的，怎么刷也刷不完；乐观的人认为题量是有尽的，刷一道少一道。我是乐观的人，我坚信这些定期反思加上我们的长期专注一定会迎来收获。</p><h3 id="了解技术细节最好的时机有两个，一个是过去，一个是现在"><a href="#了解技术细节最好的时机有两个，一个是过去，一个是现在" class="headerlink" title="了解技术细节最好的时机有两个，一个是过去，一个是现在"></a>了解技术细节最好的时机有两个，一个是过去，一个是现在</h3><p>在写代码的时候我们常会碰到一些技术细节（比如 git 命令，比如不同 IO 的区别），有些人认为这些细节无关紧要将其置之不理，而有些人非常重视这些细节并将其很快学懂。我觉得面对这些细节问题的态度是决定技术水平是否会停滞不前的重要因素。</p><p>了解技术细节最好的时机有两个，一个是过去，一个是现在。当遇到技术细节时，可以简单评估一下学习成本。如果很低（小于 1 小时），则可以在当天尽量将其搞明白。如果比较高，则可以将其放在预计学习的列表里面，定期抽出一些整块的时间集中学习。</p><p>诚然，在刚开始编程的时候就这样会比较累，因为什么都不会。但只要能够长期坚持学习，遇到不懂技术细节的概率会越来越低，自己的水平也会越来越高，形成的正反馈效应也会强化这一过程。</p><h3 id="不要给自己设限"><a href="#不要给自己设限" class="headerlink" title="不要给自己设限"></a>不要给自己设限</h3><p>在大型基础软件里，往往不同的人会负责不同的模块。如果模块解耦做的比较好的话，可能不同模块的同学不会有太多交流。比如在刚进组的时候我就只关注了分布式模块的内容，有关单机任何模块的功能和 bug 修复我基本都不会关注，这也使得我进组都一年了还不太了解时序数据库是什么。现在回想当时的自己还是太傻了。</p><p>之前振华组里的明亮学长在做秋招分享时介绍到，他认为互联网跳槽之所以容易涨薪涨职级，就是因为跳槽人往往对其所在组工作具有比较深刻的认知，这些认知可能是几十人的团队踩了数年的坑才提炼出来的经验，而只要在这个团队待几个月说不定就能够学懂大半了，这些经验可以为下家公司创造很大的价值。</p><p>基于这样的思路，我们就不应该给自己设限。在自己力所能及的范围内去关注一下其他模块的工作往往不需要很多时间，但对于自己的职业发展一定是有益无害的。这里我就非常佩服和我在一个组的苏总，他就是典型的不给自己设限的人，他一人为 IoTDB 带来了 UDF, Trigger, Select into, Continuous query, 算数表达式等高级特性，写入，查询，生态工具这些模块他都碰过。尽管他现在非常忙，但我们这一届的同学毕业后对 IoTDB 了解最深的人很可能就是他，那他的价值就会非常高了。</p><h3 id="多给自己一些正向反馈"><a href="#多给自己一些正向反馈" class="headerlink" title="多给自己一些正向反馈"></a>多给自己一些正向反馈</h3><p>这个感悟其实涉及到了一些心理学的技巧。在学习计算机的过程中，往往会遇到很多棘手的问题，而这些问题很可能会影响学习热情。很多小白在刚入门的时候遇到一个棘手的问题就被劝退了。因此我们需要定期给自己一些正向反馈，这些反馈包括但不限于做一个完整的 project/feature，解决一个别人解决不了的 bug，参加各种比赛获奖，做 talk，发论文等等。通过这种外在的对自己的认可，我们也就能继续保持着学习热情。这一点黄东旭老师在 VLDB summer school 的 panel 上也有提到。</p><h3 id="不能只有输入没有输出"><a href="#不能只有输入没有输出" class="headerlink" title="不能只有输入没有输出"></a>不能只有输入没有输出</h3><p>在工作学习中，不能只有输入没有输出。我对输入的理解是指花费的时间成本，对输出的理解则是明确的他人可以看到的工作量。</p><p>为什么要强调这一点？因为我刚到实验室就是这样的状态，后面才慢慢意识到并进行了改变。比如对于一个调研的工作，往往会花费很多时间，调研完后自己可能感觉学习到了很多东西，但也没有明确的输出（包括但不限于分享，阅读笔记，解决实际问题等等）让别人看到，过一段时间可能自己又忘了。这不仅会导致别人丧失对自己的信任，也会导致自己忽视自己的单位时间产出，形成过一天混一天的惰性思维。它们都会使得自己丧失进一步成长的机会。因此，定期适当地输出是非常重要的。</p><h3 id="敢于不卑不亢地分享自己的想法"><a href="#敢于不卑不亢地分享自己的想法" class="headerlink" title="敢于不卑不亢地分享自己的想法"></a>敢于不卑不亢地分享自己的想法</h3><p>之前振华老师跟我们分享过他在明尼苏达留学时的一个有趣现象。在每周全院的科研 idea 分享会上，尽管上台分享的印度人很多，但大多数都很 naive，相反上台分享的中国人很少，但大多都很厉害，这侧面反映了我们中国人可能受儒家思想的影响，大都不好意思去分享自己还不成熟的工作。不同的人对这种现象有不同的看法，但在振华老师看来，早分享可能早避坑，不畏惧批评反而会有更大的前景。在 VLDB summer school 上黄东旭老师也告诫我们不要畏惧权威，要敢于分享自己的想法。</p><p>对于技术问题，我们要敢于不卑不亢，就事论事的分享自己的想法。如果我们的想法是正确的，这样能够不断竖立自己的个人影响力。如果我们的想法是错误的，这样也能够早点遭致批评来纠正自己的错误想法。无论如何对自己都是有益无害的。</p><h3 id="养成闭环的做事习惯"><a href="#养成闭环的做事习惯" class="headerlink" title="养成闭环的做事习惯"></a>养成闭环的做事习惯</h3><p>养成闭环的做事习惯非常重要，这有利于自己在同事中建立信任，塑造自己靠谱的形象。</p><p>一个新功能的开发工作，从接受，需求，调研，设计，实现，测试，review，合并到最终汇报要形成一个闭环。我在刚开始参与社区的时候对这样繁琐的流程非常困惑，感觉没有什么意义。随着维护项目经验的积累，我逐步意识到了这些都是过来人的智慧。</p><p>乔老师常给我们介绍说做事要做到”事事有回应 件件有着落 凡事有交代”，务实的说，很难对所有的事情都做到这种程度，但我们可以怀抱着这样的期望尽力做到这样，至少也得把重要且紧急的事情做到这个地步。养成这样的习惯对于自己的职业发展绝对是有益无害的。</p><h3 id="开源铸就了数据库最好的时代"><a href="#开源铸就了数据库最好的时代" class="headerlink" title="开源铸就了数据库最好的时代"></a>开源铸就了数据库最好的时代</h3><p>不确定是不是幸存者偏差，感觉数据库近几年在国内越来越火了。但就算是幸存者偏差，开源基础软件在国内越来越火已经是毋庸置疑的事实了。不论是资本还是企业，都更青睐开源的基础软件。</p><p>作为学生，这也是我们迅速成长的最好时代。我们完全可以参与开源社区，迅速提升自己的能力并认识一批志同道合的朋友，我就是开源的典型受益者，不论是 Talent Plan 社区还是 Apache IoTDB 社区都让我受益良多。</p><p>此外，站在巨人的肩膀上非常重要。我们如今身处开源的时代，很多资料都是公开的，一定要避免闭门造车。多了解其他系统，多交流，多调研，这对于个人和项目的发展都是有益无害的。</p><h3 id="稳定性可维护性大于性能"><a href="#稳定性可维护性大于性能" class="headerlink" title="稳定性可维护性大于性能"></a>稳定性可维护性大于性能</h3><p>当修 bug 次数多了之后，我逐渐意识到软件工程的重要性，并开始思考项目不稳定的根因。</p><p>OceanBase 的杨传辉老师提过一个观点：每个系统设计时都需要考虑架构、稳定性和性能，这三者之间的关系是什么？一个经典的规律是“把稳定的系统做高效，远比把高效的系统做稳定更容易”。最难的是从 0 到 1 把系统做稳定。有了稳定的系统，接下来逐步优化性能往往会比较顺利，直到遇到系统架构的性能天花板。因此，系统架构设计之前，首先要考虑清楚系统的目标和性能天花板，接着基于正确的架构把系统做稳定，最后优化性能。</p><p>我个人非常认同这个观点，大型基础软件的性能和稳定性可维护性往往存在一个不那么明显的 trade-off。在开源软件里，要实现新功能或者重构，一定要优先关注可维护性和稳定性。实现一个性能最优但模块耦合不好维护的功能对项目的伤害是非常大的，甚至可以被称为技术债，这会大大影响开发者的热情并辜负客户的信任。先实现一个 naive 但好维护几乎没有 bug 的版本，逐步的去优化性能，对于开发者和客户来说都是一个正向反馈的过程，这对于社区的发展也是很有帮助的。这里我就非常喜欢 TiDB 社区的方式，模块解耦做的比较干净，从 3.0 到 4.0 再到 5.0。每个版本都有巨大的性能提升，社区也越来越好，这就是一个明显的正反馈过程。</p><p>至于如何进一步提升系统的稳定性，在 VLDB summer school 的 panel 上我请教了黄东旭老师，他分享了 PingCAP 现在已经有几百台测试服务器日夜不息的进行测试，同时每年 PingCAP 为了稳定性付出的成本（包括硬件成本，人力成本等）已经达到了总支出的 30%~40%，而这依然还没有让他满意。由此可见，稳定的产品一定是大量的测试打磨出来的，没有太多捷径可走。当然，作为写代码的工程师，多反思总结，不断提升自己的技术水平，也能够对项目的稳定性做出自己的贡献。</p><h3 id="创新往往来自假设的改变"><a href="#创新往往来自假设的改变" class="headerlink" title="创新往往来自假设的改变"></a>创新往往来自假设的改变</h3><p>在今年的 VLDB summer school 上，有一个观点始终被提及：”创新往往来自假设的改变”。这个世界本来并不存在什么假设，假设是人类描述自然规律时的前置条件，而这个前置条件并不一定绝对正确，也不一定一成不变。不论是做学术还是做产品，可以时常想想假设在部分场景是否已经发生了变化，这其中可能蕴含着巨大的创新。</p><h2 id="来年展望"><a href="#来年展望" class="headerlink" title="来年展望"></a>来年展望</h2><p>洋洋洒洒写了这么一大堆流水账，也是借着写总结的机会再反思下自己，给未来的自己一个警醒。</p><p>新的一年，希望自己能努力干好以下 6 件事情吧：</p><ul><li>和实验室同学们一起打造出一个稳定高效的分布式 IoTDB，在毕业之前不留遗憾。</li><li>和实验室同学们一起营造团队的技术氛围，希望能够将我们组塑造成高校做开源的标杆实验室，吸引一大批对分布式系统感兴趣的优秀同学过来，一起干有意义有挑战的事情。</li><li>实习和秋招的时候多找一些团队聊聊，最终选择一个能得到成长的增量赛道。</li><li>认真学习一下数据库引擎，真的动手实践一下 LLVM，JIT，CodeGen，向量化等技术，也许是学习一下 tinysql 3.0 版本或者 risinglight。</li><li>规律作息，多锻炼多运动，减减肥，让自己拥有健康的身体。</li><li>多抽一些时间陪陪我的小杨同学，祝愿她申请一切顺利。</li></ul><p>最后，在除夕这天，预祝大家新年万事如意。愿每个人在新的一年都学有所得，学有所获，学有所长。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;2021 年即将结束，这一年忙忙碌碌收获许多，也认识到了定期总结定期反省的重要性。今年回家后意识到自己应该养成写年终总结的习惯了，于是断断续</summary>
      
    
    
    
    
    <category term="IoTDB" scheme="https://tanxinyu.work/tags/IoTDB/"/>
    
    <category term="年终总结" scheme="https://tanxinyu.work/tags/%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>2021 Talent Plan KV 学习营结营总结</title>
    <link href="https://tanxinyu.work/tinykv/"/>
    <id>https://tanxinyu.work/tinykv/</id>
    <published>2022-01-14T09:23:13.000Z</published>
    <updated>2022-08-28T04:29:42.965Z</updated>
    
    <content type="html"><![CDATA[<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>2021 年 11 月 ~ 2022 年 1 月 ，<strong>PingCAP </strong>举办了第一届 <strong>Talent Plan KV 学习营</strong>，相关介绍可参考 <a href="https://m.creatby.com/v2/manage/book/oa4occ/?from=singlemessage" target="_blank" rel="noopener">推送</a>。</p><p>在本次比赛中，由于我们小组的两位成员之前都刷过 MIT 6.824，已经对教学级别的 raft 有一定的了解，所以参加此次比赛的目的就是去感受一下生产级别分布式 KV 的代码实现，学习实践一下 lsm, etcd, raftstore 和 percolator 的理论知识和 codebase。</p><p>u1s1，刷 lab 的过程十分曲折，我们俩所在的实验室到年底的时候都非常忙，前几周基本每周都只能抽出顶多一两天的时间来写代码，而理解 lab2b/lab3b raftstore 的难度是非常大的，我们用了一周多的时间才勉强看懂 raftstore 的代码。这使得到还剩两周时间的时候，我们才刷到 lab2c。最后两周我们利用中午午休时间和晚上睡觉时间疯狂加班，在 lab 上花了更多的时间，最后才堪堪刷完。</p><p>在刷 lab 的过程中，由于时间有限，我们始终秉持着<code>学习优先，成绩第二</code>的原则。即以 <strong>了解 codebase，学习知识，做最容易做且最有用的优化</strong> 为主，并没有去卷很多功能点。在处理 bug 的态度上，对于 safety 的问题比如错误读写的 bug 等，我们对这类问题进行了重点关注和解决；对于 liveness 的问题比如 request timeout 等，我们则是在有限的时间内尽力做了优化，但并没有投入太多精力，因为这种工作没有上限，tikv 的 raftstore 也一定在持续做这些工作，时间不够的情况下去卷这些就没有太大意义了。</p><p><img src="/tinykv/grade.png" srcset="/img/loading.gif" lazyload alt></p><p>出人意料的是，我们得了第二名的好成绩，具体可参考 <a href="https://asktug.com/t/topic/393068" target="_blank" rel="noopener">官宣</a>。事后反省一下，在 safety 上我们遇到的问题都解决了；在 liveness 上我们没投入太多精力；在文档上，我们简单介绍了代码实现，但将重点放在了我们对相关知识的理解和思考上；在性能上，我们重点做了最容易做的 batching 优化，其本质上是使用 raft 的优化而不是 raft 自身的优化，但对性能的提升却异常关键，比如 tidb 对于一个事务打包的一堆写请求，到 tikv 的 region 之后，这些写请求同步成一条还是多条 raftlog 对于性能的影响是巨大的。</p><p>从结果来看，我们的策略是正确的，我们在很有限的时间内拿到了很高的收益。</p><p>最后，出于对课程的保护，也出于跟大家分享一些刷 lab 的经验，让大家少踩坑，在此处我仅将文档公开，希望能为大家提供一些思路，欢迎一起交流。</p><h1 id="文档"><a href="#文档" class="headerlink" title="文档"></a>文档</h1><h2 id="lab1"><a href="#lab1" class="headerlink" title="lab1"></a>lab1</h2><h3 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h3><h4 id="Part-1-Implement-a-standalone-storage-engine"><a href="#Part-1-Implement-a-standalone-storage-engine" class="headerlink" title="Part 1 : Implement a standalone storage engine"></a>Part 1 : Implement a standalone storage engine</h4><p>本部分是对底层 badger api 的包装，主要涉及修改的代码文件是 standalone_storage.go, 需要实现 Storage 接口的 Write 和 Reader 方法，来实现对底层 badger 数据库的读写。</p><h5 id="1-Write-部分实现思路"><a href="#1-Write-部分实现思路" class="headerlink" title="1.Write 部分实现思路"></a>1.Write 部分实现思路</h5><p>Write 部分涉及到 Put 和 Delete 两种操作。</p><p>因为 write_batch.go 中已经实现了对 badger 中 entry 的 put 和 delete 操作，我们只需要判断 batch 中的每一个 Modify 的操作类型，然后直接调用 write_batch.go 中相对应的方法即可。</p><h5 id="2-Reader-部分实现思路"><a href="#2-Reader-部分实现思路" class="headerlink" title="2.Reader 部分实现思路"></a>2.Reader 部分实现思路</h5><p>Reader 部分会涉及到 point read 和 scan read 两种不同读方式。</p><p>因为提示到应该使用 badger.Txn 来实现 Reader 函数，所以我们声明了一个 badgerReader 结构体来实现 StorageReader 接口，badgerReader 结构体内部包含对 badger.Txn 的引用。</p><p>针对 point read，<br>我们直接调用 util.go 中的 GetCF 等函数，对 cf 中指定 key 进行读取。</p><p>针对 scan read，<br>直接调用 cf_iterator.go 中的 NewCFIterator 函数，返回一个迭代器，供 part2 中调用。</p><h4 id="Part-2-Implement-raw-key-value-service-handlers"><a href="#Part-2-Implement-raw-key-value-service-handlers" class="headerlink" title="Part 2 : Implement raw key/value service handlers"></a>Part 2 : Implement raw key/value service handlers</h4><p>本部分需要实现 RawGet/ RawScan/ RawPut/ RawDelete 四个 handlers，主要涉及修改的代码文件是 raw_api.go</p><p>针对 RawGet，<br>我们调用 storage 的 Reader 函数返回一个 Reader，然后调用其 GetCF 函数进行点读取即可，读取之后需要判断对应 key 是否存在。</p><p>针对 RawScan，<br>同样地调用 storage 的 Reader 函数返回一个 Reader，然后调用其 IterCF 函数返回一个迭代器，然后使用迭代器读取即可。</p><p>针对 RawPut 和 RawDelete，<br>声明对应的 Modify 后，调用 storage.Write 函数即可。</p><h3 id="相关知识学习"><a href="#相关知识学习" class="headerlink" title="相关知识学习"></a>相关知识学习</h3><p>LSM 是一个伴随 NoSQL 运动一起流行的存储引擎，相比 B+ 树以牺牲读性能的代价在写入性能上获得了较大的提升。</p><p>近年来，工业界和学术界均对 LSM 树进行了一定的研究，具体可以阅读 VLDB2018 有关 LSM 的综述：<a href="https://arxiv.org/pdf/1812.07527.pdf" target="_blank" rel="noopener">LSM-based Storage Techniques: A Survey</a>, 也可直接阅读针对该论文我认为还不错的一篇 <a href="https://blog.shunzi.tech/post/vldbj-2018lsm-based-storage-techniques-a-survey/" target="_blank" rel="noopener">中文概要总结</a>。</p><p>介绍完了 LSM 综述，可以简单聊聊 badger，这是一个纯 go 实现的 LSM 存储引擎，参照了 FAST2016 有关 KV 分离 LSM 的设计： <a href="https://www.usenix.org/system/files/conference/fast16/fast16-papers-lu.pdf" target="_blank" rel="noopener">WiscKey</a> 。有关其项目的动机和一些 benchmark 结果可以参照其创始人的 <a href="https://dgraph.io/blog/post/badger/" target="_blank" rel="noopener">博客</a>。</p><p>对于 Wisckey 这篇论文，除了阅读论文以外，也可以参考此 <a href="https://www.scienjus.com/wisckey/" target="_blank" rel="noopener">阅读笔记</a> 和此 <a href="https://www.skyzh.dev/posts/articles/2021-08-07-lsm-kv-separation-overview/" target="_blank" rel="noopener">总结博客</a>。这两篇资料较为系统地介绍了现在学术界和工业界对于 KV 分离 LSM 的一些设计和实现。</p><p>实际上对于目前的 NewSQL 数据库，其底层大多数都是一个分布式 KV 存储系统。对于 OLTP 业务，其往往采用行存的方式，即 key 对应的 value 便是一个 tuple。在这样的架构下，value 往往很大，因而采用 KV 分离的设计往往能够减少大量的写放大，从而提升性能。</p><p>之前和腾讯云的一个大佬聊过，他有说 TiKV 的社区版和商业版存储引擎性能差异很大。目前想一下，KV 分离可能便是 RocksDB 和 Titan 的最大区别吧。</p><h2 id="lab2"><a href="#lab2" class="headerlink" title="lab2"></a>lab2</h2><h3 id="解题思路-1"><a href="#解题思路-1" class="headerlink" title="解题思路"></a>解题思路</h3><h4 id="lab2a"><a href="#lab2a" class="headerlink" title="lab2a"></a>lab2a</h4><h5 id="Leader-election"><a href="#Leader-election" class="headerlink" title="Leader election"></a>Leader election</h5><p>本部分是对 raft 模块 leader 选举功能的实现，主要涉及修改的代码文件是 raft.go、log.go</p><p>raft 模块 leader 选举流程如下：</p><p><img src="/tinykv/leader%20election.jpg" srcset="/img/loading.gif" lazyload alt></p><p>第一步，我们首先实现对 raft 的初始化。</p><p>实现 log.go 中的 newLog 方法，调用 storage 的 InitialState 等方法对 RaftLog 进行初始化，读取持久化在 storage 中 term、commit、vote 和 entries，为后面的 lab 做准备。完成 RaftLog 的初始化后，再填充 Raft 中的相应字段，即完成 Raft 对象的初始化。</p><p>第二步，我们实现 Raft 对象的 tick() 函数</p><p>上层应用会调用 tick() 函数，作为逻辑时钟控制 Raft 模块的选举功能和心跳功能。因此我们实现 tick() 函数，当 Raft 状态是 Follower 时，检查自上次接收心跳之后，间隔时间是否超过了 election timeout，如果超过了，将发送 MessageType_MsgHup；当 Raft 状态时 Leader 时，检查自上次发送心跳之后，间隔时间是否超过了 heartbeat timeout，如果超过了，将发送 MessageType_MsgBeat。</p><p>第三步，我们实现 raft.Raft.becomeXXX 等基本函数</p><p>实现了 becomeFollower(),becomeCandidate(),becomeLeader() 等 stub 函数，对不同状态下的属性进行赋值。</p><p>第四步，我们实现 Step() 函数对不同 Message 的处理</p><p>主要涉及到的 Message 有</p><ul><li><p>MessageType_MsgHup</p></li><li><p>MessageType_MsgRequestVote</p></li><li><p>MessageType_MsgRequestVoteResponse</p></li></ul><p>接下来分情况实现：</p><p>（1）MessageType_Msgup</p><p>当 Raft 状态为 Follower 和 Candidate 时，会先调用 becomeCandidate() 方法，将自己的状态转变为 Candidate，然后向所有 peer 发送 MessageType_MsgRequestVote 消息，请求他们的投票</p><p>（2）MessageType_MsgRequestVote</p><p>当 Raft 接收到此消息时，会在以下情况拒绝投票：</p><ul><li><p>当 Candidate 的 term 小于当前 raft 的 term 时拒绝投票</p></li><li><p>如果当前 raft 的 term 与 candidate 的 term 相等，但是它之前已经投票给其他 Candidate 时，会拒绝投票</p></li><li><p>如果当前 raft 发现 candidate 的日志不如自己的日志更 up-to-date 时，也会拒绝投票</p></li></ul><p>（3）MessageType_MsgRequestVoteResponse</p><p>Candidate 接收到此消息时，就会根据消息的 reject 属性来确定自己的得票，当自己的得票数大于一半以上，就会调用 becomeLeader() 函数，将状态转变为 Leader；当拒绝票数也大于一半以上时，就会转回到 Follower 状态。</p><h5 id="Log-replication"><a href="#Log-replication" class="headerlink" title="Log replication"></a>Log replication</h5><p>本部分是对 raft 模块日志复制功能的实现，主要涉及修改的代码文件是 raft.go、log.go</p><p>日志复制的流程如下：</p><p><img src="/tinykv/log%20replication.jpg" srcset="/img/loading.gif" lazyload alt="Log Replication"></p><p>本部分主要实现不同状态的 raft 对以下 Message 的处理：</p><ul><li>MessageType_MsgBeat</li><li>MessageType_MsgHeartbeat</li><li>MessageType_MsgHeartbeatResponse</li><li>MessageType_MsgPropose</li><li>MessageType_MsgAppend</li><li>MessageType_MsgAppendResponse</li></ul><p>接下来分情况实现：</p><p>（1）MessageType_MsgBeat</p><p>当上层应用调用 tick() 函数时，Leader 需要检查是否到了该发送心跳的时候，如果到了，那么就发送 MessageType_MsgHeartbeat。</p><p>leader 会将自己的 commit 值赋给在 MsgHeartbeat 消息中响应值，以让 Follower 能够及时 commit 安全的 entries</p><p>（2）MessageType_MsgHeartbeat</p><p>当 Follower 接收到心跳时，会更新自己的 electionTimeout，并会将自己的 lastIndex 与 leader 的 commit 值比较，让自己能够及时 commit entry。</p><p>（3）MessageType_MsgHeartbeatResponse</p><p>当 Leader 接收到心跳回复时，会比较对应 Follower 的 Pr.Match, 如果发现 Follower 滞后，就会向其发送缺少的 entries</p><p> (4)MessageType_MsgPropose</p><p>当 Leader 要添加 data 到自己的 log entries 中时，会发送一个 local message—MsgPropose 来让自己向所有 follower 同步 log entries，发送 MessageType_MsgAppend</p><p>（5）MessageType_MsgAppend</p><p>当 Follower 接收到此消息时，会在以下情况拒绝 append：</p><ul><li>当 Leader 的 term 小于当前 raft 的 term 时拒绝 append</li><li>当 Follower 在对应 Index 处不含 entry，说明 Follower 滞后比较严重</li><li>当 Follower 在对应 Index 处含有 entry，但是 term 不相等，说明产生了冲突</li></ul><p>其他情况，Follower 会接收新的 entries，并更新自己的相关属性。</p><p>（6）MessageType_MsgAppendResponse</p><p>当 Leader 发现 Follower 拒绝 append 后，会更新 raft.Prs 中对应 Follower 的进度信息，并根据新的进度，重新发送 entries。</p><h5 id="Implement-the-raw-node-interface"><a href="#Implement-the-raw-node-interface" class="headerlink" title="Implement the raw node interface"></a>Implement the raw node interface</h5><p>本部分主要实现 raw node 的接口，涉及修改的代码文件为 rawnode.go</p><p>RawNode 对象中的属性除了 Raft 对象，还增加了 prevSoftState 和 preHardState 两个属性，用于在 HasReady() 函数中判断 node 是否 pending</p><p>此外还实现了 Advance() 函数，主要是对 Raft 内部属性进行更新。</p><h4 id="lab2b"><a href="#lab2b" class="headerlink" title="lab2b"></a>lab2b</h4><h5 id="Implement-peer-storage"><a href="#Implement-peer-storage" class="headerlink" title="Implement peer storage"></a>Implement peer storage</h5><p>本部分主要实现 peer_storage.go 中 SaveReadyState() 方法和 Append() 方法，涉及修改的代码文件为 peer_storage.go</p><p>peer storage 除了管理持久化 raft log 外，也会管理持久化其他元数据（RaftLocalState、RaftApplyState 和 RegionLocalState），因此我们需要实现 SaveReadyState() 方法，将 raft.Ready 中修改过的状态和数据保存到 badger 中。</p><p>首先我们通过实现 Append() 方法，保存需要持久化的 raft log。遍历 Ready 中 Entries，调用 SetMeta() 方法将他们保存到 raftWB，并删除可能未提交的 raft log，最后更新 raftState。</p><p>在处理完 raft log 后，我们还需要保存 Ready 中的 hardState，并在最后调用 WriteToDB() 方法保证之前的修改落盘。</p><h5 id="Implement-raft-ready-process"><a href="#Implement-raft-ready-process" class="headerlink" title="Implement raft ready process"></a>Implement raft ready process</h5><p>本部分主要实现 peer_storage_handler.go 中的 proposeRaftCommand() 和 HandleRaftReady() 方法，涉及修改的代码文件为 peer_storage_handler.go</p><p>proposeRaftCommand() 方法使得系统有能力将接收到的 client 请求通过 raft 模块进行同步，以实现分布式环境下的一致性。在本方法中，我们直接调用 raft 模块的 Propose 方法，将 client 请求进行同步，并为该请求初始化对应的 proposal，以便该请求 committed 后将结果返回给 client</p><p>当 msg 被 raft 模块处理后，会导致 raft 模块的一些状态变化，这时候需要 HandleRaftReady() 方法进行一些操作来处理这些变化：</p><ol><li>需要调用 peer_storage.go() 中的 SaveReadyState() 方法，将 log entries 和一些元数据变化进行持久化。</li><li>需要调用 peer_storage_handler 中的 send() 方法，将一些需要发送的消息，发送给同一个 region 中的 peer</li><li>我们需要处理一些 committed entries，将他们应用到状态机中，并把结果通过 callback 反馈给 client</li><li>在上述处理完后，需要调用 advance() 方法，将 raft 模块整体推进到下一个状态</li></ol><h4 id="lab2c"><a href="#lab2c" class="headerlink" title="lab2c"></a>lab2c</h4><p>因为 raft entries 不可能一直无限增长下去，所以本部分我们需要实现 snapshot 功能，清理之前的 raft entries。</p><p>整个 lab2c 的执行流程如下：</p><ol><li>gc log 的流程：</li></ol><p><img src="/tinykv/gc%20raftLog.png" srcset="/img/loading.gif" lazyload alt="gc raftLog"></p><ol><li>发送和应用 snapshot 的流程：</li></ol><p><img src="/tinykv/send%20and%20apply%20Snapshot.png" srcset="/img/loading.gif" lazyload alt="send and apply snapshot"></p><h5 id="Implement-in-raft"><a href="#Implement-in-raft" class="headerlink" title="Implement in raft"></a>Implement in raft</h5><p>当 leader 发现 follower 落后太多时，会主动向 follower 发送 snapshot，对其进行同步。在 Raft 模块内部，需要增加对 MessageType_MsgSnapshot 消息的处理，主要对以下两点进行处理：</p><ol><li>当 leader 需要向 follower 同步日志时，如果同步的日志已经被 compact 了，那么直接发送 snapshot 给 follower 进行同步，否则发送 MessageType_MsgAppend 消息，向 follower 添加 entries。通过调用 peer storage 的 Snapshot() 方法，我们可以得到已经制作完成的 snapshot</li><li>实现 handleSnapshot() 方法，当 follower 接收到 MessageType_MsgSnapshot 时，需要进行相应处理。</li></ol><p>在第二步中，follower 需要判断 leader 发送的 snapshot 是否会与自己的 entries 产生冲突，如果发送的 snapshot 是目前现有 entries 的子集，说明 snapshot 是 stale 的，那么要返回目前 follower 的进度，更新 leader 中相应的 Match 和 Next，以便再下一次发送正确的日志；如果没有发生冲突，那么 follower 就根据 snapshot 中的信息进行相应的更新，更新自身的 committed 等 index，如果 confstate 也产生变化，有新的 node 加入或者已有的 node 被移除，需要更新本节点的 confState，为 lab3 做准备。</p><h5 id="Implement-in-raftstore"><a href="#Implement-in-raftstore" class="headerlink" title="Implement in raftstore"></a>Implement in raftstore</h5><p>在本部分中，当日志增长超过 RaftLogGcCountLimit 的限制时，会要求本节点整理和删除已经应用到状态机的旧日志。节点会接收到类似于 Get/Put/Delete/Snap 命令的 CompactLogRequest，因此我们需要在 lab2b 的基础上，当包含 CompactLogRequest 的 entry 提交后，增加 processAdminRequest() 方法来对这类 adminRequest 的处理。</p><p>在 processAdminRequest() 方法中，我们需要更新 RaftApplyState 中 RaftTruncatedState 中的相关元数据，记录最新截断的最后一个日志的 index 和 term，然后调用 ScheduleCompactLog() 方法，异步让 RaftLog-gc worker 能够进行旧日志删除的工作。</p><p>另外，因为 raft 模块在处理 snapshot 相关的 msg 时，也会对一些状态进行修改，所以在 peer_storage.go 方法中，我们需要在 SaveReadyState() 方法中，调用 ApplySnapshot() 方法中，对相应的元数据进行保存。</p><p>在 ApplySnapshot() 方法中，如果当前节点已经处理过的 entries 只是 snapshot 的一个子集，那么需要对 raftLocalState 中的 commit、lastIndex 以及 raftApplyState 中的 appliedIndex 等元数据进行更新，并调用 ClearData() 和 ClearMetaData() 方法，对现有的 stale 元数据以及日志进行清空整理。同时，也对 regionLocalState 进行相应更新。最后，我们需要通过 regionSched 这个 channel，将 snapshot 应用于对应的状态机</p><h3 id="相关知识学习-1"><a href="#相关知识学习-1" class="headerlink" title="相关知识学习"></a>相关知识学习</h3><h4 id="Raft"><a href="#Raft" class="headerlink" title="Raft"></a>Raft</h4><p>Raft 是 2015 年以来最受人瞩目的共识算法，有关其前世今生可以参考我们总结的 <a href="https://tanxinyu.work/raft/">博客</a>，此处不再赘述。</p><p>etcd 是一个生产级别的 Raft 实现，我们在实现 lab2a 的时候大量参考了 etcd 的代码。这个过程不仅帮助我们进一步了解了 etcd 的 codebase，也让我们进一步意识到一个工程级别的 raft 实现需要考虑多少 corner case。整个学习过程收获还是很大的，这里贴一些 etcd 的优质博客以供学习。</p><ul><li><a href="https://www.codedump.info/post/20180922-etcd-raft/" target="_blank" rel="noopener">etcd Raft 库解析</a></li><li><a href="https://www.codedump.info/post/20181125-etcd-server/" target="_blank" rel="noopener">Etcd 存储的实现</a></li><li><a href="https://www.codedump.info/post/20210515-raft" target="_blank" rel="noopener">Etcd Raft 库的工程化实现</a></li><li><a href="https://www.codedump.info/post/20210628-etcd-wal/" target="_blank" rel="noopener">Etcd Raft 库的日志存储</a></li></ul><h4 id="KVRaft"><a href="#KVRaft" class="headerlink" title="KVRaft"></a>KVRaft</h4><p>在 Raft 层完成后，下一步需要做的便是基于 Raft 层搭建一个高可用的 KV 层。这里依然参考了 etcd KV 层驱动 Raft 层的方式。<br>即总体的思路如下所示：<br><figure class="highlight go"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Go"><span class="hljs-keyword">for</span> &#123;<br>  <span class="hljs-keyword">select</span> &#123;<br>  <span class="hljs-keyword">case</span> &lt;-s.Ticker:<br>    Node.Tick()<br>  <span class="hljs-keyword">default</span>:<br>    <span class="hljs-keyword">if</span> Node.HasReady() &#123;<br>      rd := Node.Ready()<br>      saveToStorage(rd.State, rd.Entries, rd.Snapshot)<br>      send(rd.Messages)<br>      <span class="hljs-keyword">for</span> _, entry := <span class="hljs-keyword">range</span> rd.CommittedEntries &#123;<br>        process(entry)<br>      &#125;<br>      s.Node.Advance(rd)<br>    &#125;<br>&#125;<br></code></pre></div></td></tr></table></figure></p><p>做过 tinykv 的同学应该都能够感觉到 lab2b 的难度与之前有一个大 gap，我认为主要原因是需要看的代码实现是太多了。</p><p>如今回首，建议分三个步骤来做，这样效率可能会高一些：</p><ul><li>了解读写流程的详细步骤。对于 client 的请求，其处理和回复均在 raft_server.go 中进行了处理，然而其在服务端内部的生命周期如何，这里需要知根知底。（注意在遇到 channel 打断同步的执行流程时不能瞎猜，一定要明确找到 channel 的接收端和发送端继续把生命周期理下去）</li><li>仔细阅读 raft_server.go, router.go, raftstore.go, raft_worker.go, peer_storage.go, peer_msg_handle.go 等文件的代码。这会对了解整个系统的 codebase 十分有帮助。</li><li>仔细阅读 tinykv 的 lab2 文档，了解编码，存储等细节后便可以动手实现了。</li></ul><p>在实现 lab2b 中，由于时间有限，我们重点关注了 batching 的优化和 apply 时的 safety，以下进行简单的介绍：</p><ul><li><p>batching 优化：客户端发来的一条 command 可能包含多个读写请求，服务端可以将其打包成一条或多条 raft 日志。显然，打包成一条 Raft 日志的性能会更高，因为这样能够节省大量 IO 资源的消耗。当然这也需要在 apply 时对所有的 request 均做相应的业务和容错处理。</p></li><li><p>apply 时的 safety：要想实现基于 Raft 的 KV 服务，一大难点便是如何保证 applyIndex 和状态机数据的原子性。比如在 6.824 的框架中，Raft 层对于上层状态机的假设是易失的，即重启后状态机为空，那么 applyIndex 便可以不被持久化记录，因为一旦发生重启 Raft 实例可以从 0 开始重新 apply 日志，对于状态机来说这个过程保证不会重复。然而这样的实现虽然保证了 safety，但却不是一个生产可用的实现。对于 tinykv，其状态机为非易失的 LSM 引擎，一旦要记录 applyIndex 就可能出现与状态机数据不一致的原子性问题，即重启后可能会存在日志被重复 apply 到状态机的现象。为了解决这一问题，我们将每个 Index 下 entry 的应用和对应 applyIndex 的更新放到了一个事务中来保证他们之间的原子性，巧妙地解决了该过程的 safety 问题。</p></li></ul><h4 id="Snapshot"><a href="#Snapshot" class="headerlink" title="Snapshot"></a>Snapshot</h4><p>tinykv 的 Snapshot 几乎是一个纯异步的方案，在架构上有很多讲究，这里可以仔细阅读文档和一位社区同学分享的 <a href="https://asktug.com/t/topic/273859" target="_blank" rel="noopener">Snapshot 流程</a> 后再开始编码。</p><p>一旦了解了以下两个流程，代码便可以自然而然地写出来了。</p><ul><li>log gc 流程</li><li>snapshot 的异步生成，异步分批发送，异步分批接收和异步应用。</li></ul><h2 id="lab3"><a href="#lab3" class="headerlink" title="lab3"></a>lab3</h2><h3 id="解题思路-2"><a href="#解题思路-2" class="headerlink" title="解题思路"></a>解题思路</h3><h4 id="lab3a"><a href="#lab3a" class="headerlink" title="lab3a"></a>lab3a</h4><p>本部分主要涉及 Raft 算法 leader transfer 和 conf change 功能的两个工作，主要涉及修改的代码文件是 raft.go</p><p>对于 leader transfer，注意以下几点即可：</p><ul><li>leader 在 transfer 时需要阻写。</li><li>当 leader 发现 transferee 的 matchIndex 与本地的 lastIndex 相等时直接发送 timeout 请求让其快速选举即可，否则继续发送日志让其快速同步。</li><li>当 follower 收到 leader transfer 请求时，直接发起选举即可</li></ul><p>对于 conf change，注意以下几点即可：</p><ul><li>只对还在共识组配置中的 raftnode 进行 tick。</li><li>新当选的 leader 需要保证之前任期的所有 log 都被 apply 后才能进行新的 conf change 变更，这有关 raft 单步配置变更的 safety，可以参照 <a href="https://groups.google.com/g/raft-dev/c/t4xj6dJTP6E/m/d2D9LrWRza8J" target="_blank" rel="noopener">邮件</a> 和相关 <a href="https://zhuanlan.zhihu.com/p/342319702" target="_blank" rel="noopener">博客</a>。</li><li>只有当前共识组的最新配置变更日志被 apply 后才可以接收新的配置变更日志。</li><li>增删节点时需要维护 PeerTracker。</li></ul><h4 id="lab3b"><a href="#lab3b" class="headerlink" title="lab3b"></a>lab3b</h4><p>本部分主要是在 3a 的基础上，在 raft store 层面实现对 TransferLeader、ChangePeer 和 Split 三种 AdminRequest 的处理，涉及修改的文件主要是 peer_msg_handler.go 和 peer.go</p><p>对于 TransferLeader，比较简单：</p><p>TransferLeader request 因为不需要复制到 follower 节点，所以在 peer_msg_handler.go 的 pproposeRaftCommand() 方法中直接调用 raw_node.go 中的 TransferLeader() 方法即可</p><p>对于 ConfChange，分 addNode 和 removeNode 两种行为处理。</p><p>当 addNode 的命令 commit 之后，不需要我们手动调用 createPeer() 或者 maybeCreatePeer() 来显式创建 peer。我们只需要对 d.ctx 中的 storeMeta 进行修改即可，新 peer 会通过心跳机制进行创建。</p><p>当 removeNode 的命令 commit 之后，与 addNode 命令不同的是，我们需要显式调用 destroyPeer() 函数来停止相应的 raft 模块。这时需要注意的一个点时，当 Region 中只剩下两个节点，要从这两个节点中移除一个时，如果有一个节点挂了，会使整个集群不可用，特别是要移除的节点是 leader 本身。</p><p>在测试中会遇到这样的问题：当 Region 中只剩下节点 A（leader）和 节点 B（follower），当 removeNode A 的命令被 commit 之后，leader 就进行自我销毁，如果这个时候进入了 unreliable 的状态，那么 leader 就有可能无法在 destory 之前通过 heartbeat 去更新 follower 的 commitIndex。这样使得 follower B 不知道 leader A 已经被移除，就算发起选举也无法收到节点 A 的 vote，最终无法成功，导致 request timeout。</p><p>对于 split, 需要注意：</p><ol><li>因为 Region 会进行分裂，所以需要对 lab2b 进行修改，当接收到 delete/put/get/snap 等命令时，需要检查他们的 key 是否还在该 region 中，因为在 raftCmd 同步过程中，可能会发生 region 的 split，也需要检查 RegionEpoch 是否匹配。</li><li>在比较 splitKey 和当前 region 的 endKey 时，需要使用 engine_util.ExceedEndKey()，因为 key range 逻辑上是一个环。</li><li>split 时也需要对 d.ctx 中的 storeMeta 中 region 相关信息进行更新。</li><li>需要显式调用 createPeer() 来创建新 Region 中的 peer。</li><li>在 3b 的最后一个测试中，我们遇到以下问题：<ol><li>达成共识需要的时间有时候比较长，这就会导致新 region 中无法产生 leade 与 Scheduler 进行心跳交互，来更新 Scheduler 中的 regions，产生 find no region 的错误。这一部分可能需要 pre-vote 来进行根本性地解决，但时间不够，希望以后有时间解决这个遗憾。</li><li>会有一定概率遇到“多数据”的问题，经排查发现 snap response 中会包含当前 peer 的 region 引用返回，但是这时可能会产生的一个问题时，当返回时 region 是正常的，但当 client 端要根据这个 region 来读的时候，刚好有一个 split 命令改变了 region 的 startKey 或者 endKey，最后导致 client 端多读。该问题有同学在群中反馈应该测试中对 region 进行复制。</li><li>会有一定概率遇到“少数据”的问题，这是因为当 peer 未初始化时，apply snapshot 时不能删除之前的元数据和数据。</li></ol></li></ol><h4 id="lab3c"><a href="#lab3c" class="headerlink" title="lab3c"></a>lab3c</h4><p>本部分主要涉及对收集到的心跳信息进行选择性维护和对 balance-region 策略的具体实现两个工作，主要涉及修改的代码文件是 cluster.go 和 balance_region.go</p><p>对于维护心跳信息，按照以下流程执行即可：</p><ul><li>判断是否存在 epoch，若不存在则返回 err</li><li>判断是否存在对应 region，如存在则判断 epoch 是否陈旧，如陈旧则返回 err；若不存在则选择重叠的 regions，接着判断 epoch 是否陈旧。</li><li>否则维护 region 并更新 store 的 status 即可。</li></ul><p>对于 balance-region 策略的实现，按照以下步骤执行即可：</p><ul><li>获取健康的 store 列表：<ul><li>store 必须状态是 up 且最近心跳的间隔小于集群判断宕机的时间阈值。</li><li>如果列表长度小于等于 1 则不可调度，返回空即可。</li><li>按照 regionSize 对 store 大小排序。</li></ul></li><li>寻找可调度的 store：<ul><li>按照大小在所有 store 上从大到小依次寻找可以调度的 region，优先级依次是 pending，follower，leader。</li><li>如果能够获取到 region 且 region 的 peer 个数等于集群的副本数，则说明该 region 可能可以在该 store 上被调度走。</li></ul></li><li>寻找被调度的 store：<ul><li>按照大小在所有 store 上从小到达依次寻找不存在该 region 的 store。</li><li>找到后判断迁移是否有价值，即两个 store 的大小差值是否大于 region 的两倍大小，这样迁移之后其大小关系依然不会发生改变。</li></ul></li><li>如果两个 store 都能够寻找到，则在新 store 上申请一个该 region 的 peer，创建对应的 MovePeerOperator 即可。</li></ul><h3 id="相关知识学习-2"><a href="#相关知识学习-2" class="headerlink" title="相关知识学习"></a>相关知识学习</h3><h4 id="Multi-Raft"><a href="#Multi-Raft" class="headerlink" title="Multi-Raft"></a>Multi-Raft</h4><p>Multi-Raft 是分布式 KV 可以 scale out 的基石。TiKV 对每个 region 的 conf change 和 transfer leader 功能能够将 region 动态的在所有 store 上进行负载均衡，对 region 的 split 和 merge 则是能够解决单 region 热点并无用工作损耗资源的问题。不得不说，后两者尽管道理上理解起来很简单，但工程实现上有太多细节要考虑了（据说贵司写了好几年才稳定），分析可能的异常情况实在是太痛苦了，为贵司能够啃下这块硬骨头点赞。</p><p>最近看到有一个基于 TiKV 的 hackathon <a href="https://github.com/TPC-TiKV/rfc" target="_blank" rel="noopener">议题</a>，其本质是想通过更改线程模型来优化 TiKV 的写入性能、性能稳定性和自适应能力。这里可以简单提提一些想法，其实就我们在时序数据库方向的一些经验来说，每个 TSM（TimeSeries Merge Tree）大概能够用满一个核的 CPU 资源。只要我们将 TSM 引擎额个数与 CPU 核数绑定，写入性能基本是能够随着核数增加而线性提升的。那么对于 KV 场景，是否开启 CPU 个数的 LSM 引擎能够更好的利用 CPU 资源呢？即对于 raftstore，是否启动 CPU 个数的 Rocksdb 实例能够更好的利用资源呢？感觉这里也可以做做测试尝试一下。</p><h4 id="负载均衡"><a href="#负载均衡" class="headerlink" title="负载均衡"></a>负载均衡</h4><p>负载均衡是分布式系统中的一大难题，不同系统均有不同的策略实现，不同的策略可能在不同的 workload 中更有效。</p><p>相比 pd 的实现，我们在 lab3c 实现的策略实际上很 trivial，因此我们简单学习了 pd 调度 region 的 <a href="https://asktug.com/t/topic/242808" target="_blank" rel="noopener">策略</a>。尽管这些策略道理上理解起来都比较简单，但如何将所有统计信息准确的量化成一个动态模型却是一件很难尽善尽美的事，这中间的很多指标也只能是经验值，没有严谨的依据。</p><p>有关负载均衡我们对学术界的相关工作还不够了解，之后有时间会进行一些关注。</p><h2 id="lab4"><a href="#lab4" class="headerlink" title="lab4"></a>lab4</h2><h3 id="解题思路-3"><a href="#解题思路-3" class="headerlink" title="解题思路"></a>解题思路</h3><p>本 Lab 整体相对简单，在基本了解 MVCC, 2PC 和 Percolator 后便可动手了，面向测试用例编程即可。</p><h4 id="lab4a"><a href="#lab4a" class="headerlink" title="lab4a"></a>lab4a</h4><p>本部分是对 mvcc 模块的实现，主要涉及修改的代码文件是 transaction.go。需要利用对 CFLock, CFDefault 和 CFWrite 三个 CF 的一些操作来实现 mvcc。</p><p>针对 Lock 相关的函数：</p><ul><li>PutLock：将 PUT <key, lock.tobytes()> 添加到 Modify 即可。</key,></li><li>DeleteLock：将 Delete <key> 添加到 Modify 即可。</key></li><li>GetLock：在 CFLock 中查找即可。</li></ul><p>针对 Value 相关的函数：</p><ul><li>PutValue：将 PUT <EncodeKey(key, txn.startts), value> 添加到 Modify 即可。</EncodeKey(key,></li><li>DeleteValue：将 Delete <EncodeKey(key, txn.startts)> 添加到 Modify 即可。</EncodeKey(key,></li><li>GetValue：首先从 CFWrite 中寻找在当前快照之前已经提交的版本。如果未找到则返回空，如果找到则正对不同的 Kind 有不同的行为：<ul><li>Put：根据 value 中的 StartTS 去 CFDefault 寻找即可。</li><li>Delete：返回空即可。</li><li>Rollback：继续寻找之前的版本。</li></ul></li></ul><p>针对 Write 相关的函数：</p><ul><li>PutWrite：将 PUT <EncodeKey(key, committs), write.tobytes()> 添加到 Modify 即可。</EncodeKey(key,></li><li>CurrentWrite：从 CFWrite 当中寻找当前 key 对应且值的 StartTS 与当前事务 StartTS 相同的行。</li><li>MostRecentWrite：从 CFWrite 当中寻找当前 key 对应且值的 StartTS 最大的行。</li></ul><h4 id="lab4b"><a href="#lab4b" class="headerlink" title="lab4b"></a>lab4b</h4><p>本部分是对 Percolator 算法 KVPreWrite, KVCommit 和 KVGet 三个方法的实现，主要涉及修改的代码文件是 server.go, query.go 和 nonquery.go。</p><ul><li>KVPreWrite：针对每个 key，首先检验是否存在写写冲突，再检查是否存在行锁，如存在则需要根据所属事务是否一致来决定是否返回 KeyError，最后将 key 添加到 CFDefault 和 CFLock 即可。</li><li>KVCommit：针对每个 key，首先检查是否存在行锁，如不存在则已经 commit 或 rollback，如存在则需要根据 CFWrite 中的当前事务状态来判断是否返回 KeyError，最后将 key 添加到 CFWrite 中并在 CFLock 中删除即可。</li><li>KVGet：首先检查行锁，如为当前事务所锁，则返回 Error，否则调用 mvcc 模块的 GetValue 获得快照读即可。</li></ul><h4 id="lab4c"><a href="#lab4c" class="headerlink" title="lab4c"></a>lab4c</h4><p>本部分是对 Percolator 算法 KvCheckTxnStatus, KvBatchRollback, KvResolveLock 和 KvScan 四个方法的实现，主要涉及修改的代码文件是 server.go, query.go 和 nonquery.go。</p><ul><li>KvCheckTxnStatus：检查 PrimaryLock 的行锁，如果存在且被当前事务锁定，则根据 ttl 时间判断是否过期从而做出相应的动作；否则锁很已被 rollback 或者 commit，从 CFWrite 中获取相关信息即可。</li><li>KvBatchRollback：针对每个 key，首先检查是否存在行锁，如果存在则删除 key 在 CFLock 和 CFValue 中的数并且在 CFWrite 中写入一条 rollback 即可。如果不存在或者不归当前事务锁定，则从 CFWrite 中获取当前事务的提交信息，如果不存在则向 CFWrite 写入一条 rollback，如果存在则根据是否为 rollback 判断是否返回错误。</li><li>KvResolveLock：针对每个 key，根据请求中的参数决定来 commit 或者 rollback 即可。</li><li>KvScan：利用 Scanner 扫描到没有 key 或达到 limit 阈值即可。针对 scanner，需要注意不能读有锁的 key，不能读未来的版本，不能读已删除或者已 rollback 的 key。</li></ul><h4 id="代码结构"><a href="#代码结构" class="headerlink" title="代码结构"></a>代码结构</h4><p>为了使得 server.go 逻辑代码清晰，在分别完成三个 lab 后对代码进行了进一步整理，针对读写请求分别抽象出来了接口，这样可以使得逻辑更为清晰。</p><figure class="highlight go"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Go"><span class="hljs-keyword">type</span> BaseCommand <span class="hljs-keyword">interface</span> &#123;<br>Context() *kvrpcpb.Context<br>StartTs() <span class="hljs-keyword">uint64</span><br>&#125;<br><br><span class="hljs-keyword">type</span> Base <span class="hljs-keyword">struct</span> &#123;<br>context *kvrpcpb.Context<br>startTs <span class="hljs-keyword">uint64</span><br>&#125;<br><br><span class="hljs-keyword">type</span> QueryCommand <span class="hljs-keyword">interface</span> &#123;<br>BaseCommand<br>Read(txn *mvcc.MvccTxn) (<span class="hljs-keyword">interface</span>&#123;&#125;, error)<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">ExecuteQuery</span><span class="hljs-params">(cmd QueryCommand, storage storage.Storage)</span> <span class="hljs-params">(<span class="hljs-keyword">interface</span>&#123;&#125;, error)</span></span> &#123;<br>ctx := cmd.Context()<br>reader, err := storage.Reader(ctx)<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> &amp;kvrpcpb.ScanResponse&#123;RegionError: util.RaftstoreErrToPbError(err)&#125;, <span class="hljs-literal">nil</span><br>&#125;<br><span class="hljs-keyword">defer</span> reader.Close()<br><span class="hljs-keyword">return</span> cmd.Read(mvcc.NewMvccTxn(reader, cmd.StartTs()))<br>&#125;<br><br><span class="hljs-keyword">type</span> NonQueryCommand <span class="hljs-keyword">interface</span> &#123;<br>BaseCommand<br>IsEmpty() <span class="hljs-keyword">bool</span><br>GetEmptyResponse() <span class="hljs-keyword">interface</span>&#123;&#125;<br>WriteKeys(txn *mvcc.MvccTxn) ([][]<span class="hljs-keyword">byte</span>, error)<br>Write(txn *mvcc.MvccTxn) (<span class="hljs-keyword">interface</span>&#123;&#125;, error)<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">ExecuteNonQuery</span><span class="hljs-params">(cmd NonQueryCommand, storage storage.Storage, latches *latches.Latches)</span> <span class="hljs-params">(<span class="hljs-keyword">interface</span>&#123;&#125;, error)</span></span> &#123;<br><span class="hljs-keyword">if</span> cmd.IsEmpty() &#123;<br><span class="hljs-keyword">return</span> cmd.GetEmptyResponse(), <span class="hljs-literal">nil</span><br>&#125;<br><br>ctx := cmd.Context()<br>reader, err := storage.Reader(ctx)<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> &amp;kvrpcpb.ScanResponse&#123;RegionError: util.RaftstoreErrToPbError(err)&#125;, <span class="hljs-literal">nil</span><br>&#125;<br><span class="hljs-keyword">defer</span> reader.Close()<br>txn := mvcc.NewMvccTxn(reader, cmd.StartTs())<br><br>keys, err := cmd.WriteKeys(txn)<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, err<br>&#125;<br><br>latches.WaitForLatches(keys)<br><span class="hljs-keyword">defer</span> latches.ReleaseLatches(keys)<br><br>response, err := cmd.Write(txn)<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, err<br>&#125;<br><br>err = storage.Write(ctx, txn.Writes())<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, err<br>&#125;<br><br>latches.Validation(txn, keys)<br><br><span class="hljs-keyword">return</span> response, <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></div></td></tr></table></figure><h3 id="相关知识学习-3"><a href="#相关知识学习-3" class="headerlink" title="相关知识学习"></a>相关知识学习</h3><p>有关分布式事务，我们之前有过简单的 <a href="https://tanxinyu.work/distributed-transactions/">学习</a>，对 2PL, 2PC 均有简单的了解，因此此次在实现 Percolator 时只需要关注 2PC 与 MVCC 的结合即可，这里重点参考了以下博客：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/77846678" target="_blank" rel="noopener">TiKV 源码解析系列文章（十二）分布式事务</a></li><li><a href="https://pingcap.com/zh/blog/tidb-transaction-model" target="_blank" rel="noopener">TiKV 事务模型概览，Google Spanner 开源实现</a></li><li><a href="http://mysql.taobao.org/monthly/2018/11/02/" target="_blank" rel="noopener">Google Percolator 分布式事务实现原理解读</a></li><li><a href="https://pingcap.com/zh/blog/async-commit-principle" target="_blank" rel="noopener">Async Commit 原理介绍</a></li></ul><p>实现完后，我们进一步被 Google 的聪明所折服，Percolator 基于单行事务实现了多行事务，基于 MVCC 实现了 SI 隔离级别。尽管其事务恢复流程相对复杂，但其本质上是在 CAP 定理中通过牺牲恢复时的 A 来优化了协调者正常写入时的 A，即协调者单点在 SQL 层不用高可用来保证最终执行 commit 或者 abort。因为一旦协调者节点挂掉，该事务在超过 TTL （TTL 的超时也是由 TSO 的时间戳来判断，对于各个 TiKV 节点来说均为逻辑时钟，这样的设计也避免了 Wall Clock 的同步难题）后会被其他事务 rollback，总体上来看 Percolator 比较优雅的解决了 2PC 的 safety 问题。</p><p>当然，分布式事务可以深究的地方还很多，并且很多思想都与 Lamport 那篇最著名的论文 <a href="https://tanxinyu.work/time-clock-order-in-distributed-system-thesis/"><code>Time, Clocks, and the Ordering of Events in a Distributed System</code></a> 有关。除了 TiDB 外，Spanner，YugaByte，CockroachDB 等 NewSQL 数据库均有自己的大杀器，比如 TrueTime，HLC 等等。总之这块儿挺有意思的，虽然在这儿告一段落，但希望以后有机会能深入做一些相关工作。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>实现一个稳定的分布式系统实在是太有挑战太有意思啦。</p><p>感谢 PingCAP 社区提供如此优秀的课程！</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h1&gt;&lt;p&gt;2021 年 11 月 ~ 2022 年 1 月 ，&lt;strong&gt;PingCAP &lt;/strong&gt;举办了第一届 &lt;strong&gt;Tale</summary>
      
    
    
    
    
    <category term="分布式存储" scheme="https://tanxinyu.work/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/"/>
    
    <category term="分布式系统理论" scheme="https://tanxinyu.work/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%90%86%E8%AE%BA/"/>
    
    <category term="共识算法" scheme="https://tanxinyu.work/tags/%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>第一届九坤并行程序优化大赛总结</title>
    <link href="https://tanxinyu.work/jiu-kun-parallel-program-optimization-contest/"/>
    <id>https://tanxinyu.work/jiu-kun-parallel-program-optimization-contest/</id>
    <published>2021-11-24T14:15:08.000Z</published>
    <updated>2022-01-30T03:42:14.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>2021 年 9 月，量化头部公司<strong>九坤</strong>举办了其第一届<strong>并行程序优化大赛</strong>，相关介绍可参考 <a href="https://mp.weixin.qq.com/s/iaurj-1191SahJZ9uSk7RA" target="_blank" rel="noopener">推送</a>。赛题如下：</p><p><img src="/jiu-kun-parallel-program-optimization-contest/title.png" srcset="/img/loading.gif" lazyload alt></p><p>赛题是 C/C++ 的 codebase，然而我与一起组队的两位同学基本都对 C/C++ 不太熟悉，于是我们起名叫做了”只会 JAVA 队”。作为三个在体系结构几乎一窍不通的小白，在一个多月断断续续的不到 10 次线下沟通中，我们逐渐对体系结构入了门，在 192 个队伍脱颖而出，并在决赛取得了第 4 名的成绩（PS：离苹果周边只差一步真的好可惜），具体可以参考 <a href="https://mp.weixin.qq.com/s/Ct3XwD6zR_qNpvqLbQPs4Q" target="_blank" rel="noopener">总结推送</a>。</p><p><img src="/jiu-kun-parallel-program-optimization-contest/result.jpeg" srcset="/img/loading.gif" lazyload alt></p><p>这里简单做一总结，贴一些当前的资料和想法，以备之后回忆和反思。</p><h2 id="赛题"><a href="#赛题" class="headerlink" title="赛题"></a>赛题</h2><h3 id="第一题"><a href="#第一题" class="headerlink" title="第一题"></a>第一题</h3><p>在深度学习中，卷积操作在神经网络中扮演了重要的作用。2015 年，Andrew Lavin 等人提出了快速计算卷积的算法 Winograd，通过降低计算复杂度，相比直接卷积的算法提升 4 倍效率，成为了深度学习中非常重要的一个算法。本次比赛的第一题就是优化 Winograd 算法。各参赛队的通过优化比赛方给出的 Winograd 算法代码，缩短其运行时间，提升该算法时间的每秒浮点计算次数（FLOPS）。</p><h3 id="第二题"><a href="#第二题" class="headerlink" title="第二题"></a>第二题</h3><p>金融数据是真正的“大数据”。每天市场上的交易会产生海量的数据，这些数据对于预测未来市场走势只非常重要。因此负责高速储存、读取这些数据的 IO 系统成为了行业内重要的一环。目前金融数据中常用 HDF5 文件系统库进行大规模的数据存储。本次比赛第二题要求各参赛队探索 HDF5 文件系统，通过一个跑分程序 h5bench 来完成 IO 系统的性能研究和调优。</p><h2 id="代码-amp-文档"><a href="#代码-amp-文档" class="headerlink" title="代码 &amp; 文档"></a>代码 &amp; 文档</h2><p>从赛题可以看到，此两题能够检验选手最大化压榨 CPU 和 IO 性能的能力。</p><p>有关赛题的代码和文档均已开源，可移步 <a href="https://github.com/Sunny-Island/winograd-onlyJava" target="_blank" rel="noopener">此处</a> 查看。欢迎交流~</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>就 CPU 题目而言，我们此次尝试了以下优化和工具：</p><ul><li>算法优化：winograd4x3-3D</li><li>指令级并行：循环展开，分支预测</li><li>数据级并行：AVX128，256，512</li><li>线程级并行：OPENMP</li><li>编译器：尝试 gcc 不同版本，对比 llvm</li><li>内存排布：36*STRIDE</li><li>冒险尝试：merge_array</li><li>Profiling：perf</li></ul><p>其实这里有好多思想都已经在数据库领域存在了。比如向量化引擎，比如 codegen 的 llvm 优化，比如对 cache 友好的 push 查询引擎等等。</p><p>个人认为，数据库做到极致便是对硬件性能的一种体现。因此，一个优秀的数据库工程师应该对体系结构具有一定的了解，这样才有可能进一步压榨硬件性能，从而达到更好的数据库性能。</p><p>一直以来，我希望分布式数据库能够成为自己的一个标签。通过这次比赛，我意识到高性能计算也是一个很有趣且硬核的方向，其不仅能够给企业迅速带来真金白银的收益（节约成本），而且也是很多领域做到极致的一种出路。</p><p>希望未来还能有契机去进一步深挖此方向吧。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;2021 年 9 月，量化头部公司&lt;strong&gt;九坤&lt;/strong&gt;举办了其第一届&lt;strong&gt;并行程序优化大赛&lt;/strong&gt;，相</summary>
      
    
    
    
    
    <category term="高性能计算" scheme="https://tanxinyu.work/tags/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>2021 中科院开源之夏总结</title>
    <link href="https://tanxinyu.work/2021-summer-of-code/"/>
    <id>https://tanxinyu.work/2021-summer-of-code/</id>
    <published>2021-09-30T14:33:56.000Z</published>
    <updated>2022-01-30T03:39:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>2021 年暑假，我参与了中科院组织的开源之夏活动，题目为 《Apache IoTDB 分布式混沌测试框架》。</p><p>有关该项目的详细信息可以查看该 <a href="https://gitlab.summer-ospp.ac.cn/summer2021/210070607" target="_blank" rel="noopener">文档</a>。</p><p>从结果来看，这份工作发现了 Apache IoTDB 当前分布式版本存在的很多问题，有一些容易解决的问题已经得到了修复，然而也有一些较复杂的问题到今天依然存在，这也多多少少间接引起了我们的一次大规模重构，勉强算是一件有意义的工作吧。</p><p>令人略感遗憾的是，尽管该混沌测试框架在部署好之后可以用 Dashboard 的方式方便地注入特定的异常，然而正如项目文档中所说的，该框架依然是基于物理节点来实现的，很难做到自动化。</p><p><strong>如果没有测试人员去维护并定期手动测试，如果没有开发人员愿意抽出时间来完全解决其中发现的问题，如果整个团队没有足够重视异常场景下系统的对外表现并愿意为之付出大量的精力，该框架就很难形成正向反馈，最终只能被遗忘在历史的角落里。</strong></p><p>作为一点反省，我现在觉得混沌测试还是应该尽可能的通过持续集成的方式自动化起来（参照 ChaosMesh），这样释放人力的方式是大家都喜爱的，也只有这样，混沌测试才能对项目产生持续的正向收益。</p><p>随意写点儿感想，仅做记录。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;2021 年暑假，我参与了中科院组织的开源之夏活动，题目为 《Apache IoTDB 分布式混沌测试框架》。&lt;/p&gt;
&lt;p&gt;有关该项目的详细信息可以查看该 &lt;a href=&quot;https://gitlab.summer-ospp.ac.cn/summer2021/21007</summary>
      
    
    
    
    
    <category term="测试" scheme="https://tanxinyu.work/tags/%E6%B5%8B%E8%AF%95/"/>
    
    <category term="IoTDB" scheme="https://tanxinyu.work/tags/IoTDB/"/>
    
  </entry>
  
  <entry>
    <title>15-445 数据库课程学习总结</title>
    <link href="https://tanxinyu.work/15-445/"/>
    <id>https://tanxinyu.work/15-445/</id>
    <published>2021-09-12T07:12:32.000Z</published>
    <updated>2022-01-30T03:39:50.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>众所周知，CMU 15-445/721 是数据库的入门神课，类似于 MIT 6.824 之于分布式系统一样。由于前半年学习了 MIT 6.824 课程后感觉个人收获很大，因此在今年暑假，我抽时间学习完了 CMU 15-445 的网课，现做一概要总结。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>15-445 可以当做数据库的入门课程，授课老师是著名网红教授 <a href="http://www.cs.cmu.edu/~pavlo/" target="_blank" rel="noopener">Andy Pavlo</a>，以下是他的 Google Scholar 主页，还是非常厉害的。<br><img src="/15-445/pavlo.png" srcset="/img/loading.gif" lazyload alt></p><p>本课程的组织方式采用了自底向上的方式，分别介绍了文件管理，缓冲池管理，索引管理，执行管理，查询优化，并发控制和容错恢复等内容，基本讲述了如何从 0 实现一个单机关系型数据库。由于时间有限，没来得及做课程笔记。因此在参考资料部分列出了课程所有的 PPT 资料以及一些从网上找到的优质课程笔记，以备日后温习之用。</p><p><img src="/15-445/outline.png" srcset="/img/loading.gif" lazyload alt></p><p>当然，由于课程内容涉及的范围很广，所以每个章节都只是进行了相对简单的介绍。要想了解更多细节，建议结合大黑砖《数据库系统概念》来学习。2021 年 6 月，最新第七版的中文译版已经发行，赶紧买一本镇脑吧！</p><p><img src="/15-445/database.jpeg" srcset="/img/loading.gif" lazyload alt></p><p>对于其作业 <a href="https://github.com/cmu-db/bustub" target="_blank" rel="noopener">bustub</a>，由于其需要基于 C++17 实现，而本人在目前没有太多的 C++ 知识储备，所以就暂时搁置了，毕竟想学的是数据库而不是 C++。不过我也注意到，MIT 6.830 数据库课程的作业 <a href="https://github.com/MIT-DB-Class/simple-db-hw-2021" target="_blank" rel="noopener">simple-db</a> 是基于 Java 的，且其 6 个 lab 的内容基本覆盖了 CMU 15-445 lab 的内容，所以刷一刷 MIT 6.830 的 lab 也挺有意义的，希望自己后半年能抽出些时间吧。</p><p>此外，简单看了一下 15-721 的 <a href="https://15721.courses.cs.cmu.edu/spring2020/schedule.html" target="_blank" rel="noopener">课程主页</a>，感觉其更多的是在讲 research 方向的工作，基本是在讲各个方向的 sota，那么这门课可以等到工作之后再说吧，目前来看优先级不是很高。</p><p><img src="/15-445/15-721.png" srcset="/img/loading.gif" lazyload alt></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://15445.courses.cs.cmu.edu/fall2019/schedule.html" target="_blank" rel="noopener">官网资料</a></li><li><a href="https://www.bilibili.com/video/BV1rN411f7Ef" target="_blank" rel="noopener">网课视频</a></li><li><a href="https://zhenghe.gitbook.io/open-courses/cmu-15-445-645-database-systems/relational-data-model" target="_blank" rel="noopener">课程笔记 1</a></li><li><a href="https://www.jianshu.com/nb/36265841" target="_blank" rel="noopener">课程笔记 2</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;众所周知，CMU 15-445/721 是数据库的入门神课，类似于 MIT 6.824 之于分布式系统一样。由于前半年学习了 MIT 6.8</summary>
      
    
    
    
    
    <category term="数据库" scheme="https://tanxinyu.work/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    <category term="网红课" scheme="https://tanxinyu.work/tags/%E7%BD%91%E7%BA%A2%E8%AF%BE/"/>
    
  </entry>
  
  <entry>
    <title>Awesome 学习资料分享</title>
    <link href="https://tanxinyu.work/awesome-blog/"/>
    <id>https://tanxinyu.work/awesome-blog/</id>
    <published>2021-07-30T04:33:57.000Z</published>
    <updated>2022-01-30T03:40:05.000Z</updated>
    
    <content type="html"><![CDATA[<p>计划长期维护一个自己学习过且觉得不错的资料列表，希望自己不断更新：</p><ul><li><a href="https://github.com/fuzhengwei/itstack-demo-design" target="_blank" rel="noopener">重学 Java 设计模式</a></li><li><a href="https://icyfenix.cn/introduction/about-the-fenix-project.html" target="_blank" rel="noopener">什么是“凤凰架构”</a></li><li><a href="https://www.kancloud.cn/kancloud/a-programmer-prepares/78160" target="_blank" rel="noopener">程序员的自我修养</a></li><li><a href="https://draveness.me/" target="_blank" rel="noopener">为什么系列</a></li><li><a href="https://github.com/Vonng/ddia" target="_blank" rel="noopener">DDIA</a></li><li><a href="https://netsecurity.51cto.com/art/202005/616765.htm" target="_blank" rel="noopener">线上故障排查全套路</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;计划长期维护一个自己学习过且觉得不错的资料列表，希望自己不断更新：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/fuzhengwei/itstack-demo-design&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;</summary>
      
    
    
    
    
    <category term="开源" scheme="https://tanxinyu.work/tags/%E5%BC%80%E6%BA%90/"/>
    
  </entry>
  
  <entry>
    <title>6.824 分布式系统课程学习总结</title>
    <link href="https://tanxinyu.work/6-824/"/>
    <id>https://tanxinyu.work/6-824/</id>
    <published>2021-06-30T13:59:37.000Z</published>
    <updated>2022-03-05T06:18:40.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Lab"><a href="#Lab" class="headerlink" title="Lab"></a>Lab</h2><p>2021 年 6 月 30 日，本人总算刷完了 6.824 的 lab 并整理完了文档，发篇博客庆祝一下！！！</p><p>目前能够稳定通过 6.824 lab 所有的测试，并尽可能的提升了代码可读性。</p><p>不保证绝对的 bug-free，但每个 lab 均测试 500 次以上，无一 fail。</p><p>为了遵守课程代码开放协议，只开源了文档，具体可参考 <a href="https://github.com/OneSizeFitsQuorum/MIT6.824-2021" target="_blank" rel="noopener">repo</a>。</p><p>配合 <a href="https://github.com/OneSizeFitsQuorum/raft-thesis-zh_cn" target="_blank" rel="noopener">raft 博士论文翻译</a> 和 <a href="https://tanxinyu.work/raft/">raft 算法介绍</a> 阅读效果更佳。</p><p>如有收获，希望点个 star 以表支持。十分感谢！</p><h2 id="课程内容"><a href="#课程内容" class="headerlink" title="课程内容"></a>课程内容</h2><p>基本上每篇论文都结合课程内容做了对应的论文阅读笔记。链接如下：</p><ul><li><a href="https://tanxinyu.work/mapreduce-thesis/">MapReduce</a></li><li><a href="https://tanxinyu.work/gfs-thesis/">GFS</a></li><li><a href="https://tanxinyu.work/vm-ft-thesis/">VM-FT</a></li><li><a href="https://tanxinyu.work/raft/">Raft</a></li><li><a href="https://tanxinyu.work/zookeeper-thesis/">Zookeeper</a></li><li><a href="https://tanxinyu.work/chain-replication-thesis/">Chain-Replication</a></li><li><a href="https://tanxinyu.work/aurora-thesis/">Aurora</a></li><li><a href="https://tanxinyu.work/frangipani-thesis/">Frangipani</a></li><li><a href="https://tanxinyu.work/spanner-thesis/">Spanner</a></li><li><a href="https://tanxinyu.work/farm-thesis/">Farm</a></li><li><a href="https://tanxinyu.work/spark-thesis/">Spark</a></li><li><a href="https://tanxinyu.work/scaling-memcached-thesis/">Memcached-in-Facebook</a></li><li><a href="https://tanxinyu.work/cops-thesis/">COPS</a></li><li><a href="https://tanxinyu.work/bitcoin/">BitCoin</a></li></ul><p>由于时间有限，以上博客中参考了大量其他优质博客，本人在此对于这些博客的作者表示真诚的感谢。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Lab&quot;&gt;&lt;a href=&quot;#Lab&quot; class=&quot;headerlink&quot; title=&quot;Lab&quot;&gt;&lt;/a&gt;Lab&lt;/h2&gt;&lt;p&gt;2021 年 6 月 30 日，本人总算刷完了 6.824 的 lab 并整理完了文档，发篇博客庆祝一下！！！&lt;/p&gt;
&lt;p&gt;目前能</summary>
      
    
    
    
    
    <category term="网红课" scheme="https://tanxinyu.work/tags/%E7%BD%91%E7%BA%A2%E8%AF%BE/"/>
    
    <category term="分布式存储" scheme="https://tanxinyu.work/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/"/>
    
    <category term="分布式系统理论" scheme="https://tanxinyu.work/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%90%86%E8%AE%BA/"/>
    
    <category term="共识算法" scheme="https://tanxinyu.work/tags/%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>COPS 论文阅读</title>
    <link href="https://tanxinyu.work/cops-thesis/"/>
    <id>https://tanxinyu.work/cops-thesis/</id>
    <published>2021-06-09T12:41:23.000Z</published>
    <updated>2022-01-30T03:41:27.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>考虑这么一个问题：对于大型网站的异地复制，公司存在多个数据中心，每个数据中心拥有全量数据，读均为本地读。那么写应该怎么实现？一致性应该如何取舍？</p><p>对于以上背景，Spanner 和 Facebook/Memcached 已经给出了他们自己的解决方案：</p><ul><li>Spanner： <ul><li>线性一致性。</li><li>通过 Paxos 和 2PC 实现分布式写事务。</li><li>写入需要等待 quorum 的数据中心返回 ack，性能较低。</li><li>读取从本地数据中心读，可能需要等待，性能较高。</li></ul></li><li>Facebook/Memcached：<ul><li>最终一致性。</li><li>写入需要主数据中心返回 ack，性能一般。</li><li>读取从本地数据中心的缓存中读，性能极快。</li></ul></li></ul><p>基于以上背景，本论文在性能和一致性之间找出了一个 trade-off，给出了一种中间状态的一致性：因果一致性，其相比线性一致性更弱但相比最终一致性更强。因此，其性能也会处于中间的状态。</p><p>有关因果一致性的定义，建议先阅读此 <a href="https://mp.weixin.qq.com/s?__biz=MzA4NTg1MjM0Mg==&amp;mid=2657261809&amp;idx=1&amp;sn=cff64fe049a8a04ae719b34e7bf57dd1&amp;chksm=84479128b330183e55e911bacd611c22541f734a8df0c22a7bbe73323baedf6e506a12e8d2ac&amp;scene=178&amp;cur_album_id=1550842358601187329#rd" target="_blank" rel="noopener">博客</a>。</p><p>以下简单介绍其实现。</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>COPS（订单保留服务器集群）是一种地理复制的键值存储系统，可保证因果一致性。它包含两个软件组件：客户端库和键值存储。</p><p>涉及的每个数据中心都有一个本地 COPS 集群，该集群维护其整个数据集的副本。COPS 客户端是使用客户端库与键值存储交互的应用程序。客户端仅与在同一数据中心运行的本地 COPS 集群交互。</p><p>COPS 跨集群中的节点对存储的数据进行分片，每个键属于每个集群中的一个主节点。此主节点接收键的写入。写入完成后，本地集群中的主节点将其复制到其他集群中的主节点。</p><p>每个键也有版本，代表该键的不同值。COPS 保证一旦副本返回了密钥的版本，副本将只返回该版本或后续请求中的因果更新版本。</p><h3 id="定义因果关系"><a href="#定义因果关系" class="headerlink" title="定义因果关系"></a>定义因果关系</h3><p>更正式地，该论文提到了作者用来定义操作之间潜在因果关系的三个规则，表示为-&gt;：</p><ul><li>执行线程：如果 a 和 b 是单个执行线程中的两个操作，则 a -&gt; b 如果操作 a 在操作 b 之前发生。</li><li>从获取：如果 a 是 put 操作，b 是 get 操作，返回 a 写入的值，则 a -&gt; b。</li><li>传递性：对于操作 a、b 和 c，如果 a -&gt;b 和 b -&gt; c，则 a -&gt; c</li></ul><p>图 2 中的执行说明了这些规则。</p><p><img src="/cops-thesis/causal.png" srcset="/img/loading.gif" lazyload alt></p><p>此外，因果一致性不排序并发操作。如果我们不能判断一个操作在另一个之前发生，我们就说两个操作是并发的。一个系统可以以任何顺序复制两个不相关的 put 操作，但是当 put 对同一个键有并发操作时，我们说它们是冲突的。</p><p>对于某些并发场景，<code>最后写入者胜</code>的规则能够保证集群对某一个值最终达成共识。然而应对某些<code>冲突写入</code>的场景，类似于 append 函数，原子计数器等场景，都需要更细致的考虑。</p><h3 id="上下文"><a href="#上下文" class="headerlink" title="上下文"></a>上下文</h3><p>每个客户端维护一个上下文来表示其操作的顺序。将此上下文视为包含项目的列表。每次操作后，客户端都会向其上下文中添加一个项目。这些项目在列表中的顺序捕获了版本之间的依赖关系。使用此上下文，客户端可以计算版本的依赖关系。</p><h3 id="LP-提供全局顺序"><a href="#LP-提供全局顺序" class="headerlink" title="LP 提供全局顺序"></a>LP 提供全局顺序</h3><p>使用 <code>Lamport Timestamp in higher bits + unique ID For Data Center in lower bits</code> 来支持所有操作的全序顺序。</p><p>通过结合逻辑时钟和 Wall Clock，即使不同数据中心的时间有较大误差，我们依然可以为所有操作给出一个全序排序。</p><p>时钟实现如下所示：</p><figure class="highlight pf"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs pf">T<span class="hljs-keyword">max</span> = highest version seen (<span class="hljs-keyword">from</span> <span class="hljs-literal">self</span> and others)<br>T = <span class="hljs-keyword">max</span>(T<span class="hljs-keyword">max</span> + <span class="hljs-number">1</span>, wall-clock time)<br></code></pre></div></td></tr></table></figure><h3 id="写入"><a href="#写入" class="headerlink" title="写入"></a>写入</h3><p>当客户端调用 put key 时，库会根据其上下文计算该 key 的依赖关系，并将该信息发送到本地主存储节点。在 COPS 集群写入所有计算的依赖项之前，此存储节点不会提交密钥的值。提交该值后，主存储节点使用 Lamport 时间戳为其分配一个唯一的版本号，并立即将该编号返回给客户端。通过不等待复制完成，COPS 消除了具有更强一致性保证的系统产生的大部分延迟。</p><p>主存储节点在本地提交写入后，将写入异步复制到其他集群。节点在复制它时包含有关写入依赖项的信息。当另一个集群中的节点收到此写入时，该节点会检查其集群中的本地节点是否满足所有依赖项。接收节点通过向负责这些依赖关系的本地节点发出依赖关系检查请求来做到这一点。如果本地节点没有写入依赖值，它会阻塞请求，直到写入该值。否则，它会立即响应。</p><p>总之，COPS 通过计算写入的依赖关系来保证因果一致性，并且在集群提交所有依赖关系之前不会在集群中提交写入。</p><h3 id="读取"><a href="#读取" class="headerlink" title="读取"></a>读取</h3><p>COPS 也满足本地集群中的读取。COPS 客户端可以指定他们是要读取密钥的最新版本还是特定的旧版本。当客户端库接收的读取响应，它增加了操作的情况下捕捉到潜在的因果关系。</p><h2 id="限制"><a href="#限制" class="headerlink" title="限制"></a>限制</h2><p>虽然因果一致性是一个流行的研究思想，但它有一些局限性。两个主要是：</p><ul><li>它无法捕获外部因果依赖性。一个典型的例子是一个电话：如果我做动作 A，打电话给我在另一个大陆的朋友告诉她关于 A 的事情，然后她做了动作 B，系统将无法捕捉到 A 和 B 之间的因果关系。针对此问题，Lamport 早已经给出了答案：只能通过强同步的物理时钟来实现，具体可以参考本人另一篇 <a href="https://tanxinyu.work/time-clock-order-in-distributed-system-thesis/">博客</a>。</li><li>管理冲突可能很困难，尤其是当<code>最后写入者胜</code>规则不够用时。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>简单学习一下因果一致性，虽然讲义中提到它在实际系统中应用不多，但个人感觉这是一个很有意义的方向。在未来，对于不追求数据线性一致性的场景，很可能跨数据中心的同步范式都会参考因果一致性而不是最终一致性。虽然牺牲了一点性能，但前者相比后者保证了一定程度的 safety，这是非常可贵的。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="http://nil.csail.mit.edu/6.824/2020/notes/l-cops.txt" target="_blank" rel="noopener">6.824 讲义</a></li><li><a href="http://nil.csail.mit.edu/6.824/2020/video/17.html" target="_blank" rel="noopener">6.824 视频</a></li><li><a href="http://nil.csail.mit.edu/6.824/2020/papers/cops.pdf" target="_blank" rel="noopener">论文</a></li><li><a href="https://timilearning.com/posts/mit-6.824/lecture-17-cops/#lamport-timestamps-provide-a-global-order" target="_blank" rel="noopener">MIT 6.824: Lecture 17 - Causal Consistency, COPS</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;考虑这么一个问题：对于大型网站的异地复制，公司存在多个数据中心，每个数据中心拥有全量数据，读均为本地读。那么写应该怎么实现？一致性应该如何取</summary>
      
    
    
    
    
    <category term="论文阅读" scheme="https://tanxinyu.work/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    <category term="分布式系统理论" scheme="https://tanxinyu.work/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%90%86%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>Facebook 的 Memcached 系统扩展论文阅读</title>
    <link href="https://tanxinyu.work/scaling-memcached-thesis/"/>
    <id>https://tanxinyu.work/scaling-memcached-thesis/</id>
    <published>2021-05-30T05:56:16.000Z</published>
    <updated>2022-01-30T03:43:01.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>本篇论文由 Facebook 2013 年在 NSDI 上发表，其系统地介绍了 Facebook 公司内部对于大规模缓存系统的使用实践。</p><p>这篇论文没有太多新的想法，但却传达了一个很重要的理念：不同的场景有不同的系统需求。在成本有限的情况下，并不一定线性一致性就是最好的，对于有些场景，最终一致性带来的收益远超我们的想像。</p><p>对于大部分 2C 应用，随着用户数量的逐渐增加，其后台数据存储系统大致是如此的进化路线：</p><ol><li><strong>单机 Web 服务器 + 单机数据库（MySQL / Oracle）</strong>： 随着负载增加，单机 Web 服务器逐渐占满了 CPU，需要横向扩展。</li><li><strong>多台无状态 Web 服务器 + 共享的单机数据库（MySQL / Oracle）</strong>：无状态的 Web 服务器可以横向扩展以获得更高的吞吐量。随着负载进一步增加，单机数据库成为了瓶颈。</li><li><strong>多台无状态 Web 服务器 + 关系数据库集群（分库分表 / NewSQL 数据库）</strong>： 横向扩展了关系数据库的性能，这里可以参考分库分表或者一些 NewSQL 产品。对于前者，跨分区的事务会是一个痛点。此外，这类应用的业务场景一般情况下都是读多写少的场景。对于写性能，只能通过横向扩展数据库的方式来解决。对于读性能，除了横向扩展数据库，还可以加一层缓存层以提升系统的吞吐量，同时也减少数据库的负载。</li><li><strong>多台无状态 Web 服务器 + 用于读加速的分布式缓存系统（Redis / Memcached） + 关系数据库集群（分库分表 / NewSQL 数据库）</strong>：此时对于读写性能基本都能够扩展，关系数据库中的数据也可以通过 CDC 同步到下游去做离线或近实时计算。此时需要进一步关注的便是缓存系统的一致性。</li></ol><p>脸书在 2013 年就已经进化到了第四个阶段，以下会简单介绍其架构：</p><h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p>Facebook 的架构包括多个 web、memcached 和数据库服务器。一组 web 和 memcached 服务器组成一个前端集群，多个前端集群组成一个数据中心。这些数据中心在论文中称为 region 。一个 regions 内的前端集群共享同一个存储集群。Facebook 在全球不同地区复制集群，将一个 region 指定为主 region，将其他 region 指定为次要 region。</p><p>其架构图如下：</p><p><img src="/scaling-memcached-thesis/architecture.png" srcset="/img/loading.gif" lazyload alt></p><p>对于脸书的场景，其数据一般都是用户信息，好友信息，帖子信息，喜欢信息，照片信息等等。这些消息主要有两个特点：</p><ul><li>用户能够容忍适度的旧数据，但不能容忍非常旧的数据。</li><li>用户想要能够读自己所写。</li></ul><p>基于以上特点，Facebook 使用了 memcached 来减少其数据库的读取负载。Facebook 的工作负载以读取为主，而 memcached 可防止它们为每个请求访问数据库。他们使用 memcached 作为后备缓存。这意味着当 Web 服务器需要数据时，它首先尝试从缓存中获取数据。如果该值不在缓存中，Web 服务器将从数据库中获取数据，然后用数据填充缓存。</p><p>对于写入，Web 服务器会将键的新值发送到数据库，然后向缓存发送另一个请求以删除键。对该键的后续读取将从数据库中获取最新数据。</p><p>流程如下图所示：<br><img src="/scaling-memcached-thesis/cache.png" srcset="/img/loading.gif" lazyload alt></p><p>至于为什么在写数据库成功时要删除缓存系统中的 key 而不是 set 进去当前写入成功 key 的 value，大致原因是为了降低并发写入时出现 stale read 的概率，从而进一步满足用户读自己所写的需求。</p><p>需要注意的是，即使使用了 delete 的方案，还是不能完全避免 stale read 的可能性。归根原因，是因为在 db 处的更新和在 cache 处的更新很难保证原子性，我们只能尽量减少其不一致的概率而很难完全避免它。因此，对于每个 key 都设置合理的 TTL 时间和缓存过期策略也十分重要，这其实相当于给系统不一致的阈值定了一个上限，从而保证了最终一致性。这一点本论文没有介绍，但 Twitter 的内存缓存系统有介绍，可以参考本人另一篇 <a href="https://tanxinyu.work/twitter-cache-analysis-thesis/">阅读博客</a>。</p><p>对于如何保证数据库和缓存的一致性，该 <a href="https://mp.weixin.qq.com/s/Ii0b6ORmsxjmXsVRnhG5eA" target="_blank" rel="noopener">博客</a> 有一些有趣的思考。</p><p>对于缓存穿透，缓存击穿，缓存雪崩，可以简单看看 <a href="https://mp.weixin.qq.com/s/e9NqNZD5_kjf6nfzFtSY3w" target="_blank" rel="noopener">八股文</a>。</p><p>此外，缓存系统一般由两种并行处理策略：</p><ul><li>分片<ul><li>节省内存：每个 key 一个副本。</li><li>可横向扩展：流量均匀时能够均匀的利用所有节点的资源。</li><li>更多的长连接：客户端节点需要与很多节点建立连接。</li></ul></li><li>复制<ul><li>浪费内存：每个 key 多个副本。</li><li>对 Hot Key 友好：分片对 Hot Key 无帮助，而复制可以提升 Hot Key 的吞吐量。 </li><li>更少的长连接：一个节点上可能有多个分片的副本。</li></ul></li></ul><p>根据情况可以做取舍，一般分片和复制都是需要做的。</p><p>对于 Facebook，其存在两个全量异步复制的 region ，这一定程度上保证了跨 region 的容错，同时也减少了不同地域用户的读延迟。</p><p>在每个 region 内部，其首先将数据库在数据库层做了分片以支持高性能的横向扩展，接着其利用多个缓存集群缓存了相同的 Hot Key 来共享负载，同时其也单独将不那么 hot 的 key 放到一个默认缓存集群中以减少缓存成本。</p><p>对于工业界系统，Facebook 还仔细设计了网络协议（UDP 读，TCP 写），流量控制，网络布局，缓存集群冷启动，缓存节点宕机等实际情况。感兴趣的可以关注其论文，此处不再赘述。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>简单介绍了 Facebook 使用 Memcached 的实践。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="http://nil.csail.mit.edu/6.824/2020/notes/l-memcached.txt" target="_blank" rel="noopener">6.824 讲义</a></li><li><a href="http://nil.csail.mit.edu/6.824/2020/video/16.html" target="_blank" rel="noopener">6.824 视频</a></li><li><a href="http://nil.csail.mit.edu/6.824/2020/papers/memcache-fb.pdf" target="_blank" rel="noopener">论文</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;本篇论文由 Facebook 2013 年在 NSDI 上发表，其系统地介绍了 Facebook 公司内部对于大规模缓存系统的使用实践。&lt;/</summary>
      
    
    
    
    
    <category term="论文阅读" scheme="https://tanxinyu.work/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    <category term="分布式存储" scheme="https://tanxinyu.work/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/"/>
    
  </entry>
  
  <entry>
    <title>Bitcoin 论文阅读</title>
    <link href="https://tanxinyu.work/bitcoin/"/>
    <id>https://tanxinyu.work/bitcoin/</id>
    <published>2021-05-25T08:14:02.000Z</published>
    <updated>2022-01-30T03:41:03.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="相关背景"><a href="#相关背景" class="headerlink" title="相关背景"></a>相关背景</h2><p>2008 年，中本聪设计出了比特币和区块链。在今天，区块链成为了热门的技术，其通过分布式账本技术和共识机制，构建了低成本互信机制。</p><p>区块链三个根本特性是去中心化、实现点对点的价值传递和低成本信任机制。</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>区块链是一个划时代的技术，其使得我们能够不假装相信一个中央机构而实现可信的交易。</p><p>下面简单讨论一下一些设计要点：</p><p>首先，在公网的 P2P 系统中，我们必须设计出支持拜占庭容错的共识算法。相比 Raft 等非拜占庭容错的共识算法，区块链算法本质上利用了真实物理硬件的工作量证明 POF(Proof of Work) 而非可以随意捏造的 IP，域名等信息来标识节点从而解决女巫问题，其可以保证：只要网络中的大多数节点是无恶意的，恶意的节点就无法干被大家都承认的坏事。</p><p>其次，对于双花问题，其保证了最终只会有一个交易被所有节点均认可。这主要是由于区块链的 fork 机制。</p><p>最后，区块链精妙的利用了密码学知识，从理论上隔绝了钱被别人乱花的可能性，这使得黑客能做的也仅仅是欺诈而已。</p><p>总结一下，比特币虽然有不少缺点，比如性能低，交易延时高（交易至少需要 10 分钟，往往 1 个小时可信度会更高），浪费资源等，但其创新地设计了去中心化的分布式账本实现，这甚至使得人类对于交易的本质产生了更深刻的认识，从而进一步推进了人类文明的发展。</p><p>稍微详细的论文阅读笔记可以参考 <a href="https://www.cnblogs.com/xinzhao/p/8584477.html" target="_blank" rel="noopener">精读比特币论文</a>。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>简单记录一下 6.824 课程对比特币所学。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="http://nil.csail.mit.edu/6.824/2020/notes/l-bitcoin.txt" target="_blank" rel="noopener">6.824 讲义</a></li><li><a href="http://nil.csail.mit.edu/6.824/2020/video/19.html" target="_blank" rel="noopener">6.824 视频</a></li><li><a href="http://nil.csail.mit.edu/6.824/2020/papers/bitcoin.pdf" target="_blank" rel="noopener">论文</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;相关背景&quot;&gt;&lt;a href=&quot;#相关背景&quot; class=&quot;headerlink&quot; title=&quot;相关背景&quot;&gt;&lt;/a&gt;相关背景&lt;/h2&gt;&lt;p&gt;2008 年，中本聪设计出了比特币和区块链。在今天，区块链成为了热门的技术，其通过分布式账本技术和共识机制，构建了低成本互信</summary>
      
    
    
    
    
    <category term="分布式存储" scheme="https://tanxinyu.work/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/"/>
    
  </entry>
  
  <entry>
    <title>Spanner 论文阅读</title>
    <link href="https://tanxinyu.work/spanner-thesis/"/>
    <id>https://tanxinyu.work/spanner-thesis/</id>
    <published>2021-05-19T16:28:08.000Z</published>
    <updated>2022-01-30T03:43:06.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="相关背景"><a href="#相关背景" class="headerlink" title="相关背景"></a>相关背景</h2><p>Google Spanner 是 Google 一篇跨时代的论文，开启了 NewSQL 时代的序幕。</p><p>其主要有三点特色：</p><ul><li>2PC + 共识组来避免 2PC 的无限超时阻塞。</li><li>GPS 原子钟同步技术以支持快速的只读事务。</li><li>支持 ACID 的全球型 NewSQL 数据库。</li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>暂仅搬运一些资料，之后有时间再补。</p><ul><li><a href="http://nil.csail.mit.edu/6.824/2020/notes/l-spanner.txt" target="_blank" rel="noopener">6.824 讲义</a></li><li><a href="http://nil.csail.mit.edu/6.824/2020/video/13.html" target="_blank" rel="noopener">6.824 视频</a></li><li><a href="http://nil.csail.mit.edu/6.824/2020/papers/spanner.pdf" target="_blank" rel="noopener">论文</a></li><li><a href="https://cloud.google.com/spanner/docs/whitepapers?hl=zh-cn" target="_blank" rel="noopener">Spanner 白皮书</a></li><li><a href="https://toutiao.io/posts/zdqrx0/preview" target="_blank" rel="noopener">Spanner，True Time 和 CAP</a></li><li><a href="https://zhuanlan.zhihu.com/p/47870235" target="_blank" rel="noopener">Spanner 十问</a></li></ul><h2 id="问题记录"><a href="#问题记录" class="headerlink" title="问题记录"></a>问题记录</h2><ol><li>在 Spanner 中，只读事务可以从本地数据中心读取数据，以提高性能。但是，如果只读事务向少数副本发出请求，它可能得到过时的数据。这种行为可能会破坏严格的可序列化保证。那么 Spanner 如何应对这种情况以保持其正确性条件呢？</li></ol><blockquote><p>在 Spanner 中，每个事务都会根据 TrueTime 的 Start Rule 选择一个时间戳作为事务 id：对于写请求，其会在提交事务时 2pc 的 prepare 阶段选择 TT.now().latest 作为事务 id；对于读请求，其会在读事务开始时选择 TT.now().latest 作为事务 id。</p><p>每个 replica 都会维护一个递增的 t<sub>safe</sub>  变量，对于读事务 T，当 t<sub>safe</sub>  &gt;= 读事务 T 的 tid 时，可以执行该读事务而不违背外部一致性。</p><p>对于 t_safe，其等于 min(t<sub>safe</sub><sup>paxos</sup>,t<sub>safe</sub><sup>tm</sup>)。对于 t<sub>safe</sub><sup>paxos</sup>，其等于当前 replica 能够看到的最新写事务的 tid，注意到 Paxos 的 leader 会将写入事务按照时间序发送，因此一旦某 replica 发现了已经存在 tid 大于 T 的写事务，则表明所有 tid 小于等于读事务 T 的已提交写入事务均已同步到本地。对于 t <sub>safe</sub><sup>tm</sup>，需要了解每个 paxos group 都会有 replica 个数个 transaction manager，follower 的 transaction manager 可以根据 leader 发送过来的日志保持与 leader 的同步。如果当前 replica 的 transaction manager 不存在已 prepare 但还未 commit/abort 的事务，则 t<sub>safe</sub><sup>tm</sup> 为正无穷；否则为最小的已 prepare 但还未 commit/abort 事务的 tid – 1。对于 tid 大于 t<sub>safe</sub><sup>tm</sup> 的读事务，直接去读也是不安全的。因为这部分还未提交的事务可能会提交，直接读的话便会漏掉这些数据。</p><p>因此对于一个读事务 T，一旦某 replica 发现了本地维护的 t<sub>safe</sub> &gt;= 读事务 T 的 tid，则可以直接执行读事务，这不会违背外部一致性。否则该 replica 需要等待直到本地维护的 t<sub>safe</sub> 大于 读事务 T 的 tid 为止。</p><p>值得注意的是，这样的实现在部分场景下可能也有 liveness 的问题，比如该分片短期内没有新的写入事务且当前所有的事务都已 commit，即使所有 replica 都已经 catch up，t<sub>safe</sub> 仍然始终是最后一个写入事务的 tid。如果此时来了一个新的读事务，其似乎会被永远 block 住。因为该读事务的 tid 永远大于保持不动的 t<sub>safe</sub>。对于这种情况，该 replica 可以向 leader 发送一个 rpc，待 leader 等到 tt.now().earliest 大于该读事务的 tid 时返回 ack，此时该 replica 可以执行该读事务，因为未来产生的写事务 tid 都一定大于该读事务的 tid ，虽然其还未产生，但其一定不会对该读事务产生影响。</p></blockquote><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>简单记录一下 6.824 课程对 Google Spanner 所学。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;相关背景&quot;&gt;&lt;a href=&quot;#相关背景&quot; class=&quot;headerlink&quot; title=&quot;相关背景&quot;&gt;&lt;/a&gt;相关背景&lt;/h2&gt;&lt;p&gt;Google Spanner 是 Google 一篇跨时代的论文，开启了 NewSQL 时代的序幕。&lt;/p&gt;
&lt;p&gt;其主要有</summary>
      
    
    
    
    
    <category term="论文阅读" scheme="https://tanxinyu.work/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    <category term="分布式存储" scheme="https://tanxinyu.work/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/"/>
    
    <category term="Google" scheme="https://tanxinyu.work/tags/Google/"/>
    
  </entry>
  
  <entry>
    <title>Farm 论文阅读</title>
    <link href="https://tanxinyu.work/farm-thesis/"/>
    <id>https://tanxinyu.work/farm-thesis/</id>
    <published>2021-05-10T01:31:55.000Z</published>
    <updated>2022-01-30T03:41:37.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="相关背景"><a href="#相关背景" class="headerlink" title="相关背景"></a>相关背景</h2><p>Fram 结合了乐观事务和硬件优势，在保证可串行化的基础上实现了高性能的分布式事务框架。虽然其系统仅是一个原型系统，但其思想十分具有指导意义。</p><p>其主要利用了三点硬件特性：</p><ul><li>NV RAM：减少磁盘 IO 对性能的影响。（一次 RAM 写大致需要 200ns，一次 SSD 写大致需要 100us，一次 HDD 写大致需要 100ms）</li><li>Kernal Bypass：本地应用直接与网卡交互，无系统调用，无 CPU 参与。消除 Linux 内核网络栈对性能的影响，类似于 <a href="https://www.dpdk.org/" target="_blank" rel="noopener">dpdk</a>。</li><li>RDMA：跟远程节点交互时不需要远程节点的 CPU 参与，直接读对应内存，无系统调用，无 CPU 参与。消除 Linux 内核网络栈对性能的影响。</li></ul><p>其主要有一个特点：</p><ul><li>快：在 90 台数据分片过的机器上，可以达到 100 万事务/s，比 Spanner 快 100 倍。</li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>暂仅搬运一些资料，之后有时间再补。</p><ul><li><a href="http://nil.csail.mit.edu/6.824/2020/notes/l-farm.txt" target="_blank" rel="noopener">6.824 讲义</a></li><li><a href="http://nil.csail.mit.edu/6.824/2020/video/14.html" target="_blank" rel="noopener">6.824 视频</a></li><li><a href="https://pdos.csail.mit.edu/6.824/papers/farm-2015.pdf" target="_blank" rel="noopener">论文</a></li><li><a href="https://www.yuque.com/flyrzl/iv0mdq/fntw2a?language=zh-cn" target="_blank" rel="noopener">乐观并发控制 Optimistic Concurrency Control（OCC）</a></li><li><a href="https://www.jianshu.com/p/4128b38a2312" target="_blank" rel="noopener">不妥协：分布式事务的一致性，可用性和性能</a></li><li><a href="https://blog.csdn.net/hohomi77/article/details/102511679" target="_blank" rel="noopener">分布式系统：FaRM</a></li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>简单记录一下 6.824 课程对 Farm 所学。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;相关背景&quot;&gt;&lt;a href=&quot;#相关背景&quot; class=&quot;headerlink&quot; title=&quot;相关背景&quot;&gt;&lt;/a&gt;相关背景&lt;/h2&gt;&lt;p&gt;Fram 结合了乐观事务和硬件优势，在保证可串行化的基础上实现了高性能的分布式事务框架。虽然其系统仅是一个原型系统，但其思想</summary>
      
    
    
    
    
    <category term="论文阅读" scheme="https://tanxinyu.work/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    <category term="分布式系统理论" scheme="https://tanxinyu.work/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%90%86%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>分布式事务简介</title>
    <link href="https://tanxinyu.work/distributed-transactions/"/>
    <id>https://tanxinyu.work/distributed-transactions/</id>
    <published>2021-04-29T05:46:58.000Z</published>
    <updated>2022-01-30T03:41:32.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>事务是作为单个逻辑工作单元执行的一系列操作。一个逻辑工作单元必须有四个属性，称为原子性、一致性、隔离性和持久性 (ACID) 属性。分布式事务则是尝试在多节点的环境下实现这些语义。</p><p>分布式事务涉及的知识内容较多，本篇博客并没有将其彻底整理清楚，只是简单记录了一下 6.824 课程所学。</p><h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><p>对于分布式事务，可以将其细化为并发控制和原子提交两个子问题。前者是在说如何保证并发事务的串行隔离性，后者是在说当数据分片分布在不同的节点上时，如何保证事务在不同节点上提交与否的原子性。</p><h3 id="并发控制"><a href="#并发控制" class="headerlink" title="并发控制"></a>并发控制</h3><p>对于事务的并发控制模型，一般有两个方向：悲观事务和乐观事务。前者适合于数据竞争严重且重试代价大的场景，后者适用于数据竞争不严重且重试代价不大的场景。</p><h4 id="悲观事务"><a href="#悲观事务" class="headerlink" title="悲观事务"></a>悲观事务</h4><p>在事务执行过程中对事务所用到的数据都上锁。这里的锁从事务中第一次用到对应数据时开始，直到事务结束时才释放。</p><p>2PL（2-Phase-Lock）就是一种典型的悲观事务方法，但并不是到事务结束时才释放所有锁，而是事务过程中一旦不再使用某对象即可释放该对象的锁。因此，6.824 课程中介绍的 2PL 严格来说是 S2PL（Strict-2-Phase-Lock），即所有锁都是在事务结束的同时才释放的。S2PL 相比 2PL 的性能更差，但能够避免级联终止的发生，具体可以参考此 <a href="https://niceaz.com/2019/03/24/isolation-2pl-mvcc/" target="_blank" rel="noopener">博客</a>。</p><p>不论是 2PL 还是 S2PL，都有可能导致死锁，因此一般要从两个方面入手来避免死锁：死锁检测、死锁预防</p><ul><li>死锁检测：为相互等待的事务之间维护一张 graph， 检测 graph 中是否有环。如果检测到优化，则根据一定的策略选择终止一个事务，打破循环等待。</li><li>死锁预防：按照一定顺序进行加锁，锁超时则终止等。</li></ul><p>S2PL 严格意义上不能解决幻读的问题，因为其加的都是行锁，所以其并不能实现串行隔离性，这一点与 6.824 课程中的介绍似乎有些许出入，等到之后有时间再认真研究一番。</p><h4 id="乐观事务"><a href="#乐观事务" class="headerlink" title="乐观事务"></a>乐观事务</h4><p>在事务执行过程中获取事务所用到的数据时并不上锁，直到计算完毕提交时才加锁对数据进行验证，若无变化则提交，否则 abort 或重新获取最新数据并计算。</p><p>由于分布式乐观事务在提交之前获取数据进行计算时并不需要加锁，因此一般也可以通过结合 RDMA 等技术来显著提升性能。</p><h3 id="原子提交"><a href="#原子提交" class="headerlink" title="原子提交"></a>原子提交</h3><p>在分布式事务中，参与事务的所有节点必须全部执行 Commit 操作或全部执行 Abort 操作，即他们需要在”执行 Commit 还是 Abort”这一点上达成一致（其实就是共识）。理论上有许多原子提交协议：2PC 和 3PC 等等。</p><p>原子提交协议和共识协议：</p><ul><li>目的相同：<ul><li>共识问题：解决的是如何在分布式系统中的多个节点之间就某个提议达成共识。</li><li>原子提交问题：解决的是参与分布式事务的所有节点在”执行 Commit 还是 Abort”这一点上达成共识。</li></ul></li><li>范围不同：前者要求所有节点达成共识，后者要求大多数未故障的节点达成共识。</li></ul><p>有关 2PC, 3PC, TCC, SAGA 等原子提交协议的具体内容，可以参考此 <a href="https://mp.weixin.qq.com/s/MbPRpBudXtdfl8o4hlqNlQ" target="_blank" rel="noopener">博客</a>。</p><p>对于业务上的分布式事务，还有 Seata 和基于 MQ 的分布式事务实现等等，花样很多，但实现各异。他们不是本文的重点，感兴趣自行谷歌即可。</p><p>对于数据库内核中的分布式事务实现，一般都是通过 2PC 的方式。为什么不用 3PC 呢？2PC 严格来说是保证了 safety 但牺牲了较多的 liveness（部分场景下资源会被永远锁定），而且正常工作时需要发两轮 RPC 来提交一个事务，其性能是被很多人诟病的一点；3PC 虽然提高了 liveness（资源锁定一定会在有限时间内被解除），但其是以牺牲 safety 为代价的，同时正常工作时需要发三轮 RPC 来提交一个事务，性能更差，因此很多人认为使用 3PC 是实现分布式事务的一条歪路，工业界也几乎很少有人使用 3PC，这一点 PingCAP 的 CTO 在分布式之美论坛上也提到过。</p><p>事实上，不论是 Spanner 还是 TiDB，他们的方式都是通过将协调者和参与者都分别组成共识组的方式来避免单点故障，从而在保证 safety 的基础上提升 liveness（由于协调者节点的单点故障被共识组规避掉，参与者的资源锁定一定会在有限时间内被解除），当然，由于每一个操作和决定都需要在共识组中进行同步，不可避免的其性能也会更差，但相比 3PC，其至少没有牺牲 safety，这一点非常关键，因为只要保证了 safety，至少我们可以通过 scale out 的方式来提升性能。</p><p>对于 2PC，业界和学术界针对不同的数据模型也有一些特定的优化，比如针对 KV 模型的 Percolator 算法等，它可以将 2PC 在部分场景下优化为 1PC 并结合 Async Commit 的方式来提升性能，这里之后有时间再进行研究吧。</p><p>业界也有一些替换 2PC 的声音，可以参考此 <a href="https://www.jdon.com/51588" target="_blank" rel="noopener">博客</a>。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇博客简单记录了 6.824 课程中分布式事务的内容，同时进行了一点点分析和拓宽，以做记录。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="http://nil.csail.mit.edu/6.824/2020/notes/l-2pc.txt" target="_blank" rel="noopener">6.824 讲义</a></li><li><a href="http://nil.csail.mit.edu/6.824/2020/video/12.html" target="_blank" rel="noopener">6.824 视频</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;事务是作为单个逻辑工作单元执行的一系列操作。一个逻辑工作单元必须有四个属性，称为原子性、一致性、隔离性和持久性 (ACID) 属性。分布式事</summary>
      
    
    
    
    
    <category term="分布式系统理论" scheme="https://tanxinyu.work/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%90%86%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>Aurora 论文阅读</title>
    <link href="https://tanxinyu.work/aurora-thesis/"/>
    <id>https://tanxinyu.work/aurora-thesis/</id>
    <published>2021-04-20T15:44:04.000Z</published>
    <updated>2022-01-30T03:40:01.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="相关背景"><a href="#相关背景" class="headerlink" title="相关背景"></a>相关背景</h2><p>Amazon Aurora 是一种与 MySQL 和 PostgreSQL 兼容的关系数据库，专为云而打造，既具有传统企业数据库的性能和可用性，又具有开源数据库的简单性和成本效益。</p><p>Amazon Aurora 的速度最高可以达到标准 MySQL 数据库的五倍、标准 PostgreSQL 数据库的三倍。它可以实现商用数据库的安全性、可用性和可靠性，而成本只有商用数据库的 1/10。</p><p>关于 Amazon Aurora 的进一步细节，可以参考 <a href="https://aws.amazon.com/cn/rds/aurora/?nc2=type_a&amp;aurora-whats-new.sort-by=item.additionalFields.postDateTime&amp;aurora-whats-new.sort-order=desc" target="_blank" rel="noopener">AWS 官网</a>。</p><p>2017 年，Amazon 在 SIGMOD 上发表了 Aurora 的论文，详细介绍了 Aurora 的设计思路和架构细节，以下顺着 6.824 课程的思路对 Amazon Aurora 进行简单的介绍。</p><h2 id="EC2（Elastic-Compute-Cloud）"><a href="#EC2（Elastic-Compute-Cloud）" class="headerlink" title="EC2（Elastic Compute Cloud）"></a>EC2（Elastic Compute Cloud）</h2><p>EC2 是 Amazon 最成功的云计算产品之一，用户可以租用 EC2 实例来部署网页或数据库服务等等，EC2 可以服务支持在线扩容，实时备份等功能。</p><p>最开始每个 EC2 实例往往以虚拟机的形式运行在物理节点上，其所有的磁盘 IO 都会通过虚拟机转化到对应的物理节点本地挂载的物理磁盘上。</p><p>对于无状态的网页服务，EC2 使用起来十分方便，因为其不需要关注容错，且很容易通过横向扩展 + 负载均衡的方式来提升性能。</p><p>对于有状态的数据库服务，比如 MYSQL，EC2 使用起来有一些缺点：</p><ul><li>受限的扩展：类似于 MYSQL 读写分离，单机 MYSQL 在 EC2 上可以做到读扩展，但不能做到写扩展。</li><li>受限的容错：由于没有做冗余备份，物理节点一旦挂了，其磁盘上的数据暂时不可用。</li></ul><h2 id="EBS（Elastic-Block-Store）"><a href="#EBS（Elastic-Block-Store）" class="headerlink" title="EBS（Elastic Block Store）"></a>EBS（Elastic Block Store）</h2><p>基于以上 EC2 支持 MYSQL 服务的缺点，Amazon 设计了 EBS 来进一步提升容错能力。</p><p>EBS 是一组具有容错能力的存储服务器，对外的抽象类似于 EC2 实例的磁盘，其利用默认副本数为 2 的 Chain Replication 和基于 Paxos 的配置管理器实现了容错。</p><p>这样其实是将 EC2 从有状态变成了无状态，一旦某个 EC2 宕机，另外启动一个 EC2 实例挂载相同的 EBS 即可恢复之前的状态。</p><p>然而，这样的设计依然具有一些缺点：</p><ul><li>巨大的数据流量：由于 EBS 对外的抽象类似于磁盘，所以对于 MYSQL 来说，其脏页和 redo 日志都会被同步到 EBS 中去，尤其对于前者，即使只改动了一个 page 中的一个字节也需要传输整个 page。</li><li>依然受限的容错：EC2-on-EBS 虽然比 EC2-on-Local-Disk 的容错性高，但处于 Chain Replication 的性能考虑，一组 EBS 服务器一定会在一个 AZ（Amazon 对机房或数据中心的抽象）中。因此，其对于某个 AZ 的断电，洪水等灾害依然无法能够做到容错。</li></ul><h2 id="RDS（Relational-Database-Service）"><a href="#RDS（Relational-Database-Service）" class="headerlink" title="RDS（Relational Database Service）"></a>RDS（Relational Database Service）</h2><p>基于 EBS 的缺点，Amazon 又提供了 DBaaS（database-as-a-service） 的云服务 RDS，该服务的目标是提供跨 AZ 的容错，其架构图如下所示：</p><p><img src="/aurora-thesis/EBS.png" srcset="/img/loading.gif" lazyload alt></p><p>与 EBS 相比，RDS 的容错性更强，但由于同步时需要跨 AZ，这导致数据写性能进一步下降，而且在没有减少数据流量的同时还增加了巨大的跨 AZ 流量。</p><h2 id="Aurora"><a href="#Aurora" class="headerlink" title="Aurora"></a>Aurora</h2><p>基于以上服务的缺点，Amazon 明确了其理想的容错和性能需求：</p><ul><li>即使一个 AZ 断电，写服务依然正常。</li><li>即使一个 AZ 断电 + 另一个副本不可用，读服务依然正常，该需求被称作 AZ + 1。</li><li>容忍个别慢副本对性能造成的影响。</li><li>能够快速修复宕机的副本。</li></ul><p>基于以上需求，Amazon 设计出了兼容 Mysql 协议的云数据库 Aurora，其架构图如下所示：</p><p><img src="/aurora-thesis/Aurora.png" srcset="/img/loading.gif" lazyload alt></p><p>Aurora 主要有两个创新点，他们共同使得 Aurora 相比 RDS 版的 MYSQL 有了 35 倍的性能提升：</p><ul><li>quorum 写：采用了 2+2+2 的三中心六副本部署方案。写只需要四个副本返回 ack 即可，这样即可满足上述对容错能力和性能的需求。</li><li>减少数据同步流量：使存储服务器具备将日志应用到 page 的能力，这样即可只同步物理日志而不同步脏页，从而减少大量的数据同步流量。</li></ul><p>对于 Aurora 的读写流程和一致模型，可以参考此 <a href="https://zhuanlan.zhihu.com/p/319806107" target="_blank" rel="noopener">博客</a> 和此 <a href="https://zhuanlan.zhihu.com/p/338582762" target="_blank" rel="noopener">博客</a>。</p><p>值得一提的是：为了存储层可扩展，Aurora 会将数据库文件切分成 10GB 大小的 segment，每个 segment 可以保存在不同的 6 个副本上，这保证了存储层的可扩展。然而由于只会有一个可写实例，所以其计算层并无法扩展，因此对于高并发大数据量的场景，还是需要分库分表。</p><p>总结一下：Aurora 通过对 IO 链路的优化和 quorum 写相比 RDS 版的 MYSQL 性能提升了 35 倍，但其本质上仍然是一个单机数据库。Aurora 可以被定性为一款云上的高性能单机关系数据库。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本博客沿着 6.824 课程的思路，对 Aurora 的由来，设计思路，具体架构进行了简单的介绍。</p><h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><ul><li><a href="http://nil.csail.mit.edu/6.824/2020/notes/l-aurora.txt" target="_blank" rel="noopener">6.824 讲义</a></li><li><a href="http://nil.csail.mit.edu/6.824/2020/video/10.html" target="_blank" rel="noopener">6.824 视频</a></li><li><a href="http://nil.csail.mit.edu/6.824/2020/papers/aurora.pdf" target="_blank" rel="noopener">论文</a></li><li><a href="https://aws.amazon.com/cn/rds/aurora/?nc2=type_a&amp;aurora-whats-new.sort-by=item.additionalFields.postDateTime&amp;aurora-whats-new.sort-order=desc" target="_blank" rel="noopener">Amazon Aurora 产品</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;相关背景&quot;&gt;&lt;a href=&quot;#相关背景&quot; class=&quot;headerlink&quot; title=&quot;相关背景&quot;&gt;&lt;/a&gt;相关背景&lt;/h2&gt;&lt;p&gt;Amazon Aurora 是一种与 MySQL 和 PostgreSQL 兼容的关系数据库，专为云而打造，既具有传统企业数</summary>
      
    
    
    
    
    <category term="论文阅读" scheme="https://tanxinyu.work/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    <category term="分布式存储" scheme="https://tanxinyu.work/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/"/>
    
  </entry>
  
  <entry>
    <title>Frangipani 论文阅读</title>
    <link href="https://tanxinyu.work/frangipani-thesis/"/>
    <id>https://tanxinyu.work/frangipani-thesis/</id>
    <published>2021-04-10T11:24:21.000Z</published>
    <updated>2022-01-30T03:41:47.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Frangipani 是一篇很古老的分布式存储论文，其设计思想在今天看来有很多已经过时了，但也有一定的参考意义。</p><p>该论文主要介绍了三个方面的工作：</p><ul><li>cache coherence</li><li>distributed transactions</li><li>distributed crash recovery</li></ul><h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><p>具体内容可以参考此 <a href="https://www.cnblogs.com/jamgun/p/14668522.html" target="_blank" rel="noopener">博客</a> 和 6.824 课程的 <a href="http://nil.csail.mit.edu/6.824/2020/notes/l-frangipani.txt" target="_blank" rel="noopener">讲义</a>，后者较为详细。</p><p>有关后两个工作可以直接参考以上博客的介绍，有关 cache coherence 可以进一步参考 <a href="https://mp.weixin.qq.com/s/HvgaXjHgD4_nVz81ipmbhg" target="_blank" rel="noopener">CPU 缓存的实现方式</a> 和基于 MESI 协议的 <a href="https://mp.weixin.qq.com/s/IT9clEXFb5hZJ8-ItheAng" target="_blank" rel="noopener">CPU 缓存一致性实现</a>。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>简单记录一下 Frangipani 论文的主要思想并记录一些相关博客。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="http://nil.csail.mit.edu/6.824/2020/notes/l-frangipani.txt" target="_blank" rel="noopener">6.824 讲义</a></li><li><a href="http://nil.csail.mit.edu/6.824/2020/video/11.html" target="_blank" rel="noopener">6.824 视频</a></li><li><a href="http://nil.csail.mit.edu/6.824/2020/papers/thekkath-frangipani.pdf" target="_blank" rel="noopener">论文</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;Frangipani 是一篇很古老的分布式存储论文，其设计思想在今天看来有很多已经过时了，但也有一定的参考意义。&lt;/p&gt;
&lt;p&gt;该论文主要介</summary>
      
    
    
    
    
    <category term="论文阅读" scheme="https://tanxinyu.work/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    <category term="分布式存储" scheme="https://tanxinyu.work/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/"/>
    
  </entry>
  
  <entry>
    <title>Chain Replication 论文阅读</title>
    <link href="https://tanxinyu.work/chain-replication-thesis/"/>
    <id>https://tanxinyu.work/chain-replication-thesis/</id>
    <published>2021-04-09T14:20:12.000Z</published>
    <updated>2022-01-30T03:41:12.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>对于 raft、paxos 这类共识算法，leader 节点需要将客户端的写请求编号并发送给所有 follower 以期望达成共识，这一定程度上导致写性能无法随节点个数线性增长，因为 leader 同步的数据量会随着节点数的增长而增长，从而使得主节点承载着更大的压力，往往成为了瓶颈。</p><p>2004 年，Chain Replication （之后简称 cr）方案被提出，其也能够保证多副本间的线性一致性。具体思路是每个节点只负责向后续节点进行备份，从而将压力分摊到整个链上。因为其与上述的共识协议相比，其每个节点的写入负载几乎一致，从而不存在单节点负载很高影响性能的问题。</p><p>以上都是论文中的吹的说法，我个人持怀疑态度：首先 raft 的 leader 向所有 follower 发送是并行的，而 cr 是串行的，因此就性能上，个人不觉得后者会更快；其次随着节点数增多，虽然 raft 的 leader 负载会更大可能增大延迟，但是 cr 一定会增加延迟（多一轮 RTT），因此就写扩展性上，我也没看到 cr 有什么明显的优势。至于说什么拆分读写负载，只能说 raft 早就提出 follower read 的解决方案了，cr 做的也不过是把读放到了另一个节点上，并无扩展性，craq 才相对做到了一定程度的读性能的可扩展性。 </p><p>但不论如何，很多顶级产品比如 Ceph，Parameter Server 等都用到了 cr，所以了解一下 cr 还是有必要的，其能够拓宽我们对共识协议的了解边界。毕竟其实现线性一致性的方式还是比较巧妙的，而且设计也比较简单，没 raft 那么多 corner case。</p><h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><p>有关 CR，可以参考此 <a href="https://zhuanlan.zhihu.com/p/344522347" target="_blank" rel="noopener">博客</a>。<br>有关 CRAQ，可以参考此 <a href="https://www.cnblogs.com/brianleelxt/p/13275647.html" target="_blank" rel="noopener">博客</a> 和此 <a href="https://zhuanlan.zhihu.com/p/344808961" target="_blank" rel="noopener">博客</a>。</p><p>CR 能够以很简单的设计实现多副本的线性一致性，不过其不能自己处理脑裂和分区的问题，因而还需要另一个高可用的配置服务器集群来协作提供高可用服务。</p><p>CRAQ 相比 CR 做了读性能优化，使得读性能可以线性扩展且保证线性一致性。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>简单记录了 cr 和 craq 的工作原理。</p><h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><ul><li><a href="http://nil.csail.mit.edu/6.824/2020/notes/l-craq.txt" target="_blank" rel="noopener">6.824 讲义</a></li><li><a href="http://nil.csail.mit.edu/6.824/2020/video/9.html" target="_blank" rel="noopener">6.824 视频</a></li><li><a href="https://www.cs.cornell.edu/home/rvr/papers/OSDI04.pdf" target="_blank" rel="noopener">CR 论文</a></li><li><a href="http://nil.csail.mit.edu/6.824/2020/papers/craq.pdf" target="_blank" rel="noopener">CRAQ 论文</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;对于 raft、paxos 这类共识算法，leader 节点需要将客户端的写请求编号并发送给所有 follower 以期望达成共识，这一定程</summary>
      
    
    
    
    
    <category term="论文阅读" scheme="https://tanxinyu.work/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    <category term="分布式系统理论" scheme="https://tanxinyu.work/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%90%86%E8%AE%BA/"/>
    
    <category term="共识算法" scheme="https://tanxinyu.work/tags/%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
</feed>
